## å„ç§å·ç§¯æ–¹å¼

![Conv2D](../assets/images/ConvTypes_1.png)

æœ€åŸºæœ¬çš„å·ç§¯ï¼Œæ¯ä¸ªå·ç§¯æ ¸çš„å¤§å°ä¸º `[channel_size, kernel_size, kernel_size]`ï¼Œé€šè¿‡è®¾å®š `sride` çš„å¤§å°å¯ä»¥ç¼©å°ç‰¹å¾å›¾çš„å¤§å°ã€‚åœ¨ PyTorch ä¸Šçš„å®ç°å¦‚ä¸‹ï¼š

```python
nn.Conv2d(in_channels=in_channel,    # è¾“å…¥ç‰¹å¾çš„é€šé“æ•°
          out_channels=out_channel,  # è¾“å‡ºç‰¹å¾çš„é€šé“æ•°
          kernel_size=kernel_size,   # å·ç§¯æ ¸çª—å£çš„å¤§å°
          stride=stride,             # stride è®¾ç½®ä¸º 1 ç‰¹å¾å›¾ä¸å˜ï¼Œè®¾ç½®ä¸º 2 ç‰¹å¾å›¾å˜ä¸º 1/2
          padding=padding)           # å·ç§¯æ—¶å€™åœ¨ç‰¹å¾å›¾å¤–éƒ¨è¡¥å……é›¶çš„å¤§å°

```

![GroupConv](../assets/images/ConvTypes_2.png)



## å·ç§¯ç»„å—ï¼ˆBlockï¼‰

### Inception

2014 å¹´ç”± Google åœ¨è®ºæ–‡ä¸­æå‡º

### æ®‹å·®ç»“æ„

æ®‹å·®ç»“æ„ï¼ˆResidualï¼‰ç”±ä½•æºæ˜ç­‰äººäº 2015 å¹´åœ¨å¾®è½¯ç ”ç©¶é™¢ï¼ˆMicroft Researchï¼‰åœ¨è®ºæ–‡ [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - CVPR 2016 ä¸Šæå‡ºï¼Œåœ¨ ImageNet å›¾åƒåˆ†ç±»ç«èµ›ï¼ˆILSVRC 2015ï¼‰ä¸Šè·å¾—å† å†›ï¼Œè¯æ˜äº†è¶…æ·±ç½‘ç»œï¼ˆä¾‹å¦‚ ResNet-152ï¼‰çš„è®­ç»ƒæ˜¯æœ‰å¯èƒ½å®ç°çš„ã€‚

æ®‹å·®ç»“æ„å¼•å…¥æ®‹å·®å—å’Œè·³è·ƒğŸ”—ï¼ˆSkip Connectionï¼‰è®©æ¢¯åº¦å¯ä»¥ç›´æ¥åå‘ä¼ æ’­ï¼Œå¦‚æœä½¿ç”¨æ•°å­¦è¡¨è¾¾ä¸ºï¼š

$y=F(x)+x$

å…¶ä¸­ $x$ æ˜¯è¾“å…¥ï¼Œ$F(x)$ æ˜¯å·ç§¯å±‚æå–çš„ç‰¹å¾ã€‚

### å€’æ®‹å·®ç»“æ„

å€’æ®‹å·®ç»“æ„ï¼ˆInverted Residualï¼‰åœ¨ MobileNetV2 çš„è®ºæ–‡ä¸­é¦–æ¬¡æå‡ºã€‚

## ç½‘ç»œç»“æ„

### ViTï¼ˆVision Transformerï¼‰

![ViT](../assets/images/ViT.png)

#### åŸºæœ¬åŸç†

ViT ä¸ Transformer çš„å”¯ä¸€åŒºåˆ«å°±åœ¨äºï¼ŒViT å¤šäº†ä¸€ä¸ªå°†å›¾ç‰‡è¿›è¡ŒåµŒå…¥çš„æ“ä½œï¼Œç®€å•åœ°å°†å°±æ˜¯æŠŠå›¾ç‰‡æƒ³ä¸ªåŠæ³•è½¬æ¢æˆ Transformer çš„è¾“å…¥å½¢å¼ã€‚å®ç°è¿™ä¸€ä¸ªéƒ¨åˆ†çš„æ“ä½œå°±æ˜¯ Vision Patch Embeddingã€‚

Vision Patch Embedding æŠŠå›¾ç‰‡å‡åŒ€åˆ†æˆä¸€ä¸ªä¸€ä¸ªçš„ Patchï¼Œç„¶åæŠŠæ¯ä¸€ä¸ª Patch reshape æˆä¸€ç»´ï¼Œè¿™æ ·å°±å¯ä»¥è¿›å…¥ä¸€ä¸ªçº¿æ€§å±‚ï¼Œæœ€åæŠŠæ‰€æœ‰ Patch ç»è¿‡çº¿æ€§å±‚ä¹‹åçš„è¾“å‡ºï¼Œä¸ `pos_embedding` æ‹¼æ¥æˆä¸€ä¸ªçŸ©é˜µï¼Œä½œä¸º Transformer Block çš„è¾“å…¥ã€‚çœ‹æ‡‚äº† Patch Embedding å°±æ‡‚ ViT äº†ã€‚

Vision Patch  Embedding çš„å®ç°ï¼Œå…¶å®å°±æ˜¯ä¸€ä¸ªå·ç§¯æ“ä½œï¼Œå…¶ä¸­ `kernel_size` å’Œ `stride` éƒ½ä¸º `patch_size`ï¼Œå®Œæ•´çš„ç»“æ„å¯ä»¥çœ‹ä¸Šå›¾ã€‚

#### ä»£ç å®ç°

```python
import torch
from torch import nn


class TransformerHead(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.ln = nn.LayerNorm(hidden_dim)

    def forward(self, x):
        x = self.ln(x)
        return x


class TransformerBlock(nn.Module):
    def __init__(self, num_heads, hidden_dim, mlp_dim, dropout, attention_dropout):
        super().__init__()
        self.num_heads = num_heads

        self.ln_1 = nn.LayerNorm(hidden_dim)
        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=attention_dropout, batch_first=True)
        self.dropout = nn.Dropout(dropout)

        self.ln_2 = nn.LayerNorm(hidden_dim)
        self.mlp = nn.Sequential(
            nn.Linear(hidden_dim, mlp_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_dim, hidden_dim),
            nn.Dropout(dropout),
        )

    def forward(self, input):
        x = self.ln_1(input)
        x, _ = self.self_attention(x, x, x, need_weights=False)
        x = self.dropout(x)
        x = x + input

        y = self.ln_1(x)
        y = self.mlp(y)
        return x + y


class VisionPatchEmbedded(nn.Module):
    def __init__(self, image_size, hidden_dim, patch_size, dropout):
        super().__init__()
        self.image_size = image_size
        self.patch_size = patch_size
        self.hidden_dim = hidden_dim
        self.conv_proj = nn.Conv2d(in_channels=3, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size)
        self.class_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))
        self.seq_length = (image_size // patch_size) ** 2 + 1
        self.pos_embedding = nn.Parameter(torch.empty(1, self.seq_length, self.hidden_dim).normal_(std=0.02))
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        batch_size, _, h, w = x.shape
        n_h = h // self.patch_size
        n_w = w // self.patch_size
        x = self.conv_proj(x)
        x = x.reshape(batch_size, self.hidden_dim, n_h * n_w)
        x = x.permute(0, 2, 1)

        batch_class_token = self.class_token.expand(batch_size, -1, -1)
        x = torch.cat([batch_class_token, x], dim=1)
        x = x + self.pos_embedding
        x = self.dropout(x)
        return x


class VisionTransformer(nn.Module):
    def __init__(self, image_size, patch_size, num_layers, num_heads, hidden_dim, mlp_dim, attention_dropout, dropout):
        super().__init__()
        self.patch_embedded = VisionPatchEmbedded(image_size, hidden_dim, patch_size, dropout)
        self.transformer_layers = nn.Sequential(
            *[TransformerBlock(num_heads, hidden_dim, mlp_dim, dropout, attention_dropout) for _ in range(num_layers)])
        self.head = TransformerHead(hidden_dim)

    def forward(self, x):
        x = self.patch_embedded(x)
        x = self.transformer_layers(x)
        x = self.head(x)
        return x


if __name__ == '__main__':
    image = torch.randn((1, 3, 224, 224))
    _, _, height, width = image.shape
    image_size = height
    model = VisionTransformer(image_size, patch_size=16, num_layers=12, num_heads=12, hidden_dim=768, mlp_dim=3072,
                              attention_dropout=0.5, dropout=0.5)
    output = model(image)
    print(output.shape)

```

## ç»å…¸ç½‘ç»œ

### AlexNet

![AlexNet](../assets/images/Networks/AlexNet.png)

### FCN å…¨å·ç§¯ç½‘ç»œ

å…¨å·ç§¯ç½‘ç»œï¼ˆFully Convolutional Network, FCNï¼‰

### UNet

å¤§éƒ¨åˆ†çš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œè¾“å‡ºçš„ç»“æœæ˜¯æ•´ä¸ªå›¾åƒçš„ç±»æ ‡ç­¾ï¼ŒUNet æ¯”è¾ƒä¸ä¸€æ ·ï¼Œå®ƒçš„è¾“å‡ºæ˜¯æ¯ä¸ªåƒç´ ç‚¹çš„ç±»åˆ«ï¼Œä¸åŒç±»åˆ«çš„åƒç´ ä¼šæ˜¾ç¤ºä¸åŒé¢œè‰²ã€‚UNet åœ¨ 2015 å¹´çš„è®ºæ–‡ [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) ä¸­é¦–æ¬¡æå‡ºã€‚

UNet æå‡ºçš„åˆè¡·å°±æ˜¯ä¸ºäº†è§£å†³åŒ»å­¦å›¾åƒåˆ†å‰²çš„é—®é¢˜ï¼Œå› æ­¤å®ƒé€šå¸¸ä¹Ÿç”¨åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒä¸Šï¼Œè¿™ç§ä»»åŠ¡ä¸­å›¾ç‰‡æ•°æ®å¾€å¾€æ¯”è¾ƒå°‘ã€‚UNet çš„ U å‹ç»“æ„ï¼Œå¯ä»¥ä½¿å¾—å®ƒä½¿ç”¨æ›´å°‘çš„è®­ç»ƒå›¾ç‰‡çš„åŒæ—¶ï¼Œè·å¾—ä¸é”™çš„åˆ†å‰²å‡†ç¡®åº¦ã€‚

UNet ç½‘ç»œçš„ç¼ºç‚¹ï¼š

1. ç”±äºéœ€è¦è®­ç»ƒæ¯ä¸ª patchï¼Œå¹¶ä¸” patch ä¹‹é—´çš„é‡å æœ‰å¾ˆå¤šå†—ä½™ï¼Œå¯¼è‡´åŒæ ·ç‰¹å¾è¢«å¤šæ¬¡è®­ç»ƒï¼Œé€ æˆèµ„æºæµªè´¹ï¼Œè®­ç»ƒæ—¶é—´é•¿ï¼Œæ•ˆç‡ä¹Ÿæ¯”è¾ƒä½ã€‚
2. å®šä½å‡†ç¡®å’Œè·å–ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸å¯å…¼å¾—ï¼Œå¤§çš„ patch éœ€è¦æ›´å¤šçš„ max poolingï¼Œè¿™æ ·ä¼šå‡å°‘å®šä½å‡†ç¡®æ€§ï¼›å°çš„ patch åªèƒ½çœ‹åˆ°å¾ˆå°çš„å±€éƒ¨ä¿¡æ¯ï¼ŒåŒ…å«çš„èƒŒæ™¯ä¿¡æ¯ä¸å¤Ÿã€‚

UNet ä¸­ç‰¹å¾èåˆæ–¹å¼ä½¿ç”¨æ‹¼æ¥æ–¹å¼çš„å¥½å¤„ï¼šç½‘ç»œæµ…å±‚ç‰¹å¾å›¾æ›´å…³æ³¨çº¹ç†ç‰¹å¾ï¼Œæ·±å±‚ç‰¹å¾æœ‰æ›´å¤§çš„è§†é‡åŸŸï¼Œæ›´å…³æ³¨æœ¬è´¨ã€‚é€šè¿‡åå·ç§¯æˆ–è€…ä¸Šé‡‡æ ·å¾—åˆ°çš„å¤§å°ºåº¦ç‰¹å¾å›¾ï¼Œæ˜¯ç¼ºå°‘è¾¹ç¼˜ä¿¡æ¯çš„ã€‚å› ä¸ºåœ¨æ¯ä¸€æ¬¡ä¸‹é‡‡æ ·æç‚¼ç‰¹å¾çš„åŒæ—¶ï¼Œä¹Ÿå¿…ç„¶ä¼šæŸå¤±ä¸€äº›è¾¹ç¼˜ç‰¹å¾ï¼Œå¤±å»çš„ç‰¹å¾ä¸èƒ½å¤Ÿé€šè¿‡åå·ç§¯å’Œä¸Šé‡‡æ ·æ‰¾å›ã€‚å› æ­¤ï¼Œé€šè¿‡å°†æ·±å±‚ç‰¹å¾å›¾ä¸æµ…å±‚ç‰¹å¾å›¾æ‹¼æ¥çš„æ–¹å¼ï¼Œå°±èƒ½å¤ŸåŒæ—¶æŠŠè¾¹ç¼˜ç‰¹å¾å’Œæ·±å±‚ç‰¹å¾èåˆã€‚

UNet åœ¨åŒ»ç–—å½±åƒè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­æœ‰ç”¨çš„åŸå› ï¼š

1. åŒ»ç–—å½±åƒè¯­ä¹‰ç®€å•ï¼Œç»“æ„å›ºå®šï¼Œç›¸æ¯”è¾ƒè‡ªåŠ¨é©¾é©¶ç­‰è¾ƒä¸ºå•ä¸€ï¼Œå› æ­¤ä¸éœ€è¦å»ç­›é€‰è¿‡æ»¤æ— ç”¨çš„ä¿¡æ¯ã€‚
2. åŒ»ç–—å½±åƒæ•°æ®è¾ƒå°‘ï¼Œè·å–éš¾åº¦å¤§ï¼Œæ•°æ®é‡å¯èƒ½åªæœ‰å‡ ç™¾ä¸åˆ°ï¼Œä½¿ç”¨å¤§ç½‘ç»œå®¹æ˜“è¿‡æ‹Ÿåˆã€‚
3. åŒ»å­¦å½±åƒå¾€å¾€æ˜¯å¤šæ¨¡æ€çš„ï¼Œå¾€å¾€éœ€è¦è‡ªå·±è®¾è®¡ç½‘ç»œå»æå–ä¸åŒçš„æ¨¡æ€ç‰¹å¾ï¼Œè½»é‡ç»“æ„ç®€å•çš„ UNet å¯ä»¥æœ‰æ›´å¤§çš„æ“ä½œç©ºé—´ã€‚

![UNet](../assets/images/Networks/UNet.png)

### YOLOv8

