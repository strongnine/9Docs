## å„ç§å·ç§¯æ–¹å¼

![Conv2D](../assets/images/Conv2D.png)

æœ€åŸºæœ¬çš„å·ç§¯ï¼Œæ¯ä¸ªå·ç§¯æ ¸çš„å¤§å°ä¸º `[channel_size, kernel_size, kernel_size]`ï¼Œé€šè¿‡è®¾å®š `sride` çš„å¤§å°å¯ä»¥ç¼©å°ç‰¹å¾å›¾çš„å¤§å°ã€‚åœ¨ PyTorch ä¸Šçš„å®ç°å¦‚ä¸‹ï¼š

```python
nn.Conv2d(in_channels=in_channel,    # è¾“å…¥ç‰¹å¾çš„é€šé“æ•°
          out_channels=out_channel,  # è¾“å‡ºç‰¹å¾çš„é€šé“æ•°
          kernel_size=kernel_size,   # å·ç§¯æ ¸çª—å£çš„å¤§å°
          stride=stride,             # stride è®¾ç½®ä¸º 1 ç‰¹å¾å›¾ä¸å˜ï¼Œè®¾ç½®ä¸º 2 ç‰¹å¾å›¾å˜ä¸º 1/2
          padding=padding)           # å·ç§¯æ—¶å€™åœ¨ç‰¹å¾å›¾å¤–éƒ¨è¡¥å……é›¶çš„å¤§å°

```

![Conv3D](../assets/images/Conv3D.png)

![GroupConv](../assets/images/GroupConv.png)

![DepthWiseConv](../assets/images/DepthWiseConv.png)

![PointWiseConv](../assets/images/PointWiseConv.png)

![DepthWiseSeparableConv](../assets/images/DepthWiseSeparableConv.png)

![TransposedConv](../assets/images/DilatedConv.png)

![TransposedConv](../assets/images/TransposedConv.png)

## å·ç§¯ç»„å—ï¼ˆBlockï¼‰

### Inception

2014 å¹´ç”± Google åœ¨è®ºæ–‡ä¸­æå‡º

### æ®‹å·®ç»“æ„

æ®‹å·®ç»“æ„ï¼ˆResidualï¼‰ç”±ä½•æºæ˜ç­‰äººäº 2015 å¹´åœ¨å¾®è½¯ç ”ç©¶é™¢ï¼ˆMicroft Researchï¼‰åœ¨è®ºæ–‡ [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - CVPR 2016 ä¸Šæå‡ºï¼Œåœ¨ ImageNet å›¾åƒåˆ†ç±»ç«èµ›ï¼ˆILSVRC 2015ï¼‰ä¸Šè·å¾—å† å†›ï¼Œè¯æ˜äº†è¶…æ·±ç½‘ç»œï¼ˆä¾‹å¦‚ ResNet-152ï¼‰çš„è®­ç»ƒæ˜¯æœ‰å¯èƒ½å®ç°çš„ã€‚

æ®‹å·®ç»“æ„å¼•å…¥æ®‹å·®å—å’Œè·³è·ƒğŸ”—ï¼ˆSkip Connectionï¼‰è®©æ¢¯åº¦å¯ä»¥ç›´æ¥åå‘ä¼ æ’­ï¼Œå¦‚æœä½¿ç”¨æ•°å­¦è¡¨è¾¾ä¸ºï¼š

$y=F(x)+x$

å…¶ä¸­ $x$ æ˜¯è¾“å…¥ï¼Œ$F(x)$ æ˜¯å·ç§¯å±‚æå–çš„ç‰¹å¾ã€‚

### å€’æ®‹å·®ç»“æ„

å€’æ®‹å·®ç»“æ„ï¼ˆInverted Residualï¼‰åœ¨ MobileNetV2 çš„è®ºæ–‡ä¸­é¦–æ¬¡æå‡ºã€‚

## ç½‘ç»œç»“æ„

### ViTï¼ˆVision Transformerï¼‰

![ViT](../assets/images/ViT.png)

#### åŸºæœ¬åŸç†

ViT ä¸ Transformer çš„å”¯ä¸€åŒºåˆ«å°±åœ¨äºï¼ŒViT å¤šäº†ä¸€ä¸ªå°†å›¾ç‰‡è¿›è¡ŒåµŒå…¥çš„æ“ä½œï¼Œç®€å•åœ°å°†å°±æ˜¯æŠŠå›¾ç‰‡æƒ³ä¸ªåŠæ³•è½¬æ¢æˆ Transformer çš„è¾“å…¥å½¢å¼ã€‚å®ç°è¿™ä¸€ä¸ªéƒ¨åˆ†çš„æ“ä½œå°±æ˜¯ Vision Patch Embeddingã€‚

Vision Patch Embedding æŠŠå›¾ç‰‡å‡åŒ€åˆ†æˆä¸€ä¸ªä¸€ä¸ªçš„ Patchï¼Œç„¶åæŠŠæ¯ä¸€ä¸ª Patch reshape æˆä¸€ç»´ï¼Œè¿™æ ·å°±å¯ä»¥è¿›å…¥ä¸€ä¸ªçº¿æ€§å±‚ï¼Œæœ€åæŠŠæ‰€æœ‰ Patch ç»è¿‡çº¿æ€§å±‚ä¹‹åçš„è¾“å‡ºï¼Œä¸ `pos_embedding` æ‹¼æ¥æˆä¸€ä¸ªçŸ©é˜µï¼Œä½œä¸º Transformer Block çš„è¾“å…¥ã€‚çœ‹æ‡‚äº† Patch Embedding å°±æ‡‚ ViT äº†ã€‚

Vision Patch  Embedding çš„å®ç°ï¼Œå…¶å®å°±æ˜¯ä¸€ä¸ªå·ç§¯æ“ä½œï¼Œå…¶ä¸­ `kernel_size` å’Œ `stride` éƒ½ä¸º `patch_size`ï¼Œå®Œæ•´çš„ç»“æ„å¯ä»¥çœ‹ä¸Šå›¾ã€‚

#### ä»£ç å®ç°

```python
import torch
from torch import nn


class TransformerHead(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.ln = nn.LayerNorm(hidden_dim)

    def forward(self, x):
        x = self.ln(x)
        return x


class TransformerBlock(nn.Module):
    def __init__(self, num_heads, hidden_dim, mlp_dim, dropout, attention_dropout):
        super().__init__()
        self.num_heads = num_heads

        self.ln_1 = nn.LayerNorm(hidden_dim)
        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=attention_dropout, batch_first=True)
        self.dropout = nn.Dropout(dropout)

        self.ln_2 = nn.LayerNorm(hidden_dim)
        self.mlp = nn.Sequential(
            nn.Linear(hidden_dim, mlp_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_dim, hidden_dim),
            nn.Dropout(dropout),
        )

    def forward(self, input):
        x = self.ln_1(input)
        x, _ = self.self_attention(x, x, x, need_weights=False)
        x = self.dropout(x)
        x = x + input

        y = self.ln_1(x)
        y = self.mlp(y)
        return x + y


class VisionPatchEmbedded(nn.Module):
    def __init__(self, image_size, hidden_dim, patch_size, dropout):
        super().__init__()
        self.image_size = image_size
        self.patch_size = patch_size
        self.hidden_dim = hidden_dim
        self.conv_proj = nn.Conv2d(in_channels=3, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size)
        self.class_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))
        self.seq_length = (image_size // patch_size) ** 2 + 1
        self.pos_embedding = nn.Parameter(torch.empty(1, self.seq_length, self.hidden_dim).normal_(std=0.02))
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        batch_size, _, h, w = x.shape
        n_h = h // self.patch_size
        n_w = w // self.patch_size
        x = self.conv_proj(x)
        x = x.reshape(batch_size, self.hidden_dim, n_h * n_w)
        x = x.permute(0, 2, 1)

        batch_class_token = self.class_token.expand(batch_size, -1, -1)
        x = torch.cat([batch_class_token, x], dim=1)
        x = x + self.pos_embedding
        x = self.dropout(x)
        return x


class VisionTransformer(nn.Module):
    def __init__(self, image_size, patch_size, num_layers, num_heads, hidden_dim, mlp_dim, attention_dropout, dropout):
        super().__init__()
        self.patch_embedded = VisionPatchEmbedded(image_size, hidden_dim, patch_size, dropout)
        self.transformer_layers = nn.Sequential(
            *[TransformerBlock(num_heads, hidden_dim, mlp_dim, dropout, attention_dropout) for _ in range(num_layers)])
        self.head = TransformerHead(hidden_dim)

    def forward(self, x):
        x = self.patch_embedded(x)
        x = self.transformer_layers(x)
        x = self.head(x)
        return x


if __name__ == '__main__':
    image = torch.randn((1, 3, 224, 224))
    _, _, height, width = image.shape
    image_size = height
    model = VisionTransformer(image_size, patch_size=16, num_layers=12, num_heads=12, hidden_dim=768, mlp_dim=3072,
                              attention_dropout=0.5, dropout=0.5)
    output = model(image)
    print(output.shape)

```

## ç»å…¸ç½‘ç»œ

### AlexNet

![AlexNet](../assets/images/Networks/AlexNet.png)
