## 「类别型特征」的编码方式

序号编码（Ordinal Encoding）：通常用于处理类别间具有大小关系的数据，转换后依然保留相对的大小关系。

独热编码（One-hot Encoding）：通常用于处理类别间不具有大小关系的特征。对于类别取值较多的情况下使用 One-hot Encoding 需要注意以下问题：

- 使用稀疏向量来节省空间。
- 配合特征选择来降低维度。高维度特征会带来几方面的问题：
  - 在 K 近邻算法中，高维度空间下两点之间的距离很难得到有效的衡量；
  - 在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合的问题；
  - 通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度；

二进制编码（Binary Encoding）：先用序号编码给每个类别赋予一个类别 ID，然后将类别 ID 对应的二进制编码作为结果。

Helmert Contrast

Sum Contrast

Polynomial Contrast

Backward Difference Contrast

## 组合特征

为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。在实际问题中，需要面对多种高维特征，简单地两两组合，依然容易存在参数过多、过拟合等问题。

**怎样有效地找到组合特征？**可以利用决策树来寻找特征组合方式。

例如，影视推荐问题有两个低阶特征「语言」和「类型」，其中有语言分为中文和英文，类型分为电影和电视剧，那么这两个特征的高阶组合特征有（中文，电影）、（英文，电视剧）、（英文，电影）、（中文，电视剧）四种。下表的数据，就可以变为新的数据：

| 是否点击 | 语言 |  类型  |
| :------: | :--: | :----: |
|    0     | 中文 |  电影  |
|    1     | 英文 |  电影  |
|    1     | 中文 | 电视剧 |
|    0     | 英文 | 电视剧 |

| 是否点击 | 语言 = 中文，类型 = 电影 | 语言 = 英文，类型 = 电影 | 语言 = 中文，类型 = 电视剧 | 语言 = 英文，类型 = 电视剧 |
| :------: | :----------------------: | :----------------------: | :------------------------: | :------------------------: |
|    0     |            1             |            0             |             0              |             0              |
|    1     |            0             |            1             |             0              |             0              |
|    1     |            0             |            0             |             0              |             1              |
|    0     |            0             |            0             |             0              |             1              |

以逻辑回归为例，假设数据的特征向量为 $X=(x_1,x_2,\dots,x_k)$，则有：



$Y=\text{sigmoid}(\sum_i\sum_jw_{ij}\langle x_i,x_j\rangle)$



其中 $\langle x_i,x_j\rangle$ 表示 $x_i$ 和 $x_j$ 的组合特征，$w_{ij}$ 的维度等于第 $i$ 和第 $j$ 个特征不同取值的个数。在上例中，「语言」这个特征有中文和英文两个选择，「类型」这个特征有电影和电视剧两个选择，那么 $w_{ij}$ 的维度就为 $2\times 2=4$. 当组合之前的两个特征不同取值的个数都不大时，用这种方式不会有太大的问题。但是对于某些问题，有用户 ID 和物品 ID，而用户和物品的数量动辄几千万，几千万乘几千万 $m\times n$，这么大的参数量，无法进行学习。

**对于这种「高维组合特征」要如何处理？**假设用户和物品的数量分别为 $m$ 和 $n$，一种行之有效的方法是将两个特征分别用 $k$ 维的低维向量表示（$k\ll m,k\ll n$），这样原本 $m\times n$ 的学习参数就降低为 $m\times k + n\times k$，这其实等价于推荐算法中的**矩阵分解**。

## 文本表示模型

最基础的文本表示模型是**词袋模型**，就是将每篇文章看成一袋子词，并忽略每个词出现的顺序。每篇文章可以表示成一个长向量，向量中的每一维度代表一个单词，而该维对应的权重则反映了这个词在原文章中的重要程度。常用 TF-IDF（Term Frequency-Inverse Document Frequency）来计算权重：



$\text{TF-IDF}(t,d)=\text{TF}(t,d)\times \text{IDF}(t)$



其中 $\text{TF}(t,d)$ 为单词 $t$ 在文档 $d$ 中出现的频率，$\text{IDF}(t)$ 是逆文档频率，用来衡量单词 $t$ 对表达语义所起的重要性，表示为：



$\text{IDF}(t) = \log{\frac{\text{Num. of articles}}{\text{Num. of articles containing word $t$ }+1}}$



直观解释为，如果一个单词在非常多的文章里面都出现，那么它可能是一个比较通用的词汇，对于区分谋篇文章特殊语义的贡献比较小，因此对权重做一定惩罚。

有的时候，多个不同的单词组合起来会有特殊的含义，比如 natural language processing 组合起来就有「自然语言处理」的意思，但是把这三个单词拆开，就没有组合起来的特别。将类似这样的连续出现的 $n$ 个词（$n\le N$）组成的词组（N-gram）也作为一个单独的特征放到向量表示中去，构成 N-gram 模型。

**词干抽取（Word Stemming）**，同一个词可能有多种词性变化，却有相似的含义。在实际应用中，一般会对单词进行词干抽取，即将不同词性的单词统一成为同一词干的形式。

**主题模型**用于从文本库中发现有代表性的主题（得到每个主题上面词的分布特性）。

**词嵌入**是一类将词向量化的模型的统称，将每个词都映射成低维空间上的一个稠密向量（Dense Vector），通常维度 $K=50\sim 300$。词嵌入将每个词映射成一个 $K$ 维向量，如果一篇文章有 $N$ 个词，就可以用一个 $N\times K$ 的矩阵来表示这篇文章。但是这样的表示仅仅只是底层的表示，在实际应用中，如果仅仅把这个矩阵作为原文本的表示特征输入到机器学习模型当中，很难得到令人满意的结果。
