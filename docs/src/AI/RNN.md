## 循环神经网络

记录网络的输入序列为 $x_1,x_2,\cdots,x_n$，一个循环神经网络（RNN）展开后可以看做一个 $n$ 层的前馈神经网络，第 $t$ 层对应着 $t$ 时刻的状态（$t=1,2,\cdots,n$），记第 $t$ 层（时刻）的输入状态、隐藏状态、输出状态分别为 $x_t, h_t, o_t$，训练时的目标输出值为 $y_t$，则有：**隐藏状态 $h_t$ 由当前时刻的输入状态 $x_t$ 和上一时刻的隐藏状态 $h_{t-1}$ 共同确定**，即

$h_t=\sigma(Ux_t+Wh_{t-1}+b),$

其中，$U$ 是输入层到隐藏层的权重矩阵，$W$ 是不同时刻的隐藏层之间的连接权重，$b$ 是偏置向量，$\sigma(\cdot)$ 是激活函数（通常使用 $\texttt{Tanh}$ 函数）。循环神经网络最大的特点就是**当前时刻的隐藏状态不仅与当前时刻的输入状态有关，还受上一时刻的隐藏状态影响。**

输出状态 $o_t$ 的计算公式为：

$o_t=g(Vh_t+c),$

其中，$V$ 是隐藏层到输出层的权重矩阵，$c$ 是偏置向量，$g(\cdot)$ 是输出层的激活函数（对于分类任务可以采用 $\texttt{Softmax}$ 函数）。在训练时，网络在整个序列上的损失可以定义为不同时刻的损失之和：

$\mathcal{L}=\sum_t\mathcal{L}_t=\sum_t Loss(o_t,y_t).$

上述的权重矩阵 $U, W, V$ 是所有时刻共享参数的，这种机制不仅可以极大地减少网络需要学习的参数数量，而且使得网络可以处理长度不固定的输入序列。在 RNN 的训练过程中，由于不同时刻的状态是相互依赖的，因此需要存储各个时刻的状态信息，而且无法进行并行计算，这导致整个训练过程内存消耗大，并且速度较慢。

RNN 之所以能够在序列数据的处理上获得出色的表现，是因为它拥有长期记忆功能，能够压缩并获得长期数据的表示。实际上，在 RNN 训练过程中，为了防止梯度爆炸（或弥散）的问题，通常采用带截断的反向传播算法，即仅反向传播 $k$ 个时间步的梯度。理论上的无限记忆优势在实际中几乎不存在。

实际上，在序列任务中，卷积神经网络（CNN）在空洞卷积的帮助下（例如 TextCNN），具有更好的并行化和可训练性。只不过长短期记忆网络（LSTM）和 Seq2Seq 网络依然是序列数据处理中最为通用的架构。还有人很多工作对卷积神经网络和循环神经网络进行组合使用，提升序列数据处理能力（如 TrellisNet）。

## 长短期记忆网络（LSTM）



### 长程依赖问题

**LSTM 是如何实现长短期记忆功能的？**

（1）一般的 RNN 中，只有一个隐藏状态（hidden state）单元 $h_t$，不同时刻隐藏状态单元的参数是相同（共享）的。LSTM 在普通 RNN 的基础上增加了一个**元胞状态（cell state）单元** $c_t$，其在不同时刻有着可变的连接权重，可解决普通循环神经网络中的梯度消失或爆炸问题。

（2）LSTM 引入了**门控单元**，是神经网络学习到的用于控制信号的存储、利用和舍弃的单元。对于每一个时刻 $t$，LSTM 有**输入门** $i_t$、遗忘门 $f_i$ 和**输出门** $o_t$ 共 3 个门控单元。每个门控单元的输入包括当前时刻的序列信息 $x_t$ 和上一时刻的隐藏状态单元 $h_{t-1}$，具体公式：

$i_t=\sigma\left( W_i x_t + U_i h_{t-1} + b_i \right),$

$f_t=\sigma\left( W_f x_t + U_f h_{t-1} + b_f \right),$

$o_t=\sigma\left( W_o x_t + U_o h_{t-1} + b_o \right).$

3 个门控单元都相当于一个全连接层，激活函数 $\sigma(\cdot)$ 的取值范围是 $[0, 1]$，常用 $\texttt{Sigmoid}$ 作为激活函数。**当门控单元的状态为 0 时，信号会被全部丢弃；当状态为 1 时，信号会被全部保留。**

（3）元胞状态单元从上一个时刻 $c_{t-1}$ 到当前时刻 $c_t$ 的转移是由输入门和遗忘门共同控制的。输入门决定了当前时刻输入信息 $\tilde{c}_t$ 有多少被吸收，遗忘门决定了上一时刻元胞状态单元 $c_{t-1}$ 有多少不被遗忘，最终的元胞状态单元 $c_t$ 由两个门控处理后的信号取和产生。具体公式：

$\tilde{c}_t = \texttt{Tanh}\left( W_c x_t + U_c h_{t-1} + b_c \right),$

$c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t,$

其中 $\odot$ 为逐元素点乘操作。LSTM 的隐藏状态单元 $h_t$ 则由输出门 $c_t$ 决定：

$h_t = o_t \odot \texttt{Tanh}(c_t).$

不仅隐藏状态单元 $h_{t-1}$ 和 $h_t$ 之间有着较为复杂的循环连接，内部的元胞状态单元 $c_{t-1}$ 和 $c_t$ 之间还具有线性自循环关系，这个关系可以看作是在滑动处理不同时刻的信息。

（4）LSTM 中遗忘门和输出门的激活函数十分重要。删除遗忘门的激活函数会导致之前的元胞状态不能很好地被抑制；而删除输出门的激活函数则可能会出现非常大的输出状态。

## 门控循环单元（GRU）

（1）GRU 只有两个门控单元，分别为**重置门** $r_t$ 和 **更新门** $z_t$，一个控制短期记忆，另一个控制长期记忆。

（2）GRU 中每个门控单元的输入包括当前时刻和序列信息 $x_t$ 和上一时刻的隐藏状态单元 $h_{t-1}$，具体计算公式为：

$r_t = \sigma\left( W_r x_t + U_r h_{t-1} \right),$

$z_t = \sigma\left( W_z x_t + U_z h_{t-1} \right),$

其中 $\sigma(\cdot)$ 是激活函数，一般用 $\texttt{Sigmoid}$。

（3）GRU 中重置门决定先前的隐藏状态单元是否被忽略，而更新门则控制当前隐藏状态单元是否需要被新的隐藏状态单元更新，具体公式：

$\tilde{h}_t = \texttt{Tanh} \left( W_h x_t + U_h (r_t \odot h_{t-1}) \right),$

$h_t = (1 - z_t) h_{t - 1} + z_t \tilde{h}_t,$

其中，$(1 - z_t)h_{t-1}$ 表示上一时刻保留下来（没被遗忘）的信息，$z_t \tilde{h}_t$ 是当前时刻记忆下来的信息。

用 $1 - z_t, z_t$  作为系数，表明对上一时刻遗忘多少权重的信息，就会在这一时刻记忆多少权重的信息以作为弥补。GRU 就是用这样的一种方式用一个更新门 $z_t$ 实现遗忘和记忆两个功能。

（4）GRU 只有一个隐藏状态单元 $h_t$，而 LSTM 有隐藏状态单元 $h_t$ 和元胞状态单元 $c_t$。

（5）GRU 具有更少的参数，更易于计算和实现。在不同数据集、不同超参配置下，可以取得与 LSTM 相当甚至更好的性能，并且具有更快地收敛速度。

## 序列到序列（Seq2Seq）

（1）Seq2Seq 的映射架构能够将一个可变长序列映射到另一个可变长序列。

（2）Seq2Seq 框架由于输入序列和输出序列是不等长的因此整个处理过程需要拆分为对序列的理解和翻译，也就是编码和解码。

（3）采用一个固定尺寸的状态向量 $C$ 作为编码器与解码器之间的「桥梁」。

（4）假设输入序列为 $X=(x_1,x_2,\cdots,x_T)$，编码器可以是一个简单的循环神经网络，其隐藏状态 $h_t$ 的计算公式为：

$h_t = f(h_{t-1}, x_t),$

其中，$f(\cdot)$ 是非线性激活函数，可以是简单的 $\texttt{Sigmoid}$ 函数，也可以是复杂的门控函数（LSTM、GRU 等）。将上述循环神经网络（编码器）最后一个时刻的隐藏状态 $h_T$ 作为状态向量，并输入到解码器。$C$ 是一个尺寸固定的向量，并且包含了整个序列的所有信息。

（5）解码器需要根据固定尺寸的状态向量 $C$ 来生成长度可变的解码序列 $Y=(y_1, y_2, \cdots, y_T)$。这里解码序列的长度 $T^\prime$ 和编码长度 $T$ 可以是不同的。解码器也可以用一个简单的循环神经网络来实现，其隐藏状态 $h_t$ 可以按照如下公式计算：

$h_t = f(h_{t-1},y_{t-1},C).$

解码器的输出由如下公式决定：

$P(y_t\mid y_{t-1},y_{t-2},\cdots,y_1,C) = g(h_t, y_{t-1},C),$

其中，$g(\cdot)$ 会产生一个概率分布（例如用 $\texttt{Softmax}$ 函数产生概率分布）。

（6）解码器的工作流程：

- 首先在收到一个启动信号（如 $y_0=\text{< start gt;}$）后开始工作，根据 $h_t, y_{t-1}, C$ 计算出 $y_t$  的概率分布；
- 然后对 $y_t$ 进行采样获得具体取值；
- 循环上述操作，直到遇到结束信号（如 $y_t=\text{< eos gt;}$；

（7）解码器的实现还能够用一种更加简单的方式，仅在初始时刻需要状态向量 $C$，其他时刻仅接收隐藏状态和上一时刻的输出信息 $P(y_t)=g(h_t,y_{t-1})$。

（8）在训练时，需要让模型输出的序列尽可能正确，这可以通过最大化对数似然概率来实现：

$\max_\theta \frac{1}{N}\sum_{n=1}^N \log p_\theta(Y_n \mid X_n),$

其中 $\theta$ 为模型参数，$X_n$ 是一个输入序列，$Y_n$ 是对应的输出序列， $(X_n,Y_n)$ 构成一个训练样本对。

（9）因为是序列到序列的转换，实际应用中可以通过贪心法求解 Seq2Seq，当度量标准、评估方式确定后，解码器每次根据当前的状态和已解码的序列选择一个最佳的解码结果，直至结束。

