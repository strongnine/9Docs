## Transformer 知识总结

### 原理

Transformer 整个网络结构由 Attention 机制组成。在 RNN（包括 LSTM、GRU 等）中计算是顺序的，只能从左向右或者从右向左依次计算，这种机制带来的 2 个问题：

- 时间片 $t$ 的计算依赖 $t-1$ 时刻的计算结果，限制了模型的并行能力；
- 顺序计算的过程中信息会丢失。尽管 LSTM 使用门机制的结构来缓解长期依赖的问题，但是在特别长期时依旧表现不好；

Transformer 通过以下方式来解决上面的问题：

- 使用 Attention 机制，讲序列中的任意两个位置之间的距离缩小为一个常量；
- 因为不是类似 RNN 的顺序结构，因此具有更好的并行性。也更为符合现有的 GPU 框架；

### Encoder 和 Decoder 模块

## 参考

[1] [知乎专栏：计算机视觉面试题 - Transformer 相关问题总结，作者：爱者之贻](https://zhuanlan.zhihu.com/p/554814230)
