## 验证方法

Holdout 检验

交叉检验

**自助法（Bootstrap）**：有放回地从 N 个样本中抽样 n 个样本。当样本规模比较小的时候，将样本集进行划分会让训练集进一步减小，这可能会影响模型训练效果。自助法是基于自助采样的检验方法。在 n 次采样过程中，有的样本会被重复采样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集，进行模型验证，这就是自助法的验证过程。

**交并比（Intersection over Union，IoU）**：交并比 IoU 衡量的是两个区域的重叠程度，是两个区域的交集比上并集。在目标检测任务重，如果模型输出的矩形框与人工标注的矩形框 IoU 值大于某个阈值（通常为 0.5）时，即认为模型输出正确。

**精准率与召回率（Precision & Recall）**：在目标检测中，假设有一组图片，Precision 代表我们模型检测出来的目标有多少是真正的目标物体，Recall 就是所有真实的目标有多少比例被模型检测出来了。目标检测中的真正例（True Positive）、真负例（True Negative）、假正例（False Positive）、假负例（False Positive）的定义如下：

|              | 实际为正 | 实际为负 |
| :----------: | :------: | :------: |
| **预测为正** |    TP    |    FP    |
| **预测为负** |    FN    |    TN    |

对于这四个指标可以这样去理解，后面的 Positive 和 Negative 以预测的结果为主，因为我们关注的是模型的预测，如果模型的预测与实际的标注不一样，那么这个预测就是「假的」，比如预测为负那么就称为 Negative，但是实际为正，与预测的不一样，那么就是「假的」False，因此这个预测就是 False Negative，这是一个「假的正例」是「错误的正例」。

精准率，就是在预测为正样本中实际为正样本的概率，也就是所有的 Positive 中 True Positive 的概率

$Precision = \frac{TP}{TP+FP},$

召回率，就是在实际为正样本中预测为正样本的概率，就是所有的实际标注为正样本的（TP + FN）预测为正样本的概率（TP）

$Recall = \frac{TP}{TP+FN},$

准确率，就是模型预测正确的（所有的 True：TP + TN）占全部的比例

$Accuracy = \frac{TP+TN}{TP+TN+FP+FN},$

**平均精度（Average precision，AP）**：是主流的目标检测模型评价指标，它的意思是不同召回率上的平均精度。我们希望训练好的模型 Precision 和 Recall 都越高越好，但是这两者之间有个矛盾，当 Recall 很小的时候 Precision 可能会很高，当 Recall 很大的时候，Precision 可能会很低。我们将不同 Recall 对应的 Precision 做一个曲线（PR 曲线），然后在这个曲线上计算 Precision 的均值。

**曲线下面积（Area Under Curve，AUC）**：

## 优化算法

### 损失函数总结

为了刻画模型输出与样本标签的匹配程度，定义损失函数 $L(\cdot,\cdot):Y\times Y\rightarrow \mathbb{R}_{\ge 0}$，$L(f(x_i,\theta),y_i)$ 越小，表明模型在该样本点匹配得越好。

> 为了具有更加简介的表达，将网络的输出表示为 $f$，而实际标签表达为 $y$。

在分类问题上常用的损失函数：

（1）**$0-1$ 损失函数**：最常用于二分类问题，$Y=\{1,-1\}$，我们希望 $\texttt{sign}\, f(x_i,\theta)=y_i$，所以 $0-1$ 损失函数为

$L_{0-1}(f,y) = 1_{fy\le 0},$

其中 $1_{P}$ 是指示函数（Indicator Function），当且仅当 $P$ 为真时取值为 1，否则取值为 0。$0-1$ 损失的优点是可以直观地刻画分类的错误率，缺点是由于其非凸、非光滑的特点，算法很难对该函数进行优化。

（2）**Hinge 损失函数**：是 $0-1$ 损失函数相对紧的凸上界，且当 $fy\ge 1$ 时，函数不对其做任何惩罚。它在 $fy=1$ 处不可导，不能够用梯度下降法进行优化，而是用次梯度下降法（Subgradient Descent Method）。适用于 Maximum-Margin 分类，主要用于支持向量机（SVM）中，用来解间距最大化的问题。

$L_{\text{hinge}}(f,y)=\max\{0,1-fy\}.$

（3）**感知损失函数（Perceptron Loss）**：是 Hinge 损失函数的一个变种。Hinge 对判定边界附近的点（正确端）惩罚力度很高，但是 Perceptron 只要样本的判定类别正确就行，不管其判定边界的距离。它比 Hinge 更加简单，不是 Max-margin Boundary，所以模型的泛化能力没有 Hinge 强。

$L_{\text{Perceptron}}=\max(0, -f).$

（4）**Logistic 损失函数**：是 $0-1$ 损失函数的凸上界，该函数处处光滑，对所有的样本点都有所惩罚，因此对异常值相对更敏感一点。

$L_{\text{logistic}}(f,y)=\log_2(1+\exp(-fy)).$

（5）**Log 对数损失函数**：即对数似然损失（Log-likelihood Loss），它的标准形式

$L_{\text{log}}(f(\boldsymbol{x};\theta),y)=-\log f_y(\boldsymbol{x};\theta),$

其中 $f_y(\boldsymbol{x};\theta)$ 可以看作真实类别 $y$ 的似然函数。

（6）**交叉熵（Cross Entropy）损失函数**：对于两个概率分布，一般可以用交叉熵去衡量它们的差异。标签的真实分布 $\boldsymbol{y}$ 和模型预测分布 $f(\boldsymbol{x};\theta)$ 之间的交叉熵为

$\mathcal{L}(f(\boldsymbol{x};\theta),\boldsymbol{y})=-\boldsymbol{y}^\top\log f(\boldsymbol{x};\theta)=-\sum_{c=1}^Cy_c\log f_c(\boldsymbol{x};\theta).$

因为 $\boldsymbol{y}$ 为 one-hot 向量，因此交叉熵可以写为

$\mathcal{L}(f(\boldsymbol{x};\theta),\boldsymbol{y})=-\log f_y(\boldsymbol{x};\theta),$

其中 $f(\boldsymbol{x};\theta)$ 可以看作真实类别 $y$ 的似然函数。因此交叉熵损失函数也就是**负对数似然函数（Negative Log-Likelihood）**。

在回归问题中常用的损失函数：

（1）**平方损失（Mean Squared Error）函数**：在回归问题中最常用的损失函数。对于 $Y=\mathbb{R}$，我们希望 $f(x_i,\theta)\approx y_i$

$L_{\text{MSE}}(f,y)=(f-y)^2.$

（2）**绝对损失（Mean Absolute Error）函数**：当预测值距离真实值较远的时候，平方损失函数的惩罚力度大，也就是说它对于异常点比较敏感。如果说平方损失函数是在做均值回归的话，那么绝对损失函数就是在做中值回归，对于异常点更加鲁棒一点。只不过绝对损失函数在 $f=y$ 处无法求导。

$L_{\text{MAE}}(f,y)=|f-y|.$

（3）**Huber 损失函数**：也称为 Smooth L1 Loss， 综合考虑可导性和对异常点的鲁棒性。在 $|f-y|$ 较小的时候为平方损失，比较大的时候为线性损失

$L_{\text{Huber}}(f,y)=\begin{cases}(f-y)^2,\qquad |f-y|\le \delta\\ 2\delta|f-y|-\delta^2,\quad|f-y|>\delta\end{cases}$

（4）**Log-Cosh 损失函数**：

（5）**分位数损失函数**：

### 随机梯度算法

随机梯度下降法本质上是采用迭代方式更新参数，每次迭代在当前位置的基础上，沿着某一方向迈一小步抵达下一位置，不断地重复这个步骤，它的更新公式为

$\theta_{t+1}=\theta_{t} - \eta g_t,$

其中 $\eta$ 是学习率。

**动量（Momentum）方法：**类比中学物理知识，当前梯度就好比当前时刻受力产生的加速度，前一次步长 $v_{t-1}$ 好比前一时刻的速度，当前步长 $v_t$ 好比当前加速度共同作用的结果。这就好比小球有了惯性，而刻画惯性的物理量是动量。模型参数的迭代公式为：

$v_t = \gamma v_{t-1} + \eta g_t,$

$\theta_{t+1} = \theta_t - v_t,$

在这里当前更新步长 $v_t$ 直接依赖于前一次步长 $v_{t-1}$ 和当前梯度 $g_t$，衰减系数 $\gamma$ 扮演了阻力的作用。

**AdaGrad 方法：**在应用中，我们希望更新频率低的参数可以拥有较大的更新步幅，而更新频率高的参数的步幅可以减小，AdaGrad 方法采用「历史梯度平方和」来衡量不同参数的梯度的稀疏性，取值越小表明越稀疏。AdaGrad 借鉴了 $\mathscr{l}_2$ 正则化的思想，每次迭代时自适应地调整每个参数的学习率。这样的方式保证了不同的参数有具有自适应学习率。具体的更新公式表示为：

在第 $t$ 次迭代时，先计算每个参数梯度平方的累计值

$G_t = \sum_{\tau=1}^t \boldsymbol{g}_\tau \odot \boldsymbol{g}_\tau,$

其中 $\odot$ 为按元素乘积，$\boldsymbol{g}_\tau\in \mathbb{R}^{|\theta|}$ 是第 $\tau$ 次迭代时的梯度。参数更新差值为

$\Delta\theta_t=-\frac{\eta}{\sqrt{G_t+\epsilon}}\odot\boldsymbol{g}_t,$

其中 $\alpha$ 是初始学习率，$\epsilon$ 是为了保持数值稳定性而设定的非常小的常数，一般取值为 $e^{-7}\sim e^{-10}$。分母中求和的形式实现了退火过程，意味着随着时间推移，学习速率 $\frac{\eta}{\sqrt{G_t+\epsilon}}$ 越来越小，保证算法的最终收敛。在 AdaGrad 算法中，如果某个参数的偏导数积累比较大，其学习率相对较小；相反如果其偏导数积累较小，其学习率相对较大，但整体是随着迭代次数的增加，学习率逐渐变小。

### Adam 算法

Adam 算法的全称是**自适应动量估计算法**（Adaptive Moment Estimation Algorithm），它将惯性保持和自适应两个优点结合，可以看作是动量法和 RMSprop 算法（或者 AdaGrad 算法）的结合。

它一方面记录梯度的一阶矩（First Moment）$M_t$，即过往梯度与当前梯度的平均，理解为「惯性」，是梯度 $\boldsymbol{g}_t$ 的指数加权平均。

另一方面记录梯度的二阶矩（Second Moment）$G_t$，即过往梯度平方与当前梯度平方的平均，理解为「自适应部分」，是梯度 $\boldsymbol{g}_t^2$ 的指数加权平均。

> 一阶矩可以理解为均值；二阶矩可以理解为未减去均值的方差

$M_t = \beta_1 M_{t-1} + (1 - \beta_1)\boldsymbol{g}_t,$

$G_t = \beta_2 G_{t-1} + (1 - \beta_2)\boldsymbol{g}_t\odot\boldsymbol{g}_t,$

其中 $\beta_1$ 和 $\beta_2$ 分别为两个移动平均的衰减率，通常取值为 $\beta_1 = 0.9$, $\beta_2 = 0.99$. 

Adam 算法考虑了 $M_t, G_t$ 在零初始情况下的偏置矫正。假设 $M_0=0, G_0=0$，那么在迭代初期 $M_t$ 和 $G_t$ 的值会比真实的均值和方差要小，特别是当 $\beta_1$ 和 $\beta_2$ 都接近于 1 时，偏差会很大。具体来说，Adam 算法的更新公式为：

$\hat{M}_t = \frac{M_t}{1 - \beta_1^t},$

$\hat{G}_t = \frac{G_t}{1 - \beta_2^t},$

$\Delta\theta_t = -\frac{\alpha}{\sqrt{\hat{G}_t + \epsilon}} \hat{M}_t,$

其中学习率 $\alpha$ 通常设为 0.001，并且也可以进行衰减，比如 $\alpha_t=\alpha_0/\sqrt{t}$. 

Adam 算法的物理意义：

> 《百面机器学习》163 页

### 逐层归一化

逐层归一化（Layer Normalization）是将传统机器学习中的数据归一化方法应用到深度神经网络中，对神经网络中隐藏的输入进行归一化，使得网络更容易训练。常用的逐层归一化方法有：批量归一化、层归一化、权重归一化和局部响应归一化。

**内部协变量偏移（Internal Covariate Shift）**：当使用随机梯度下降来训练网络时，每次参数更新都会导致该神经层的输入分布发生改变，越高的层，其输入分布会改变得越明显。从机器学习角度来看，如果一个神经层的输入分布发生了改变，那么其参数需要重新学习。

逐层归一化的能够提高训练效率的原因：

（1）**更好的尺度不变性**：把每个神经层的输入分布都归一化为标准正态分布，可以使得每个神经层对其输入具有更好的尺度不变性。不论低层的参数如何变化，高层的输入保持相对稳定。另外，尺度不变性可以使得我们更加高效地进行参数初始化以及超参选择。

（2）**更平滑的优化地形**：逐层归一化一方面可以使得大部分神经层的输入处于不饱和区域，从而让梯度变大，避免梯度消失问题；另一方面还可以使得神经网络的优化地形（Optimization Landscape）更加平滑，以及使梯度变得更加稳定，从而允许我们使用更大的学习率，并提高收敛速度。

**批量归一化（Batch Normalization，BN）方法** 是一种有效的逐层归一化方法，可以对神经网络中任意的中间层进行归一化操作。假设神经网络第 $l$ 层的净输入为 $\boldsymbol{z}^{(l)}$，神经元输出为 $\boldsymbol{a}^{(l)}$，即

$\boldsymbol{a}^{(l)} = f(\boldsymbol{z}^{(l)})=f\left( \boldsymbol{W}\boldsymbol{a}^{(l)} + \boldsymbol{b} \right),$

其中 $f(\cdot)$ 是激活函数，$\boldsymbol{W}, \boldsymbol{b}$ 是神经网络的参数。为了提高优化效率，就要使得净输入 $\boldsymbol{z}^{(l)}$ 的分布一致，比如都归一化到标准正态分布。归一化操作一般应用在仿射变换（Affine Transformation）$\boldsymbol{W}\boldsymbol{a}^{(l)}+\boldsymbol{b}$ 之后，激活函数之前。

为了提高归一化效率，一般使用标准化将净输入 $\boldsymbol{z}^{(l)}$ 的每一维都归一化到标准正态分布

$\hat{\boldsymbol{z}}^{(l)} = \frac{\boldsymbol{z}^{(l)}-\mathbb{E}[\boldsymbol{z}^{(l)}]}{\sqrt{\text{var}(\boldsymbol{z}^{(l)})+\epsilon}},$

其中 $\mathbb{E}[\boldsymbol{z}^{(l)}]$ 和 $\text{var}(\boldsymbol{z}^{(l)})$ 是当前参数下 $\boldsymbol{z}^{(l)}$ 的每一维在整个训练集上的期望和方差。



## 决策树与集成学习

### 决策树（Decision Tree）

一颗决策树包含一个根结点、若干个内部结点和若干个叶结点；

- 叶结点对应于决策结果，其他每个结点则对应于一个属性测试；
- 每个结点包含的样本集合根据属性测试的结果被划分到子结点中；
- 根结点包含样本全集；

决策树的生成是一个递归过程，遵循「分治策略」（divide-and-conquer）。

决策树基本算法中导致递归返回的三种情况：

1. 当前结点包含的样本全部属于同一个类别；
2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分。在这种情况下，把当前结点标记为叶结点，并将其类别设定为该结点所含样本最多的类别。这相当于利用了当前结点的后验分布；
3. 当前结点包含的样本集合为空，不能划分。在这种情况下，同样把当前结点标记为叶结点，但将其类别设定为其父结点所含样本最多的类别；

信息熵（Information Entropy）：是度量样本集合纯度最常用的一种指标。假定当前样本集合 $D$ 中第 $k$ 类样本所占的比例为 $p_k (k=1,2,\dots,|\mathcal{Y}|)$，则 $D$ 的信息熵为：

$\text{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y}|}p_k\log_2p_k,$

如果 $\text{Ent}(D)$ 的值越小，则 $D$ 的纯度越高，纯度越高说明包含的信息量越少。$\text{Ent}(D)\in[0,\log_2|\mathcal{Y}|]$.

> 注意：计算信息熵时约定，若 $p=0$，则 $p\log_2p=0$，这与极限一致 $\lim_{p\rightarrow 0^{+}}p\log p=0$。

信息增益（Information Gain）：一般而言，信息增益越大，表示用属性 $a$ 来进行划分所获得的「纯度提升」越大。

假定离散属性 $a$ 有 $V$ 个可能的取值 $\{a^1, a^2,\dots,a^V\}$，如果使用 $a$ 来对样本集 $D$ 进行划分，则会产生 $V$ 个分支结点，其中第 $v$ 个分支结点包含了 $D$ 中所有在属性 $a$ 上取值为 $a^v$ 的样本，记为 $D^v$.

因此用属性 $a$ 对样本集 $D$ 进行划分所获得的信息增益为：

$\text{Gain}(D, a)=\text{Ent}(D)-\sum_{v=1}^{V}\frac{|D^v|}{|D|}\text{Ent}(D^v).$

其中考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重 $|D^v|/|D|$，表示样本数越多的分支结点的影响越大。

> 著名的决策树算法 ID3（Iterative Dichotomiser，迭代二分器）就是以信息增益为准则来选择划分属性的。

以信息增益作为准则来生成决策树有一个缺点，就是信息增益对可取值数目较多的属性有更大的偏好，一个更好的方法就是使用「增益率」（Gain Ratio）来选择最优划分属性，著名的 C4.5 决策树算法就利用增益率来生成决策树。增益率的定义为：

$\text{Gain\_ratio}(D, a)=\frac{\text{Gain(D, a)}}{\text{IV}(a)},$

其中

$\text{IV}(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|},$

称为属性 $a$ 的「固有值」（intrinsic value），属性 $a$ 的可能取值书目越多，$V$ 就会越大，$\text{IV}(a)$ 的值通常就会越大。

但是增益率准则带来的一个缺点又变成了对可取值数目较少的属于更加偏好，因此 C4.5 算法并不直接选择增益率最大的候选划分属性，它的具体做法是：

- 先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的属性作为「最优划分属性」。

基尼指数（Gini index）：反映了从数据集 $D$ 中随机抽取两个样本，其类别标记不一致的概率。因此 $\text{Gini}(D)$ 越小，则数据集 $D$ 的纯度越高。属性 $a$ 的基尼指数定义为：

$\text{Gini\_index}(D, a)=\sum_{v=1}^V\frac{|D^v|}{|D|}\text{Gini}(D^v).$

在候选属性集合 $A$ 中，选择那个使得划分后基尼指数最小的属性作为划分属性。

> CART 决策树（Classification and Regression Tree）就是使用基尼指数来选择划分属性的。



剪枝（pruning）是决策树学习算法对付「过拟合」的主要手段，基本策略有「预剪枝」（pre-pruning）和「后剪枝」（post-pruning）。



### Boosting 与 Bagging

机器学习问题的两种策略：一种是研发人员尝试各种模型，选择其中表现最好的模型，做重点调参优化；另一种是将多个分类器的结果统一成一个最终的决策，其中每个单独的分类器称为**基分类器**，使用这类策略的机器学习方法统称为**集成学习**。

集成学习分为 Boosting 和 Bagging 两种。**Boosting 方法**训练基分类器时采用串行方式，各个基分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。**Bagging** 与 Boosting 的串行训练方式不同，Bagging 方法在训练过程中，各基分类器之间无强依赖，可以进行并行训练。最著名的算法之一就是基于决策树基分类器的**随机森林（Random Forest）**。Bagging 方法更像是一个集体决策的过程，每个个体都进行单独学习，在最终做决策时，每个个体单独做出判断，再通过投票的方式做出最后的集体决策。

**基分类器**，有时候又被称为弱分类器。基分类器的错误，是偏差和方差两种错误之和。偏差主要是由于分类器的表达能力有限导致的系统性错误，表现在训练误差不收敛，方差是由于分类器对于样本分布过于敏感，导致在训练样本数较少时，产生过拟合。而 Boosting 方法通过逐步聚焦于基分类器分错的样本，减小集成分类器的偏差。Bagging 方法则是采取分而治之的策略，通过对训练样本多次采样，并分别训练出多个不同模型，然后做综合，来减小集成分类器的方差。

最常用的基分类器是决策树：

- 决策树可以较为方便地将样本的权重整合到训练过程当中，而不需要使用过采样的方法来调整样本权重；
- 决策树的表达能力和泛化能力，可以通过调节树的层数来做折中；
- 数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性较大，这样的「不稳定学习期」更适合作为基分类器。（在这个点上，神经网络也因为不稳定性而适合作为基分类器，可以通过调节神经元数量、连接方式、网络层数、初始权值等方式引入随机性）；

**集成学习的基本步骤**。集成学习一般可以分为以下 3 个步骤：

（1）找到误差互相独立的基分类器；

（2）训练基分类器；

（3）合并基分类器的结果；

合并基分类器的方法有 voting 和 stacking 两种，前者对应 Bagging 方法，后者对应 Boosting 方法。

以 Adaboost 为例，其基分类器的训练和合并的基本步骤如下：

（1）确定基分类器：可以选择 ID3 决策树作为基分类器。虽然任何分类模型都可以作为基分类器，但树形模型由于结构简单且较为容易产生随机性所以比较常用。

（2）训练基分类器：假设训练集为 $\{x_i,y_i\},i=1,\dots,N$，其中 $y_i\in\{-1,1\}$，并且有 $T$ 个基分类器，则可以按照如下过程来训练基分类器：

- 初始化采样分布 $D_1(i)=1/N$；

- 令 $t=1,2,\dots,T$ 循环：

  - 从训练集中，按照 $D_t$ 分布，采样出子集 $S_i=\{x_i,y_i\},i=1,\dots,N$；

  - 用 $S_i$ 训练出基分类器 $h_t$；

  - 计算基分类器 $h_t$ 的错误率：

    $\varepsilon_t=\frac{\sum_{i=1}^{N_t}I[h_t(x_i)\neq y_i]D_i(x_i)}{N_t}$

    其中 $I[\cdot]$ 为判别函数；

  - 计算基分类器 $h_t$ 权重 $a_t=\log{\frac{(1-\varepsilon_t)}{\varepsilon_t}}$，这里可以看到错误率 $\varepsilon_t$ 越大，基分类器的权重 $a_t$ 就越小；

  - 设置下一次采样：
    
    $D_{t+1}=\begin{cases}D_t(i) \text{ or } \frac{D_t(i)(1-\varepsilon_t)}{\varepsilon_t}, \, h_t(x_i)\neq y_i;\\
    \frac{D_t(i)\varepsilon_t}{(1-\varepsilon_t)}, \, h_t(x_i)= y_i.\end{cases}$

（3）合并基分类器：给定一个未知样本 $z$，输出分类结果为加权投票的结果 $\text{Sign}(\sum_{t=1}^Th_t(z)a_t)$.

### 梯度提升决策树（GBDT）

### XGBoost

XGBoost 是陈天奇等人开发的一个开源机器学习项目，高效地实现了 GBDT 算法并进行了算法和工程上的许多改进，被广泛应用在 Kaggle 竞赛以及其他许多机器学习竞赛中。

XGBoost 本质上还是一个 GBDT（Gradient Boosting Decision Tree），只是把速度和效率发挥到极致，所以前面加上了 X（代表 Extreme）。原始的 GBDT 算法基于经验损失函数的负梯度来构造新的决策树，只是在决策树构建完成后再进行剪枝。XGBoost 在决策树构建阶段就加入了正则项，即

$L_t=\sum_i l\left(y_i,\, F_{t-1}(x_i)+f_t(x_i)\right)+\Omega(f_t),$

其中 $F_{t-1}(x_i)$ 表示现有的 $t-1$ 棵树最优解，树结构的正则项定义为

$\Omega(f_t)=\gamma T+\frac{1}{2}\lambda\sum_{j=1}^Tw^2_j,$

其中 $T$ 为叶子节点个数，$w_j$ 表示第 $j$ 个叶子节点的预测值。对该损失函数在 $F_{t-1}$ 处进行二阶泰勒展开可以推导出

$L_t\approx\overset{\sim}{L_t}=\sum_{j=1}^T\left[G_jw_j+\frac{1}{2}(H_j+\lambda)w^2_j\right]+\gamma T$

从所有的树结构中寻找最优的树结构是一个 NP-hard 问题，在实际中往往采用贪心法来构建出一个次优的树结构，基本思想是根据特定的准则选取最优的分裂。不同的决策树算法采用不同的准则，如 IC3 算法采用信息增益，C4.5 算法为了克服信息增益中容易偏向取值较多的特征而采用信息增益比，CART 算法使用基尼指数和平方误差，XGBoost 也有特定的准则来选取最优分裂。

XGBoost 与 GBDT 的区别和联系：

（1）GBDT 是机器学习算法，XGBoost 是该算法的工程实现；

（2）在使用 CART 作为基分类器时，XGBoost 显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力；

（3）GBDT 在模型训练时只使用了代价函数的一阶导数信息，XGBoost 对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数；

（4）传统的 GBDT 采用 CART 作为基分类器，XGBoost 支持多种类型的基分类器，比如线性分类器；

（5）传统的 GBDT 在每轮迭代时使用全部的数据，XGBoost 则采用了与随机森林相似的策略，支持对数据进行采样；

（6）传统的 GBDT 没有设计对缺失值进行处理，XGBoost 能够自动学习出缺失值的处理策略；

**XGBoost 的并行化：**boosting 是一种串行结构，它的并行不是在 tree 粒度上的，而是在特征粒度上的并行。决策树学习最耗时的一个步骤就是对特征的值进行排序（为了确定最佳分割点）。XGBoost 训练之前，预先对数据进行排序，保存为 block 结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

**XGBoost 的特点：**

- 传统的 GBDT 以 CART 作为基函数，而 XGBoost 相当于有 L1/L2 正则化项的分类或者回归
- 传统的 GBDT 在优化的时候只用到一阶导数，XGBoost 对代价函数进行了二阶泰勒展开，同时用到一阶和二阶导数。并且 XGBoost 工具支持自定义代价函数，只要函数可以一阶和二阶求导；
- XGBoost 在代价函数里加入了正则项，控制模型复杂度。正则项里包含了树的叶节点个数、每个叶子节点上输出 score 的 L2 模的平方和。从 Bias-variance tradeoff 角度来讲，正则项降低了模型 variance，使学习出来的模型更加简单，防止过拟合，这也是 XGBoost 优于传统 GBDT 的一个特性。 剪枝是都有的，叶子节点输出 L2 平滑是新增的；
- shrinkage 缩减和 column subsampling。shrinkage 缩减：类似于学习速率，在每一步 tree boosting 之后增加了一个参数 n（权重），通过这种方式来减小每棵树的影响力，给后面的树提供空间去优化模型。column subsampling：列（特征）抽样，随机森林那边学习来的，防止过拟合的效果比传统的行抽样还好（行抽样功能也有），并且有利于后面提到的并行化处理算法；
- split finding algorithms（划分点查找算法），树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法 greedy algorithm 枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以 XGBoost 还提出了一种可并行的近似直方图算法（Weighted Quantile Sketch），用于高效地生成候选的分割点；
- 对缺失值的处理。对于特征的值有缺失的样本，XGBoost 可以自动学习出它的分裂方向。 稀疏感知算法 Sparsity-aware Split Finding；
- 内置交叉验证（Built-in Cross-Validation），XGBoost 可以在 boosting 过程的每次迭代中运行交叉验证，因此很容易在一次运行中获得准确的最佳 boosting 迭代次数；
- XGBoost 支持并行，提高计算速度；

### LightGBM

LightGBM 是 XGBoost 的更高效实现，由微软发布。LightGBM 相比于 Xgboost，添加了很多新的方法来改进模型，包括：并行方案、基于梯度的单边检测（GOSS）、排他性特征捆绑等。

LightGBM 的设计思路主要是两点：

1. 减小数据对内存的使用，保证单个机器在不牺牲速度的情况下，尽可能地用上更多的数据；
2. 减小通信的代价，提升多机并行时的效率，实现在计算上的线性加速。

由此可见，LightGBM 的设计初衷就是提供一个快速高效、低内存占用、高准确度、支持并行和大规模数据处理的数据科学工具。

LightGBM 并没有垂直的切分数据集，而是每个 worker 都有全量的训练数据，因此最优的特征分裂结果不需要传输到其他 worker 中，只需要将最优特征以及分裂点告诉其他 worker，worker 随后本地自己进行处理。处理过程如下：

- 每个 worker 在基于局部的特征集合找到最优分裂特征；
- worker 间传输最优分裂信息，并得到全局最优分裂信息；
- 每个 worker 基于全局最优分裂信息，在本地进行数据分裂，生成决策树；

## 其他知识

HMM：EM 算法、维特比算法、前向后向算法、极大似然估计

在 HMM 中，如果已知观察序列和产生观察序列的状态序列，可以用极大似然估计进行阐述估计。

EM 算法只有观测序列，无状态序列时来学习模型参数，即 Baum-Welch 算法。

维特比算法是用动态规划解决 HMM 的预测问题的，不是参数估计。

前向后向算法是用来计算概率的。

极大似然估计是观测序列和相应序列都存在时的监督学习算法，用来进行阐参数估计。

序列模式挖掘算法、AprioriAll 算法、GSP 算法、FreeSpan 算法、PrefixSpan 算法

Apriori 算法：关联分析原始算法，用于从候选项集中发现频繁项集。两个步骤：进行自连接、进行剪枝。缺点：无时序先后性。

AprioriAll 算法：AprioriAll 算法与 Apriori 算法的执行过程是一样的，不同点在于候选集的产生，需要区分最后两个元素的前后。

AprioriSome 算法：可以看做是 AprioriAll 算法的改进

AprioriAll 算法和 AprioriSome 算法的比较：

（1）AprioriAll 用  去计算出所有的候选 Ck，而 AprioriSome 会直接用  去计算所有的候选 ，因为 包含 ，所以 AprioriSome 会产生比较多的候选。

（2）虽然 AprioriSome 跳跃式计算候选，但因为它所产生的候选比较多，可能在回溯阶段前就占满内存。

（3）如果内存占满了，AprioriSome 就会被迫去计算最后一组的候选。

（4）对于较低的支持度，有较长的大序列，AprioriSome 算法要好些。

GPS算法：类Apriori算法。用于从候选项集中发现具有时序先后性的频繁项集。两个步骤：进行自连接、进行剪枝。缺点：每次计算支持度，都需要扫描全部数据集；对序列模式很长的情况，由于其对应的短的序列模式规模太大，算法很难处理。

SPADE算法：改进的GPS算法，规避多次对数据集D进行全表扫描的问题。与GSP算法大体相同，多了一个ID_LIST记录，使得每一次的ID_LIST根据上一次的ID_LIST得到（从而得到支持度）。而ID_LIST的规模是随着剪枝的不断进行而缩小的。所以也就解决了GSP算法多次扫描数据集D问题。

FreeSpan算法：即频繁模式投影的序列模式挖掘。核心思想是分治算法。基本思想为：利用频繁项递归地将序列数据库投影到更小的投影数据库集中，在每个投影数据库中生成子序列片断。这一过程对数据和待检验的频繁模式集进行了分割，并且将每一次检验限制在与其相符合的更小的投影数据库中。
优点：减少产生候选序列所需的开销。缺点：可能会产生许多投影数据库，开销很大，会产生很多的

PrefixSpan 算法：从FreeSpan中推导演化而来的。收缩速度比FreeSpan还要更快些。

**模型过拟合**：

原因：（1）训练数据太少；（2）模型太复杂；（3）参数过多；（4）噪声过多。

解决办法：（1）获得更多的训练数据；（2）降低特征维度；（3）正则化；（4）Dropout；（5）早停 Early Stop；（6）数据清洗。

---

**参考**

[1] [GitHub 项目：ML-NLP](https://github.com/NLP-LOVE/ML-NLP)；

[2] [XGBoost 特点、调参、讨论](https://blog.csdn.net/niaolianjiulin/article/details/76574216)；

[3] 诸葛越，葫芦娃，《百面机器学习》，中国工信出版集团，人民邮电出版社

