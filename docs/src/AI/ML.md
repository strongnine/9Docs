## 验证方法

Holdout 检验

交叉检验

**自助法（Bootstrap）**：有放回地从 N 个样本中抽样 n 个样本。当样本规模比较小的时候，将样本集进行划分会让训练集进一步减小，这可能会影响模型训练效果。自助法是基于自助采样的检验方法。在 n 次采样过程中，有的样本会被重复采样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集，进行模型验证，这就是自助法的验证过程。

## 集成学习

### Boosting 与 Bagging

机器学习问题的两种策略：一种是研发人员尝试各种模型，选择其中表现最好的模型，做重点调参优化；另一种是将多个分类器的结果统一成一个最终的决策，其中每个单独的分类器称为**基分类器**，使用这类策略的机器学习方法统称为**集成学习**。

集成学习分为 Boosting 和 Bagging 两种。**Boosting 方法**训练基分类器时采用串行方式，各个基分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。**Bagging** 与 Boosting 的串行训练方式不同，Bagging 方法在训练过程中，各基分类器之间无强依赖，可以进行并行训练。最著名的算法之一就是基于决策树基分类器的**随机森林（Random Forest）**。Bagging 方法更像是一个集体决策的过程，每个个体都进行单独学习，在最终做决策时，每个个体单独做出判断，再通过投票的方式做出最后的集体决策。

**基分类器**，有时候又被称为弱分类器。基分类器的错误，是偏差和方差两种错误之和。偏差主要是由于分类器的表达能力有限导致的系统性错误，表现在训练误差不收敛，方差是由于分类器对于样本分布过于敏感，导致在训练样本数较少时，产生过拟合。而 Boosting 方法通过逐步聚焦于基分类器分错的样本，减小集成分类器的偏差。Bagging 方法则是采取分而治之的策略，通过对训练样本多次采样，并分别训练出多个不同模型，然后做综合，来减小集成分类器的方差。

最常用的基分类器是决策树：

- 决策树可以较为方便地将样本的权重整合到训练过程当中，而不需要使用过采样的方法来调整样本权重；
- 决策树的表达能力和泛化能力，可以通过调节树的层数来做折中；
- 数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性较大，这样的「不稳定学习期」更适合作为基分类器。（在这个点上，神经网络也因为不稳定性而适合作为基分类器，可以通过调节神经元数量、连接方式、网络层数、初始权值等方式引入随机性）；

**集成学习的基本步骤**。集成学习一般可以分为以下 3 个步骤：

（1）找到误差互相独立的基分类器；

（2）训练基分类器；

（3）合并基分类器的结果；

合并基分类器的方法有 voting 和 stacking 两种，前者对应 Bagging 方法，后者对应 Boosting 方法。

以 Adaboost 为例，其基分类器的训练和合并的基本步骤如下：

（1）确定基分类器：可以选择 ID3 决策树作为基分类器。虽然任何分类模型都可以作为基分类器，但树形模型由于结构简单且较为容易产生随机性所以比较常用。

（2）训练基分类器：假设训练集为 $\{x_i,y_i\},i=1,\dots,N$，其中 $y_i\in\{-1,1\}$，并且有 $T$ 个基分类器，则可以按照如下过程来训练基分类器：

- 初始化采样分布 $D_1(i)=1/N$；

- 令 $t=1,2,\dots,T$ 循环：

  - 从训练集中，按照 $D_t$ 分布，采样出子集 $S_i=\{x_i,y_i\},i=1,\dots,N$；

  - 用 $S_i$ 训练出基分类器 $h_t$；

  - 计算基分类器 $h_t$ 的错误率：
    $$
    \varepsilon_t=\frac{\sum_{i=1}^{N_t}I[h_t(x_i)\neq y_i]D_i(x_i)}{N_t}
    $$
    其中 $I[\cdot]$ 为判别函数；

  - 计算基分类器 $h_t$ 权重 $a_t=\log{\frac{(1-\varepsilon_t)}{\varepsilon_t}}$，这里可以看到错误率 $\varepsilon_t$ 越大，基分类器的权重 $a_t$ 就越小；

  - 设置下一次采样：
    $$
    D_{t+1}=\begin{cases}D_t(i) \text{ or } \frac{D_t(i)(1-\varepsilon_t)}{\varepsilon_t}, \, h_t(x_i)\neq y_i;\\
    \frac{D_t(i)\varepsilon_t}{(1-\varepsilon_t)}, \, h_t(x_i)= y_i.\end{cases}
    $$

（3）合并基分类器：给定一个未知样本 $z$，输出分类结果为加权投票的结果 $\text{Sign}(\sum_{t=1}^Th_t(z)a_t)$.

### 梯度提升决策树（GBDT）

### XGBoost

XGBoost 是陈天奇等人开发的一个开源机器学习项目，高效地实现了 GBDT 算法并进行了算法和工程上的许多改进，被广泛应用在 Kaggle 竞赛以及其他许多机器学习竞赛中。

XGBoost 本质上还是一个 GBDT（Gradient Boosting Decision Tree），只是把速度和效率发挥到极致，所以前面加上了 X（代表 Extreme）。原始的 GBDT 算法基于经验损失函数的负梯度来构造新的决策树，只是在决策树构建完成后再进行剪枝。XGBoost 再决策树构建阶段就加入了正则项，即
$$
L_t=\sum_i l\left(y_i,\, F_{t-1}(x_i)+f_t(x_i)\right)+\Omega(f_t),
$$
其中 $F_{t-1}(x_i)$ 表示现有的 $t-1$ 棵树最优解，树结构的正则项定义为
$$
\Omega(f_t)=\gamma T+\frac{1}{2}\lambda\sum_{j=1}^Tw^2_j,
$$
其中 $T$ 为叶子节点个数，$w_j$ 表示第 $j$ 个叶子节点的预测值。对该损失函数在 $F_{t-1}$ 处进行二阶泰勒展开可以推导出
$$
L_t\approx\overset{\sim}{L_t}=\sum_{j=1}^T\left[G_jw_j+\frac{1}{2}(H_j+\lambda)w^2_j\right]+\gamma T
$$
$\cdots$ ***（之后的内容有很多偏数学的，先「不求甚解」，到时候用到的时候详细再去看看《百面机器学习》第 12 章第 06 个问题）***

从所有的树结构中寻找最优的树结构是一个 NP-hard 问题，在实际中往往采用贪心法来构建出一个次优的树结构，基本思想是根据特定的准则选取最优的分裂。不同的决策树算法采用不同的准则，如 IC3 算法采用信息增益，C4.5 算法为了克服信息增益中容易偏向取值较多的特征而采用信息增益比，CART 算法使用基尼指数和平方误差，XGBoost 也有特定的准则来选取最优分裂。

XGBoost 与 GBDT 的区别和联系：

（1）GBDT 是机器学习算法，XGBoost 是该算法的工程实现；

（2）在使用 CART 作为基分类器时，XGBoost 显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力；

（3）GBDT 在模型训练时只使用了代价函数的一阶导数信息，XGBoost 对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数；

（4）传统的 GBDT 采用 CART 作为基分类器，XGBoost 支持多种类型的基分类器，比如线性分类器；

（5）传统的 GBDT 在每轮迭代时使用全部的数据，XGBoost 则采用了与随机森林相似的策略，支持对数据进行采样；

（6）传统的 GBDT 没有设计对缺失值进行处理，XGBoost 能够自动学习出缺失值的处理策略；

### XGBoost 的并行化

boosting 是一种串行结构，它的并行不是在 tree 粒度上的，而是在特征粒度上的并行。决策树学习最耗时的一个步骤就是对特征的值进行排序（为了确定最佳分割点）。XGBoost 训练之前，预先对数据进行排序，保存为 block 结构，后面的迭代中重复地使用这个结构，大大减小计算量。

### XGBoost 的特点

- 传统的 GBDT 以 CART 作为基函数，而 XGBoost 相当于有 L1/L2 正则化项的分类或者回归
- 传统的 GBDT 在优化的时候只用到一阶导数，XGBoost 对代价函数进行了二阶泰勒展开，同时用到一阶和二阶导数。并且 XGBoost 工具支持自定义代价函数，只要函数可以一阶和二阶求导；
- XGBoost 在代价函数里加入了正则项，控制模型复杂度。正则项里包含了树的叶节点个数、每个叶子节点上输出 score 的 L2 模的平方和。从 Bias-variance tradeoff 角度来讲，正则项降低了模型 variance，使学习出来的模型更加简单，防止过拟合，这也是 XGBoost 优于传统 GBDT 的一个特性。 剪枝是都有的，叶子节点输出 L2 平滑是新增的；
- shrinkage 缩减和 column subsampling。shrinkage 缩减：类似于学习速率，在每一步 tree boosting 之后增加了一个参数 n（权重），通过这种方式来减小每棵树的影响力，给后面的树提供空间去优化模型。column subsampling：列（特征）抽样，随机森林那边学习来的，防止过拟合的效果比传统的行抽样还好（行抽样功能也有），并且有利于后面提到的并行化处理算法；
- split finding algorithms（划分点查找算法），树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法 greedy algorithm 枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以 XGBoost 还提出了一种可并行的近似直方图算法（Weighted Quantile Sketch），用于高效地生成候选的分割点；
- 对缺失值的处理。对于特征的值有缺失的样本，XGBoost 可以自动学习出它的分裂方向。 稀疏感知算法 Sparsity-aware Split Finding；
- 内置交叉验证（Built-in Cross-Validation），XGBoost 可以在 boosting 过程的每次迭代中运行交叉验证，因此很容易在一次运行中获得准确的最佳 boosting 迭代次数；
- XGBoost 支持并行，提高计算速度；



---

**参考**

[1] [GitHub 项目：ML-NLP](https://github.com/NLP-LOVE/ML-NLP)；

[2] [XGBoost 特点、调参、讨论](https://blog.csdn.net/niaolianjiulin/article/details/76574216)；

[3] 诸葛越，葫芦娃，《百面机器学习》，中国工信出版集团，人民邮电出版社

