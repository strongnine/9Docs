var documenterSearchIndex = {"docs":
[{"location":"Linux/#Linux-命令","page":"Linux","title":"Linux 命令","text":"","category":"section"},{"location":"Linux/","page":"Linux","title":"Linux","text":"创建文件夹：mkdir 可以创建一级目录，如果要创建更深的目录（碰到不存在的文件夹自动创建可以使用 mkdir -p）","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"查看权限：","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"查看文件权限：ls -l\n查看所在文件夹权限：ls -ld\n修改文件夹权限：chmod xxx dir/file 其中 xxx 不同的数字代表不同的权限\n600 只有所有者有读和写的权限；644 所有者有读和写的权限，组用户只有读的权限；700 只有所有者有读和写以及执行的权限；666 每个人都有读和写的权限；777 每个人都有读和写以及执行的权限；","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"移动文件或者目录：mv 原路径 目标路径","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"复制文件：cp 原路径 目标路径","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"查看 CUDA 和 cuDNN 的版本：","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"1、方法一：查看 CUDA 版本 nvcc --version 或 nvcc -V；方法二：cat /usr/local/cuda/version.json","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"2、查看 cuDNN 版本：cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"修改 /tmp 目录：","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"仅对当前终端有效：··","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"$ mkdir -p $HOME/tmp  # 在 HOME 目录创建一个 tmp 文件夹\n$ export TMPDIR=$HOME/tmp  # 指定","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"永久生效：","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"$ mkdir -p $HOME/tmp\n$ echo \"export TMPDIR=$HOME/tmp\" >> ~/.bashrc\n$ source ~/.bashrc","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"ncdu","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"查看文件大小：https://blog.51cto.com/u_14691718/3432088","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"创建软连接：","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"查看已经挂载的硬盘：df -T，其中 -T 表示显示文件类型","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"查看磁盘占用情况：df -h","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"查看所有被系统识别的磁盘：fdisk -l","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"挂载 device 到 dir：mount -t <type> <device> <dir>","category":"page"},{"location":"Linux/#终端复用器-Tmux","page":"Linux","title":"终端复用器 Tmux","text":"","category":"section"},{"location":"Linux/","page":"Linux","title":"Linux","text":"推荐阅读：Tmux 使用教程","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"Tmux 是一个终端复用器（terminal multiplexer），在不同系统上安装的方式如下：","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"# Ubuntu 或 Debian\n$ sudo apt-get install tmux\n\n# CentOS 或 Fedora\n$ sudo yum install tmux\n\n# Mac\n$ brew install tmux","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"输入 tmux 就可以开始使用，输入 exit 或者 Ctrl + d 就可以退出 Tmux 窗口：","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"新建一个指定名字的会话：tmux new -s <session-name>","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"重新接入某个已存在的会话：tmux attach -t <session-name>","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"将当前会话与窗口分离：tmux detach 或者 快捷键 Ctrl + b, d","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"查看当前所有的 Tmux 会话：tmux ls 或者 tmux attach -t <session-name>","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"关掉某个会话：tmux kill-session -t 0 或者 tmux kill-session -t <session-name>","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"切换会话：tmux switch -t <session-name>","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"重命名会话：tmux rename-session -t 0 <new-name>","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"重命名当前窗口：tmux rename-window <new-name>","category":"page"},{"location":"Linux/#段错误","page":"Linux","title":"段错误","text":"","category":"section"},{"location":"Linux/","page":"Linux","title":"Linux","text":"https://komodor.com/learn/sigsegv-segmentation-faults-signal-11-exit-code-139/","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"https://tldp.org/FAQ/sig11/html/index.html","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"https://stackoverflow.com/questions/26521401/segmentation-fault-signal-11","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"https://github.com/JuliaLang/julia/issues/35005","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"https://discourse.julialang.org/t/signal-11-segmentation-fault-11-with-differentialequations-jl/23264","category":"page"},{"location":"Linux/","page":"Linux","title":"Linux","text":"https://github.com/JuliaLang/julia/issues/31758","category":"page"},{"location":"AI/Math/#数学基础","page":"数学基础","title":"数学基础","text":"","category":"section"},{"location":"AI/Math/#信息论","page":"数学基础","title":"信息论","text":"","category":"section"},{"location":"AI/Math/","page":"数学基础","title":"数学基础","text":"熵（Entropy）早","category":"page"},{"location":"AI/CNN/#卷积神经网络","page":"卷积神经网络","title":"卷积神经网络","text":"","category":"section"},{"location":"AI/CNN/#深度卷积神经网络","page":"卷积神经网络","title":"深度卷积神经网络","text":"","category":"section"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"感受野（Receptive Field），指的是神经网络中神经元「看到的」输入区域，在卷积神经网络中，feature map 上某个元素的计算受输入图像上某个区域的影响，这个区域即该元素的感受野。感受野是个相对概念，某层 feature map 上的元素看到前面不同层上的区域范围是不同的，通常在不特殊指定的情况下，感受野指的是看到输入图像上的区域。","category":"page"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"例如两个级联的卷积核大小为 3times 3，stride = 2 的卷积层的感受野为 7times 7，如图所示","category":"page"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"(Image: 感受野)","category":"page"},{"location":"AI/CNN/#卷积详解","page":"卷积神经网络","title":"卷积详解","text":"","category":"section"},{"location":"AI/CNN/#卷积","page":"卷积神经网络","title":"卷积","text":"","category":"section"},{"location":"AI/CNN/#因果卷积（Causal-Convolution）","page":"卷积神经网络","title":"因果卷积（Causal Convolution）","text":"","category":"section"},{"location":"AI/CNN/#空洞卷积","page":"卷积神经网络","title":"空洞卷积","text":"","category":"section"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"空洞卷积具有更大的感受野，有助于构建长期记忆功能。","category":"page"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"变形卷积","category":"page"},{"location":"AI/CNN/#CNN-历史","page":"卷积神经网络","title":"CNN 历史","text":"","category":"section"},{"location":"AI/CNN/#从-LeNet-5-到-ResNet","page":"卷积神经网络","title":"从 LeNet-5 到 ResNet","text":"","category":"section"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"LeNet-5","category":"page"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"AlexNet 首次亮相是在 2012 年的 ILSVRC 大规模视觉识别竞赛上，它的主要成果是将图像分类任务的 Top-5 错误率降低到 15.3%。AlexNet 主要的网络结构是堆砌的卷积层和池化层，最后在网络末端加上全连接层和 Softmax 层来处理多分类任务。AlexNet 的改进：","category":"page"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"采用修正线性单元（Rectified Linear Unit，ReLU）作为激活函数（在此之前常用 Sigmoid 函数），缓解了深层网络训练时的梯度消失问题。\n引入局部响应归一化（Local Response Normalization，LRN）模块。\n应用 Dropout 和数据扩充（data augmentation）技术来提升训练效果。\n用分组卷积来突破当时 GPU 的显存瓶颈。","category":"page"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"AlexNet 输入的图片大小为 227times 227，每个批次只有 1 张图片。","category":"page"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"VGGNet 出现在2014 年的 ILSVRC 上，单个模型就将图像分类任务的 Top-5 错误率降低到 8.0%；如果采用多模型集成（ensemble），则可以将错误率进一步降至 6.8%。VGGNet 的改进：","category":"page"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"VGGNet-16 输入的图片为 224times 224，每个批次有 10 张图片。","category":"page"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"ResNet 的提出是为了解决网络退化（degeneration）的问题。退化是指随着网络层数的加深，网络的训练误差和测试误差都会上升。而过拟合是训练误差降低而测试误差反而升高的现象。ResNet-152 模型在 ImageNet 2012 数据集的图像分类任务上，单模型使得 Top-5 错误率降至 4.49%，采用多模型集成可以进一步将错误率降低到 3.57%。ResNet 的改进：","category":"page"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"增加了跳跃连接（shortcut connection），在网络中构筑多条「近道」：\n缩短误差反向传播到各层的路径，有效抑制梯度消失的现象，使得网络在不断加深时性能不会下降。\n若网络在层数加深时性能退化，则它可以通过控制网络中「近道」和「非近道」的组合比例来退回到浅层时的状态，即「近道」具备自我关闭的能力。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"计算机视觉的任务有：图像分类、物体检测、语义分割、文字识别、人脸识别等","category":"page"},{"location":"AI/CV/#目标检测","page":"计算机视觉","title":"目标检测","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"目标检测学习路径：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"理论：要求能够复现经典论文的代码","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"图像分类：VGG、Inception、ResNet、MobileNet、SENet\n图像分割：UNet、DeepLab 系列、FCN、SegNet、BiSeNet\n目标检测：YOLOv3、Faster R-CNN\nGAN：GAN、DCGAN、Pix2Pix","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"实践：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"数据增强技巧：MixUp、Label Smoothing\n长尾分布（Long-Tail）、渐进式采样（PB-sampling, Progressively-balanced Sampling）\n数据爬取与筛选：常规筛选方法（经典图像处理和分析方法）、高阶筛选方法（model-base，基于内容的筛选）\n语义分割：\n自动驾驶语义分割：CamVid 数据集，训练 UNet、SegNet；deeplabv3+ 进行模型评估和推理\n人像分割：Portrait 数据集；训练 BiseNet；Dice-Loss、CE Dice Loss、Focal Loss\n数据增强工具：Albumentations\n目标检测：\nYOLOX：Neck、Head、正负样本分配方式\nCOCO 数据集：Mosaic、Mixup、Affine 变化等数据增强方法\n轻量级目标检测器：NanoDetPlus\n算法终端部署：OpenVINO","category":"page"},{"location":"AI/CV/#基础概念","page":"计算机视觉","title":"基础概念","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"计算特征图大小：计算经过卷积、池化等操作之后的特征图大小，这是一个十分常见的考题。假设特征图的输入尺寸为 l_i，Padding 大小为 p，卷积核或者池化核大小为 k，步长为 s，那么特征图的输出尺寸 l_o 计算公式为：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"l_o=lfloorfracl_i+2p-ksrfloor+1","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"其中 lfloorcdotrfloor 代表向下取整。很多深度学习框架会采取向下取整的方式，放弃输入特征图的一部分边界数据。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"计算感受野：假设网络的原始输入特征图的尺寸为 L，第 i 层卷积核（池化核）尺寸 k_i，第 j 层的步长为 s_j，则第 i 层的感受野大小 R_i 计算如下","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"R_i=minleft( R_i-1 + (k_i-1)prod_j=0^i-1s_j L right)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"其中对于原始输入层 R_0=1 s_0=1. ","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"目标检测（Object Detection）是计算机视觉中极为重要的基础问题，是实例分割（Instance Segmentation)、场景理解（Secne Understanding）、目标跟踪（Object Tracking）、图像标注（Image Captioning）等问题的基础。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"目标检测任务：给定一张图片，将图片中的每个物体识别出来并且提出一个置信度，用矩形方框（Bounding Box）或者不规则的区域标识出来。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"目标检测模型分为单步（one-stage）模型和两步（two-stage）模型两大类。单步模型在计算效率上有优势，两步模型在检测精度上有优势。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"单步模型是指没有独立地、显式地提取候选区域（region proposal），直接由输入图像得到其中存在的物体的类别和位置信息的模型。例如 OverFeat、SSD（Single Shot multibox-Detector）、YOLO（You Only Look Once） 等模型。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"两步模型是指有独立的、显式的候选区域提取过程，即先在输入图像上筛选出一些可能存在物体的候选区域，然后针对每个候选区域，判断其是否存在物体，如果存在就给出物体的类别和位置修正信息。例如 R-CNN、SPPNet、Fast R-CNN、Faster R-CNN、R-FCN、Mask R-CNN 等模型。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"交并比（Intersection-over-Union，IoU）：即两个 Bounding Boxes 之间交集与并集的比值。对于预测 Bounding Box 与 Ground-truth Box 来说，比值越大代表预测的 Bounding Box 结果越好。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"可以学习一下 IoU 的 Python 代码 IoU_demo.py。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"# 这六行短短的代码可以囊括所有 pred bbox 和 gt bbox 之间的关系。包括相交、不相交、各种相交形式等等\nixmin = max(pred_bbox[0], gt_bbox[0])\niymin = max(pred_bbox[1], gt_bbox[1])\nixmax = min(pred_bbox[2], gt_bbox[2])\niymax = min(pred_bbox[3], gt_bbox[3])\niw = np.maximum(ixmax - ixmin + 1., 0.)\nih = np.maximum(iymax - iymin + 1., 0.)\n\ninters = iw * ih  # 交集\nuni = ((pred_bbox[2] - pred_bbox[0] + 1.) * (pred_bbox[3] - pred_bbox[1] + 1.) +\n           (gt_bbox[2] - gt_bbox[0] + 1.) * (gt_bbox[3] - gt_bbox[1] + 1.) -\n           inters)  # 并集 union = S1 + S2 - inters\n\noverlaps = inters / uni  # IoU","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"均交并比（Mean Intersection over Union, MIoU）：MIoU 是语义分割的标准度量，其计算两个集合的交集和并集之比。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"textMIoU=frac1k+1sum^k_i=0fracp_iisum_j=0^kp_ij+sum_j=0^kp_ji-p_ii","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"其中 p_ij 表示真实值为 i，被预测为 j 的数量。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"(Image: )","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"橙色是真实值，蓝色是预测值，中间是两个部分的相交部分。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"均像素精度（Mean Pixel Accuracy, MPA）：预测正确的部分占整个真实值的比例，或者说真正例占假负例的比例，即面积 3 和面积 1 的比例。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"而 MIoU 就是两个部分交集部分与并集部分的比，越接近 1 证明预测结果越好，最理想的情况是 1. ","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"非极大值抑制（Non-Maximum Suppression，NMS）：目标检测过程中在同一个目标的位置上会产生大量的候选框，这些候选框之间可能会有重叠，NMS 的作用就是消除冗余的边界框，找到最佳的目标边界框。NMS 的流程如下：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"步骤 1. 根据置信度得分进行排序；\n步骤 2. 选择置信度最高的边界框添加到最终输出列表中，将其从边界框列表中删除；\n步骤 3. 计算所有边界框的面积；\n步骤 4. 计算置信度最高的边界框与其他候选框的 IoU；\n步骤 5. 删除 IoU 大于给定阈值的边界框；\n步骤 6. 重复上述过程，直到边界框列表为空；","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"NMS 中的阈值给得越大，则越有可能出现同一个物体有多个边界框的情况。步骤 4 中如果置信度最高的边界框与其他候选框的 IoU 比较大的话，就可以认为这两个边界框中是同一个物体，因此只要留下最大的那一个，把其他的删除了。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"代码：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"out = net(x)  # forward pass, 将图像 x 输入网络，得到 pred cls + reg\nboxes, scores = detector.forward(out, priors)  # 结合 priors，将 pred reg（即预测的 offsets）解码成最终的 pred bbox\nboxes = boxes[0]\nscores = scores[0]\n\n# scale each detection back up to the image\nboxes *= sca;e  # (0, 1) 区间坐标的 bbox 做尺度反正则化\nboxes = boxes.cpu().numpy()\nscores = scores.cpu().numpy()\n\nfor\tj in range(1, num_classes):  # 对每一个类 j 的 pred bbox 单独做 NMS\n    # 因为第 0 类是 background，不用做 NMS，因此 index 从 1 开始\n    inds = np.where(scores[:, j] > thresh)[0]  # 找到该类 j 下，所有 cls score 大于 thresh 的 bbox\n    # score 小于阈值的 bbox 直接过滤掉，不用进行 NMS\n    if len(inds) == 0:  # 没有满足条件的 bbox，返回空，跳过\n        all_boxes[j][i] = np.empty([0, 5], dtype=np.float32)\n        continue\n    c_bboxes = boxes[inds]\n    c_scores = scores[inds, j]  # 找到对应类 j 下的 score 即可\n    c_dets = np.hstack((c_bboxes, c_scores[:, np.newaxis])).astype(np.float32, copy=False)  # 将满足条件的 bbox + cls score 的 bbox 通过 hstack 完成合体\n    \n    keep = nms(c_dets, 0.45, force_cpu=args.cpu)  # NMS，返回需要保存的 bbox index: keep\n    c_dets = c_dets[keep, :]\n    all_boxes[j][i] = c_dets  # i 对应每张图片，j 对应图像中类别 j 的 bbox 清单","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"True Positive (TP)：textIoU  05 的检测框数量（同一个 Ground Truth 只计算一次）","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"False Positive (FP)：textIoUle 05 的检测框（检测到同一个 Ground Truth 的多余检测框的数量","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"False Negative (FN)：没有检测到的 Ground Truth 的数量","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"True Negative (TN)：在 mAP 评价指标中不会使用到","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"查准率（Precision）：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"textPrecision = fractextTP(textTP + textFP) = fractextFPtextall detections","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"查全率、查全率（Recall）：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"textRecall = fractextTP(textTP+textFN)=fractextTPtextall ground truths","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"PR 曲线（Precision-Recall Curve）：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"平均精确度（Average Precision）：PR 曲线下面积","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"mAP（mean Average Precison）：各类别 AP 的平均值。在 VOC2010 以前（VOC07），只要选择当 textRecallge 0 01 02 dots 1 共 11 个点时的 Precision 最大值，然后 AP 就是这 11 个 Precision 的平均值；在 VOC2010 开始，需要针对每一个不同的 Recall 值（包括 0 和 1），选取其大于等于这些 Recall 值时的 Precision 最大值，然后计算 PR 曲线下面积作为 AP 值。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"通常 VOC10 标准下计算的 mAP 值会高于 VOC07，原因如下：插值平均精度（Interpolated Average Precision）：一些作者选择了另一种近似值，称为插值平均精度。 通常，他们仍然称其为平均精度。 这种方法不使用 P(k)，在 k 个图像的截止处的精度，插值平均精度使用：max_tildekge kP(tildek)换句话说，插值平均精度不是使用在截止 k 处实际观察到的精度，而是使用在所有具有更高召回率的截止上观察到的最大精度。计算插值平均精度的完整方程为：sum_k=1^Nmax_tildekge kP(tildek)Delta r(k)近似平均精度（Approximated Average）与实际观察到的曲线非常接近。 插值平均精度高估了许多点的精度，并产生比近似平均精度更高的平均精度值。此外，在计算插值平均精度时，在何处采集样本存在差异。 有些人在从 0 到 1 的固定 11 个点采样：0 01 02  09 10。 这称为 11 点插值平均精度。 其他人在召回率发生变化的每个 k 处采样。","category":"page"},{"location":"AI/CV/#目标检测历史","page":"计算机视觉","title":"目标检测历史","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"(Image: )","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"在一开始的 CNNs 上，把一张图片划分成为固定的区域，然后识别这些区域中是否有某个物体，有的话就把这个区域标识出来。但是在实际中，图片上的物体大小是不固定的，用这种固定大小的区域去识别物体显然是不合理的。人们想到，如果想要让框更加合适，可以增加框的数量，然后让每个区域都变得尽可能地小。但是这样框太多的时候，又会导致计算量的增加。","category":"page"},{"location":"AI/CV/#R-CNN","page":"计算机视觉","title":"R-CNN","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"基于区域的卷积神经网络（Region-based CNN，R-CNN）出现于 2014 年，是第一个将 CNN 用于目标检测的深度学习模型。它是是解决这种缺点的更好方法，它使用生成区域建议的方式来选择区域。R-CNN 的选框方式是根据选择性搜索来进行的，选框也叫做区域（regions）。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"首先使用无监督的选择性搜索（Selective Serch，SS）方法将图像中具有相似颜色直方图特征的区域进行合并，产生 2000 个大小不一样的候选区域。这个最后合成的区域就是物体在图片中的位置，即感兴趣区域（Region of Interest，RoI）；\n然后从输入图像中截取这些候选区域对应的图像，将其裁剪缩放 reshape 至合适的尺寸，并相继送入一个 CNN 特征提取网络进行高层次的特征提取；\n提取出的特征再被送入一个支持向量机（Support Vector Machine，SVM）来对这些区域进行分类，以及一个线性回归器进行边界框位置和大小的修正，即边界框回归（Bounding Box Regression）；\n最后对检测结果进行非极大值抑制（Non-Maximum Suppression，NMS），得到最终的检测结果；","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"R-CNN 的不足：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"每一张图片都会生成很多个 RoI；\n整个过程用了三个模型：特征提取的 CNN、物体分类的 SVM、预测边界框的回归模型，让 R-CNN 变得非常慢，预测一张图片要几十秒；","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"选择性搜索（Selective Serch，SS）：一个物体会包括四种信息：不同的尺度、颜色、纹理和边界，选择性搜索目标就是识别这些模式，提出不同的区域。首先，先生成最初的分割得很细的子分割，然后再将这些很细的小区域按照颜色相似度、纹理相似度、大小相似度和形状相似兼容性来合并成更大的感兴趣区域 RoI。","category":"page"},{"location":"AI/CV/#SPPNet","page":"计算机视觉","title":"SPPNet","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"SPPNet 出现于 2015 年，","category":"page"},{"location":"AI/CV/#Fast-R-CNN","page":"计算机视觉","title":"Fast R-CNN","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Fast R-CNN 出现于 2015 年，它添加了一个 RoI 池化层（RoI Pooling Layer）来把所有的建议区域转换成适合的尺寸，输入到后面的全连接层（Fully Connection）。Fast R-CNN 将 R-CNN 的三个独立的模型集合到一个模型中，因为减少了很多的计算量，Fast R-CNN 在时间花费大大地减少了。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"具体步骤为：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"图片通过 CNN 得到 RoI，然后 RoI 池化层将 RoI 改变成相同的尺寸；\n再将这些区域输入到全连接层上进行分类，同时使用 softmax 和线性回归层（Linear Regression Layers）来输出 Bounding Boxes；","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"RoI 池化层（RoI Pooling Layer）：目的是对非均匀尺寸的输入执行最大池化以获得固定尺寸的特征图。RoI 池化层的原型是何凯明提出的空间金字塔池化（Spatial Pyramid Pooling），RoI 池化是 SPP 只使用其中一层的特殊情况。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"RoI Pooling 接收卷积特征图作为输入；\n将 RoI 分割为 Htimes W 个网格（论文中为 7times 7），对每一个网格都进行 max pooling 得到最终 Htimes W 大小的特征图；","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"下面是 RoI Pooling 的一个 GIF 示例：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"(Image: )","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Fast R-CNN 的优势和不足：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"依然在使用选择性搜索来作为寻找 RoI 的方法，虽然速度提高了，但是一张图片依旧需要花费 2 秒的时间。","category":"page"},{"location":"AI/CV/#R-FCN","page":"计算机视觉","title":"R-FCN","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"R-FCN 出现于 2016 年","category":"page"},{"location":"AI/CV/#SSD","page":"计算机视觉","title":"SSD","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"SSD 出现于 2016 年","category":"page"},{"location":"AI/CV/#YOLO","page":"计算机视觉","title":"YOLO","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"YOLO 出现于 2016 年","category":"page"},{"location":"AI/CV/#Faster-R-CNN","page":"计算机视觉","title":"Faster R-CNN","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Faster R-CNN 出现于 2017 年，它使用一个区域建议网络（Region Proposal Network，RPN）来获得比 Fast R-CNN 更高的效率。RPN 将图片特征 map 作为输入，生成一系列带目标分数的建议，也就是告诉网络给出的区域有物体的可能性有多大，分数越高代表包含了物体的可能性越高。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"具体步骤：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"把图片作为输入放进卷积网络中去，返回的是一个特征映射（feature map）；\nRPN 处理这些 map，返回带分数的物体建议；\n接下来的 RoI pooling 把这些建议都 reshape 成相同的尺寸；\n最后，放到含有 softmax 层和线性回归层的全连接层上，来分类和输出 bounding boxes。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"RPN 被集成在了网络里面，等于从区域建议到最后的分类回归都在同一个网络，实现了端到端。即我们给这个网络输入一张图片，网络就会输出 bounding boxes 和分数。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"区域建议网络（Region Proposal Network，RPN）：可以输入任何大小的图片（或者特征映射图），然后输出一系列目标建议矩形框，每个矩形框都会有一个对应的分数，代表这个框里面有多大的概率是一个物体。在 Faster R-CNN 中 RPN 是一个全卷积网络（Fully-Convolutional Network，FCN）","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"(Image: )","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"图中的数据，在原论文中的具体值为：C_2=256 text or  512，H=W=16，k=9. ","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"RPN 实际上可以看成是一个小型的 CNN，原文说的是它在 feature map 上使用一个大小为 ntimes n 的滑动窗口（sliding window），在 Faster R-CNN 论文里 n=3：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"步骤 1：实际上 RPN 就是一个 3times 3 的卷积层，将维数（或者说通道数 Channel）为 C_1 的特征图 1 映射成维度为 C_2 的特征图 2（在 Faster R-CNN 论文中，在使用 ZF 模型时 C_2=256，在使用 VGG 模型时 C_2=512）；\n步骤 2：这个特征图 2 会分别进入两个 1times 1  卷积层，一个做矩形框分类（判断是否为物体），对应特征图 3-1，另一个做矩形框回归，对应特征图 3-2。1times 1 卷积的作用是压缩通道数（Channel），图中用于矩形框分类的特征图 3-1 通道数变为 2k，用于矩形框回归的特征图 3-2 通道数变为 4k，这里的 k 是 anchor boxes 的数量（在论文里取 k=9）。分类部分的维度为 2，分别表示框出的部分为「目标」与「非目标」的概率；回归部分的维度为 4，分别表征不同 anchor boxes 对 groud-truth 的长、宽、X 坐标、Y 坐标的预测；\n在训练的时候，只有 RPN 输出的区域建议与 groud-truth 的 textIoU07 的 anchor boxes 与 groud-truth 的位置大小误差才会对最终的损失 mathcalLoss 有贡献。\n对于特征图 1 中的每一个 ntimes n 的滑动窗口， RPN 输出 k 个区域建议，这 k 区域建议都是由 k 个 anchor boxes 作为基准调整得到的。特征图 1 中的每一个点都可以对应到原图的某个点，这个点称为锚点（anchor）。\n在论文中，对于每一个 anchor，以其为中心选择 9 个不同大小和不同长宽比的 anchor boxes，具体为 128^2 256^2 512^2 三种尺度，每个尺度按 11 12 21 的 3 种长宽比例进行缩放，因此一共有 9 个。\n实际上 RPN 并不是直接预测最终的区域建议，而是调节所有的 anchor boxes 并且经过非极大值抑制得到最终的区域建议。对于一个大小为 Htimes W 的特征图，会有 kHW 个 anchor boxes。\n对于每个 anchor，如果满足两种情况：（1）与 ground-truth box 有最大的 IoU（并不一定会大于 0.7）；（2）与 ground-truth 的 IoU 大于 0.7，那么给其分配正标签，表示预测的效果是好的；如果与 ground-truth 的 IoU 小于 0.3 那么给其分配负标签，表示预测的结果很差。除了这些情况，其他的 anchor 不会对对损失函数有贡献。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Faster RCNN 的损失函数由 4 个部分组成：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"RPN 分类损失：anchor 是否为 Ground Truth，二类交叉熵损失；\nRPN 位置回归损失：anchor 位置微调，bbox 的第一次修正；\nRoI 分类损失：RoI 所属类别，分类损失；\nRoI 位置回归损失：继续对 RoI 位置微调，第二次对 bbox 的修正；","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"最终的损失是这 4 个损失相加。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"对于每一个图片，损失函数为：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Lleft( p_it_i right) = frac1N_clssum_iL_cls(p_ip_i^*) + lambdafrac1N_regsum_ip_i^*L_reg(t_it_i^*)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"其中，i 是 mini-batch 中 anchor 的索引，p_i 是第 i 个 anchor 中为物体的预测概率。对于 ground-truth label  p_i^*，如果 anchor 是正标签那么其为 1，如果为负标签那么其为 0。t_i 为一个向量，表示所预测的边界框（Bounding Box）的 4 个坐标，t_i^* 表示正标签 anchor 所对应的 ground-truth box 的坐标。分类损失 L_cls 是两个类别（「目标」与「非目标」）的对数损失（Log Loss）；回归损失 L_reg(t_it_i^*)=R(t_it_i^*). ","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Faster R-CNN 的优势和不足：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"通过使用端到端的方式去进行，并且也不会考虑所有的 RoI，处理一张图片只需要 0.2 秒。","category":"page"},{"location":"AI/CV/#Light-Head-R-CNN","page":"计算机视觉","title":"Light-Head R-CNN","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Light-Head R-CNN 出现于 2017 年","category":"page"},{"location":"AI/CV/#Mask-R-CNN","page":"计算机视觉","title":"Mask R-CNN","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Mask R-CNN 出现于 2017 年","category":"page"},{"location":"AI/CV/#FPN","page":"计算机视觉","title":"FPN","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"特征金字塔网络（Feature Pyramid Networks，FPN）：低层的特征语义信息比较少，但是目标位置准确；高层的特征语义信息比较丰富，但是目标位置比较粗略。有些算法采用多尺度特征融合的方法，但是一般是采用融合后的特征做预测。这篇文章创新的点在于预测是在不同特征层独立进行的。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"(Image: )","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"论文中的图 1 展示了 4 种利用特征的方式：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"图像金字塔（Featurized image pyramid）：将图像 reshape 为不同的尺度，不同尺度的图像生成对应不同尺度的特征。这种方式缺点在于增加了时间成本；\n单个特征图（Single feature map）：像 SPPNet、Fast R-CNN、Faster R-CNN 等模型采用的方式，只使用最后一层的特征图；\n金字塔特征层次结构（Pyramidal feature hierarchy）：像 SSD 模型采用多尺度特征融合的方式，没有上采样的过程，从网络不同层抽取不同尺度的特征做预测。优点在于不会增加额外的计算量；缺点在于 SSD 没有用到足够底层的特征（SSD 中最底层的特征是 VGG 网络的 Conv4_3）；\n特征金字塔网络（Feature Pyramid Network）：顶层特征通过上采样和低层特征做融合，每层独立预测；","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"(Image: )","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"论文的图 2 展示的是两种不同的金字塔结构，上面的结构将最顶部最小的特征图进行上采样之后与前面阶段的特征图相融合，最终只在最底层最大的特征图（自顶向下的最后一层也可以叫做 Finest Level）上进行预测。下面的结构预测是在每一层中独立进行的。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"论文的算法结构如图 3 所示，其结构包括一个自底向上的路径（bottom-up pathway）、自顶向下的路径（top-down pathway）以及横向连接（lateral connections），1times 1 卷积层的主要作用是减少卷积核的个数。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"(Image: )","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Bottom-Up Pathway 是网络的前向过程。论文将不改变 feature map 大小的层视为在同一个网络阶段（stage），每次抽取出来的 feature map 都是每个 stage 的最后一层输出，因为最后一层的特征是最强的，每个阶段的 feature map 记为 C_2 C_3 C_4 C_5。\nTop-Down Pathway 过程采用上采样（Upsampling）进行，生成的 feature map 记为 P_2 P_3 P_4 P_5。\nLateral Connections 是将上采样的结果和自底向上生成的相同大小的 feature map 进行融合（merge）。\n在融合过后会使用 3times 3 卷积对每个融合结果进行卷积，以消除上采样的混叠效应（Aliasing Effect）。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"将 FPN 用于 RPN 网络中生成 Region Proposal，在每一个 stage 都定义了不同大小的 anchor，对于 P_2 P_3 P_4 P_5 P_6 分别为 32^2 64^2 128^2 256^2 512^2，每种尺度的 anchor 有不同的比例 12 11 21，整个特征金字塔有 15 种 anchors。","category":"page"},{"location":"AI/CV/#RetinaNet-(Focal-Loss)","page":"计算机视觉","title":"RetinaNet (Focal Loss)","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"推荐阅读：MMDetection；\nGitHub：MMDetection；","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"何凯明在 ICCV2017 上的新作 Focal Loss for Dense Object Detection 提出了一个一个新的损失函数 —— Focal Loss，主要用于解决在单阶段目标检测场景上训练时前景（foreground）和背景（background）类别极端失衡（比如 1:1000）的问题。Focal Loss 可以抑制负样本对最终损失的贡献以提升网络的整体表现。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"将不含有待检测物体的区域称为负样本，含有待检测物体的区域称为正样本。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Focal Loss 的最终形式为：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"textFL(p_t)=-alpha_t(1-p_t)^gammalog(p_t)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"演变过程如下，一般来说，对于二分类问题，交叉熵损失为：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"textCE(py)=begincases-log(p)qquad textif  y=1 -log(1-p)quad textotherwise endcases","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"其中 yinpm 1 是类别标签，pin0 1 是模型对于样本类别属于 y=1 的预测概率，定义","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"p_tbegincasesp qquad textif  y=1 1-p quad textotherwise endcases","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"因此交叉熵损失可以重写为 textCE(py)=textCE(p_t)=-log(p_t)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"这里应该区分的一个点是：难易样本不平衡和正负样本不平衡，Focal Loss 主要是在解决难易样本不平衡的问题上。一般解决正负样本不平衡的问题，会在交叉熵损失前面加上一个参数 alpha 得到","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"textCE(p_t)=-alpha_tlog(p_t)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"只是这样的方案只能解决正负样本不平衡的问题，至于难易样本不平衡，Focal Loss 的思想就是降低高置信度样本的损失：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"textFL(p_t)=-(1-p_t)^gammalog(p_t)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"假设 gamma=2 时，如果样本置信度为 p=0968，那么 (1-0968)^2approx 0001 就可以将这个高置信度样本的损失衰减 1000 倍。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"将增加参数 alpha 添加到 Focal Loss 上就可以同时解决正负以及难易样本不平衡的问题，最终 Focal Loss 的形式为：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"textFL(p_t)=-alpha_t(1-p_t)^gammalog(p_t)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"在 MMDetection 的 GItHub 开源代码中可以看到对于 Focal Loss 实现的 Python 代码，实际上真正使用的是 CUDA 版本代码，因此这里给出的代码只是供人学习的。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Focal Loss 存在的问题：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"模型过多地关注那些特别难分的样本 —— 离群点（outliers），即便是模型已经收敛了，但是这些离群点依旧是","category":"page"},{"location":"AI/CV/#SENet","page":"计算机视觉","title":"SENet","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"目标检测中的注意力机制","category":"page"},{"location":"AI/CV/#FCOS","page":"计算机视觉","title":"FCOS","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"发表于 ICCV2019 的论文：FCOS: Fully Convolutional One-Stage Object Detection 提出了 FCOS，与 YOLO 类似，它直接将 backbone 输出的 feature map 上的每一个像素当做预测起点，即把每一个位置都当做训练样本，只要该位置落入某个 Ground-Truth 框，就将其当做正样本进行训练。为了让一个目标在推理时不会在多个 feature map 上被重复输出，认为限制了每一层回归目标的尺度大小，超过该限制的目标，这一层就不检测。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"论文中图 2 展示了 FCOS 的具体结构：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"(Image: )","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"FCOS 在检测头增加一个中心度（Centerness）分支，保证回归框的中心和 GT 较为接近，同时和 FPN 结合，在每一层上只回归特定大小的目标，从而将不同尺度的目标分配到对应层级上","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"textcenterness^*=sqrtfracmin(l^*r^*)max(l^*r^*)times fracmin(t^*b^*)max(t^*b^*)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"其中 ","category":"page"},{"location":"AI/CV/#CenterNet","page":"计算机视觉","title":"CenterNet","text":"","category":"section"},{"location":"AI/CV/#ATSS","page":"计算机视觉","title":"ATSS","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"论文 Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection 中提出了一种根据目标的统计信息自动选择正负样本的自适应样本选择机制（Adaptive Training Sample Selection，ATSS）。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"论文里提到无论是 anchor-based 方法还是 anchor-free 方法，","category":"page"},{"location":"AI/CV/#GFL","page":"计算机视觉","title":"GFL","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"推荐阅读：GFL 作者本人 李翔 的文章 知乎：大白话 Generalized Focal Loss；","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"论文 Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection 所提出的 广义焦点损失（Generalized Focal Loss，GFL）的具体形式如下：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"textGFL(p_y_l p_y_r)=-y-(y_l p_y_l+y_rp_y_r)^betaleft( (y_r-y)log(p_y_l) + (y-y_l)log(p_y_r)right)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"GFL 主要解决两个问题：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"在训练和推理的时候，分类和质量估计的不一致性；\n狄拉克分布针对复杂场景下（模糊和不确定性边界）存在不灵活的问题；","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"解决这两个问题的方式是设计新的「表示」方法：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"通过联合质量估计和分类设计新的 Loss；\n定义一种新的边界框表示方式来进行回归；","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"GFL 工作的核心是围绕表示（representation）的改进去进行的，表示具体是指检测器最终的输出，也就是 head 末端的物理对象，以 FCOS、ATSS 为代表的 one-stage anchor-free 检测器基本会包含 3 个表示：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"分类表示；\n检测框表示；\n检测框的质量估计。在 FCOS、ATSS 中采用 centerness，一些其他的工作会采用 IoU，这些 score 基本都在 0 到 1 之间；","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"现有的表示主要存在的问题：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"classification score 和 IoU / centerness score 训练测试不一致，具体有：\n用法不一致。训练的时候，分类和质量估计是分开训练的，但是在测试的时候又是乘在一起作为 NMS score 排序的依据；\n对象不一致。质量估计通常只针对正样本训练，对于 one-stage 检测器而言，在做 NMS score 排序的时候，所有的样本都会将分类 score 和质量预测 score 相乘用于排序，这样会引发一个情况：一个分类 score 相对低的真正负样本，由于预测了一个不可信的高质量 score，导致到它可能排到一个分类 score 不高并且质量 score 较低的真正的正样本的前面；\nBounding Box 回归采用的表示不够灵活，没有办法建模复杂场景下的 uncertainty；","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Focal Loss 支持 0 或者 1 类型的离散 label，而对于分类 - 质量联合表示，label 是 0～1 之间的连续值。如果对 Focal Loss 在连续 label 上进行拓展，就可以使其即保证平衡正负难易样本的特性，又支持连续数值的监督，因此得到 Quality Focal Loss（QFL），具体形式如下：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"textQFL(sigma)=-y-sigma^betaleft((1-y)log(1-sigma)+ylog(sigma) right)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"其中 y 为 0～1 的质量标签，sigma 为预测，QFL 的全局最小解是 sigma=y。之后又增加了一个称为 Distribution Focal Loss（DFL）的 loss，目的是希望网络能够快速地聚焦到标注位置附近的数值，使得它们概率尽可能大，DFL 的具体形式如下：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"textDFL(S_iS_i+1)=-left((y_i+1-y)log(S_i)+(y-y_ilog(S_i+1)) right)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"如果将 QFL 和 DFL 统一起来，就可以表示为 GFL。","category":"page"},{"location":"AI/CV/#人脸识别","page":"计算机视觉","title":"人脸识别","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"人脸识别中的模型 ArcFace……","category":"page"},{"location":"AI/CV/#SphereFace","page":"计算机视觉","title":"SphereFace","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"推荐阅读：人脸识别合集｜8 SphereFace 解析，作者：Mengcius：这篇文章写得十分好，十分详细地介绍了 SphereFace 以及介绍了 Softmax Loss 的进化路线。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"SphereFace（超球面），是佐治亚理工学院 Weiyang Liu 等在 CVPR 2017 年的论文 SphereFace: Deep Hypersphere Embedding for Face Recognition. 提出了将 Softmax Loss 从欧几里得距离转换到角度间隔，增加决策余量 m，限制 W=1 和 b=0. ","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"主要思想：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"开集人脸识别（Open-set face recognition）：通常，人脸识别可分为人脸识别和人脸验证。前者将一个人脸分类为一个特定的标识，而后者确定一对图片是否属于同一人。闭集（open-set）是测试图像在训练集中可能出现过；开集（close-set）是测试图像没有在训练集中出现过。开集人脸识别比闭集人脸识别需要更强的泛化能力。过拟合会降低性能。\n闭集的人脸识别：相当于分类问题，学习可分离的特征就可以了，人脸验证或识别时提取出标签。所有测试标识都在训练集中预先定义。很自然地将测试人脸图像分类为给定的身份。在这种情况下，人脸验证相当于分别对一对人脸图像进行识别。\n开集的人脸识别：测试集通常与训练集分离，因为不可能将所有人脸图像归纳在一个训练集中，我们需要将人脸映射到一个可辨别的本地特征空间。在这种情况下，人脸识别被视为在输入人脸图片和数据库中的每个身份之间执行人脸验证。它是度量学习问题，关键是学习有判别力的大间隔特征（discriminative large-margin features），人脸验证或识别时都要比较特征间的距离。\nOpen-set FR 对特征要求的准则：在特定的度量空间内， 需要类内的最大距离小于类间的最小距离。\nA-Softmax Loss (Angular Softmax Loss)：使得 CNN 能够学习角度识别特征，引入了角度间隔 m，以使人脸特征的最大类内距离要小于最小类间距离，使学习的特征将更具有判别力；\nL-Softmax Loss、A-Softmax Loss、CosFace、ArcFace、COCO Loss、Angular Triplet Loss等都是 angular margin learning 系列；\n预处理（人脸对齐）：人脸关键点由 MTCNN 检测，再通过相似变换得到了被裁剪的对齐人脸。RGB 图像中的每个像素范围在 [0, 255]，通过减去 127.5 然后除以 128 进行标准化；\n训练（人脸分类器）：CNN + A-Softmax Loss，CNN 使用 ResNet 中的残差单元；\nCNN 框架与传统的方法相同，可以兼容不同的网络架构（VGG/GoogLeNet/ResNet 等）；\n使用 m=4 的 Angular Softmax Loss，使得学习的特征更具有判别力；\n测试：\n从人脸分类器 FC1 层的输出中提取表示特征 SphereFace，拼接了原始人脸特征和其水平翻转特征获得测试人脸的最终表示；\n对输入的两个特征计算余弦距离（Cosine Similarity），得到角度度量（Angular Metric）；\n人脸验证：用阈值判断余弦距离；\n人脸识别：用最近邻分类器；\nLFW 上 99.42%，YTF 上 95.0%，训练集使用 CASIA-WebFace。2017 年在 MegaFace上 识别率在排名第一。","category":"page"},{"location":"AI/CV/#度量学习","page":"计算机视觉","title":"度量学习","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"度量学习（metric learning）：旨在学习一个相似的距离函数。传统的度量学习常常会学习一个距离度量矩阵 A ，在给定的特征 x_1x_2 上距离度量为：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"x_1 - x_2_A=sqrt(x_1 - x_2)^top A (x_1 - x_2)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"最近流行的深度度量学习通常使用神经网络自动学习具有可区分性的特征 x_1x_2，然后是简单的进行距离度量，如欧几里得距离。用于深度度量学习的最广泛的损失函数是对比损失和三元组损失，两者都对特征施加了欧几里得距离。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"不同算分的度量学习：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"DeepFace、DeepID：通过 SoftMax Loss 学习面部特征，但只具有可分离性而不具有明显的可判别性。\nDeepID2：结合了 Softmax Loss 和 Contrastive Loss 以增强特征的判别能力。但它们产生不同的特征分布，Softmax 损失会产生一个角度的特征分布，对比损失是在欧几里得空间学习到的特征分布，所以特征结合时可能不是一个自然的选择。\nFaceNet：使用 Triplet Loss 来监督嵌入学习。但它需要非常大量数据（2 亿张人脸图像），计算成本很高。对比损失和三元组损失都不能限制在单个样本上，因此需要精心设计的双/三重挖掘过程，这既耗时又对性能敏感。\nVGGFace：先训练 CNN+Softmax Loss，再用 Triplet Loss 度量学习。\nA discriminative feature learning approach for deep face recognition. In ECCV2016：将 Softmax loss 与 Center loss 结合以增强特征的判别能力，但中心损失只起到缩小类间距离的作用，不具有增大类间距离的效果。\nL-Softmax Loss：作者和 A-Softmax Loss 是同一批人。L-Softmax Loss 也隐含了角度的概念。利用改进的 Softmax Loss 进行具有角度距离的度量学习。作为一种正则化方法，它在闭集分类问题上显示了很大的进步。A-Softmax Loss 简单讲就是在 Large-Margin Softmax Loss 的基础上添加了两个限制条件 W=1 归一化和 b=0，使得预测仅取决于 W 和 x 之间的角度。\n人脸识别的 DCNNs 有两个主要的研究方向：分类学习（对应 Softmax Loss）、度量学习（对应 Triplet Loss 等）。Contrastive Loss、Triplet Loss 等都将开集人脸识别问题视为度量学习问题，对比损失和三元组损失都是基于欧几里得距离的度量学习。","category":"page"},{"location":"AI/CV/#Softmax-Loss-的进化","page":"计算机视觉","title":"Softmax Loss 的进化","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Softmax 损失学习按角度分布的特征：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Softmax 损失可以自然地学习按角度分布的特征，如在训练集和测试集不同类别的特征只在角度上分离开，因此不会自然地促使包含任何欧几里德损失。\n从某种意义上说，基于欧几里德距离的损失与 Softmax 损失是不相容的，因此将这两种类型的损失结合起来并不是很好。\n学习特征时增大欧几里得距离，似乎是广泛认可的选择，但问题出现了：欧几里得距离是否总是适合于学习具有可判别性的面部特征？不适合，在 SphereFace 的文章中建议用角度距离代替。\n为什么用角度间隔？\n首先角度间隔直接与流形上的区别性联系在一起，流形上的区别性本质上与前面的一致，面也位于流形上。其次，由原始 Softmax Loss 获得的特征具有固有的角分布，将角度间隔与 Softmax Loss 结合起来实际上是更自然的选择。\n首先，它们只将欧几里得距离强加于学习到的特征，而我们的则直接考虑角度间隔。第二，contrastive loss、 triplet loss 在训练集中构成成对/三重集时都会受到数据扩展的影响，而我们的则不需要样本挖掘，并且对整个小批量都施加了可判别性约束（相比之下，对比损失和三重损失只会影响几个具有代表性的成对/三重集）。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"从一张图讲起：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"(Image: )","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"（1）原始 Softmax Loss","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"如图 a, b，x 为学习到的特征向量，W_i 和 b_i 是最后一个全连接层对应类 i 的权值和偏置。Softmax 计算两个类的概率为：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"hatboldsymboly=textsoftmax(boldsymbolW^top boldsymbolx)=fracexp(boldsymbolW^top boldsymbolx+boldsymbolb)boldsymbol1^top_Cexp(boldsymbolW^top boldsymbolx+boldsymbolb)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"如果以二分类为例：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"p_1=fracexp(W_1^top x + b_1)exp(W_1^top x + b_1) + exp(W_2^top x + b_2)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"p_2=fracexp(W_2^top x + b_2)exp(W_1^top x + b_1) + exp(W_2^top x + b_2)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Softmax 损失的让两个类别分开来的决策边界（Decision Boundary）是：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"(W_1 - W_2)x + b_1 - b_2 = 0","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"如果将 Softmax 重写成 W 和 x 的内积形式，就有了 cos 夹角：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"mathcalL_i=-logleft(fracexpleft(W_y_i^top x_i + b_y_iright)sum_j expleft(W_j^top x_i + b_jright)right)=-logleft( fracexp left(W_y_i  x_i cos(theta_y_ii) + b_y_i right)sum_j exp left( W_jx_icos(theta_j i) + b_j right) right)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"学习到的特征分布投射到一个球体上，就可以看到两类别间不能简单地通过角度分类。两类别是可以分离开的，但还是有一些误差，Softmax 只学习到了可分离的特征，但内聚性不好，判别性不够。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"（2）Modified Softmax Loss","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"如图 c, d，Modified softmax loss 能够直接优化角度，使 CNN 能够学习角度分布特征。为了实现角度决策边界，最终 FC 层的权重实际上是无用的。因此，首先对权重进行归一化并将偏置项归零（W_i=1 b_i=0），其公式为","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"mathcalL_textmodified=frac1Nsum_i-logleft( fracexp(x_icos(theta_y_i i))sum_j exp(x_icos(theta_j i)) right)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"后验概率为 p_1 = xcos(theta_1) p_2=x cos(theta_2)，决策边界变为","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"x(costheta_1 - costheta_2)=0","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"结果仅取决于 theta_1 和 theta_2. 这个改进的 Softmax Loss 可以学习带有角边界的特征，加强了角度可分性，但是这些特征还是没有判别性（discriminative）","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"（3）A-Softmax Loss","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"如图 e, f，进一步引入角度间隔（Angular Margin），让分类更加困难从而学习判别性。角度间隔更加大了，但分布的弧长变短了。A-Softmax loss（Angular Softmax Loss）针对不同的类别采用不同的决策边界（每个边界都比原边界更严格），从而产生角间隔。引入一个整数 m 来定量控制决策边界，二分类的类 1 和类 2 的决策边界分别变为：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"从类别 1 正确分类，需要 cos(mtheta_1)cos(theta_2)，决策边界就是 cos(mtheta_1)=cos(theta_2)。从类别 2 则相反。\n从角度的观点来考虑，从标识 1 正确分类 x 需要 mtheta_1theta_2，而从标识 2 正确分类 x 则需要 mtheta_2theta_1。\n因为 m 是正整数，cos 函数在 0 到 pi 范围又是单调递减的，所以 mtheta_1 要小于 theta_2，m 值越大，theta_1 越聚合，则学习的难度也越大。因此通过这种方式定义损失会逼得模型学到类间距离更大的，类内距离更小的特征。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"x(cos(mtheta_1) - cos(theta_2)) = 0 text for class 1","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"x(cos(theta_1) - cos(mtheta_2)) = 0 text for class 2","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"A-Softmax Loss 公式是将改进的 Softmax Loss 中 theta 乘以系数 m 整数间隔值。即以乘法的方式惩罚深度特征与其相应权重之间的角度。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"mathcalL_textang = frac1Nsum_i-logleft( fracexp(x_icos(mtheta_y_ii))exp(x_icos(mtheta_y_ii))+sum_jneq y_iexp(x_icos(theta_ji)) right)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"上是中 theta 的范围是 0pim，为了摆脱这一限制，将 cos(mthetai) 推广到一个单调递减的角函数 psi(thetai). mge1 是控制角度间隔大小的整数，当 m=1 时它就是 Modified Softmax Loss。需要注意的是在每一次迭代中权重归一化为 1. A-Softmax Loss 的最终公式为：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"mathcalL_textang=frac1Nsum_i-logleft(fracexp(x_ipsi(theta_y_ii))exp(x_ipsi(theta_y_ii) + sum_jneq y_iexp(x_icos(theta_ji))) right)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"其中 psi(theta_y_ii) = (-1)^kcos(mtheta_y_ii) - 2k theta_y_iiinfrackpimfrac(k+1)pim，且 kin0 m-1.","category":"page"},{"location":"AI/CV/#人脸识别中的损失函数","page":"计算机视觉","title":"人脸识别中的损失函数","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Softmax Loss：最常见的人脸识别函数，原理是去掉最后的分类层，作为解特征网络导出解特征向量用于人脸识别。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"hatboldsymboly=textsoftmax(boldsymbolW^top boldsymbolx)=fracexp(boldsymbolW^top boldsymbolx+boldsymbolb)boldsymbol1^top_Cexp(boldsymbolW^top boldsymbolx+boldsymbolb)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"其中 boldsymbolW=boldsymbolw_1cdotsboldsymbolw_C 是由 C 个类的权重向量组成的矩阵，boldsymbol1^top_C 为 C 维的全 1 向量，hatboldsymbolyinmathbbR^C 为所有类别的预测条件概率组成的向量，第 c 维的值是第 c 类的预测条件概率。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"mathcalL_textsoftmax=-frac1N_bsum_i=1^N_blog fracexp(boldsymbolw_y_iboldsymbolx+b_y_i)boldsymbol1^top_Cexp(boldsymbolW^top boldsymbolx+boldsymbolb)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"其中 boldsymbolw_y_i b_y_i 代表实际标签 y_i 对应的权重和偏置。softmax 在训练的时候收敛迅速，但是精确度一般达到 0.9 时就不会再上升。一方面作为分类网络，softmax 不能像 metric learning 一样显式地优化类间和类内距离，所以性能不会特别好；另外，人脸识别的关键在于得到泛化能力强的 feature，与分类能力并不是完全等价的。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"L-Softmax Loss：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"SphereFace（A-Softmax）：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Focal Loss：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Triplet Loss：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Triplet Loss 是在谷歌 2015 年的 FaceNet 论文中的提出来的，用于解决人脸识别相关的问题，原文为：《FaceNet: A Unified Embedding for Face Recognition and Clustering》。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Triplet 三元组指的是 anchor, negative, positive 三个部分，每一部分都是一个 embedding 向量，其中","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"anchor 指的是基准图片；\npositive 指的是与 anchor 同一分类下的一张图片；\nnegative 指的是与 anchor 不同分类的一张图片；","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Triplet Loss 的目的是让 anchor 和 positive 的距离变得越来越小，而与 negative 的距离变得越来越大，损失函数定义如下：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"mathcalL=max(d(a p) - d(a n) + textmargin 0)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"其中 a p n 分别代表 anchor，positive 和 negative。如果 negative example 很好识别时，anchor 与 negative 的距离会相对较大，即 d(an) 相比之下偏大，那么损失为 mathcalL=0；否则通过最小化损失函数，可以让 anchor 与 positive 的距离 d(ap) 更加接近 0，而与 negative 的距离 d(an) 更加接近给定的 margin。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"基于 triplet loss 的定义，可以将 triplet（三元组）分为三类：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"easy triplets（简单三元组）：triplet 对应的损失为 0 的三元组：d(an)d(ap) + textmargin；\nhard triplets（困难三元组）：negative example 与 anchor 距离小于 anchor 与 positive example 的距离，形式化定义为：d(an)d(ap)；\nsemi-hard triplets（一般三元组）：negative example 与 anchor 距离大于 anchor 与 positive example 的距离，但还不至于使得 mathcalLoss 为 0，即 d(ap)d(an)d(ap)+textmargin；","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"PyTorch 实现：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"class TripletLoss(nn.Module):\n    \"\"\"Triplet loss with hard positive/negative mining.\n    \n    Reference:\n        Hermans et al. In Defense of the Triplet Loss for Person Re-Identification. arXiv:1703.07737.\n    \n    Imported from `<https://github.com/Cysu/open-reid/blob/master/reid/loss/triplet.py>`_.\n    \n    Args:\n        margin (float, optional): margin for triplet. Default is 0.3.\n    \"\"\"\n    \n    def __init__(self, margin=0.3,global_feat, labels):\n        super(TripletLoss, self).__init__()\n        self.margin = margin\n        self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n \n    def forward(self, inputs, targets):\n        \"\"\"\n        Args:\n            inputs (torch.Tensor): feature matrix with shape (batch_size, feat_dim).\n            targets (torch.LongTensor): ground truth labels with shape (num_classes).\n        \"\"\"\n        n = inputs.size(0)\n        \n        # Compute pairwise distance, replace by the official when merged\n        dist = torch.pow(inputs, 2).sum(dim=1, keepdim=True).expand(n, n)\n        dist = dist + dist.t()\n        dist.addmm_(1, -2, inputs, inputs.t())\n        dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n        \n        # For each anchor, find the hardest positive and negative\n        mask = targets.expand(n, n).eq(targets.expand(n, n).t())\n        dist_ap, dist_an = [], []\n        for i in range(n):\n            dist_ap.append(dist[i][mask[i]].max().unsqueeze(0))\n            dist_an.append(dist[i][mask[i] == 0].min().unsqueeze(0))\n        dist_ap = torch.cat(dist_ap)\n        dist_an = torch.cat(dist_an)\n        \n        # Compute ranking hinge loss\n        y = torch.ones_like(dist_an)\n        return self.ranking_loss(dist_an, dist_ap, y)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Center Loss：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"在 Triplet Loss 之后又提出了一个 Center Loss。Triplet 学习的是样本间的相对距离，没有学习绝对距离，尽管考虑了类间的离散性，但没有考虑类内的紧凑性。Center Loss 希望可以通过学习每个类的类中心，使得类内的距离变得更加紧凑，其公式如下：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"mathcalL_C=frac12sum_i=1^m x_i -c_y_i _2^2","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"其中 c_y_iinmathbbR^d 表示深度特征的第 y_i 类中心。理想情况下，c_y_i 应该随着深度特性的变化而更新。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"训练时：第一是基于mini-batch执行更新。在每次迭代中，计算中心的方法是平均相应类的特征（一些中心可能不会更新）。第二，避免大扰动引起的误标记样本，用一个标量 alpha 控制中心的学习速率，一般这个 alpha 很小（如 0.005）。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"计算 mathcalL_C 相对于 x_i 的梯度和 c_y_i 的更新方程为：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"fracpartial mathcalL_Cpartial x_i=x_i-c_y_i","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"Delta c_j = fracsum_i=1^m delta(y_i=j)cdot(c_j-x_i)1+sum_i=1^m delta(y_i=j)","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"class CenterLoss(nn.Module):\n    \"\"\"Center loss.\n    Reference:\n    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\n    Args:\n        num_classes (int): number of classes.\n        feat_dim (int): feature dimension.\n    \"\"\"\n \n    def __init__(self, num_classes=751, feat_dim=2048, use_gpu=True):\n        super(CenterLoss, self).__init__()\n        self.num_classes = num_classes\n        self.feat_dim = feat_dim\n        self.use_gpu = use_gpu\n \n        if self.use_gpu:\n            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n        else:\n            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n \n    def forward(self, x, labels):\n        \"\"\"\n        Args:\n            x: feature matrix with shape (batch_size, feat_dim).\n            labels: ground truth labels with shape (num_classes).\n        \"\"\"\n        assert x.size(0) == labels.size(0), \"features.size(0) is not equal to labels.size(0)\"\n \n        batch_size = x.size(0)\n        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n        distmat.addmm_(1, -2, x, self.centers.t())\n \n        classes = torch.arange(self.num_classes).long()\n        if self.use_gpu: classes = classes.cuda()\n        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n        print(mask)\n \n        dist = []\n        for i in range(batch_size):\n            print(mask[i])\n            value = distmat[i][mask[i]]\n            value = value.clamp(min=1e-12, max=1e+12)  # for numerical stability\n            dist.append(value)\n        dist = torch.cat(dist)\n        loss = dist.mean()\n        return loss","category":"page"},{"location":"AI/CV/#光学字符识别","page":"计算机视觉","title":"光学字符识别","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"光学字符识别（Optical Character Recognition，OCR）：挖掘图像中的文本信息，需要对图像中的文字进行检测和识别。OCR 的确切定义是，将包含键入、印刷或场景文本的电子图像转换成机器编码文本的过程。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"OCR 算法通常分为两个基本模块，属于物体检测其中一个子类的文本检测以及文本识别。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"传统的文本检测：基于二值化的连通区域提取，基于最大极值稳定区域（Maximally Stable Extremal Regions，MSER），方向梯度直方图（Histogram of Oriented Gradient，HOG）可以提取特征；隐马尔可夫模型（Hidden Markov Model，HMM）对最终的词语进行预测。","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"文本检测框架的两种类型：","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"基于候选框：在通用物体检测的基础上，通过设置更多不同长宽比的锚框来适应文本变长的特性，以达到文本定位的效果。类似的模型包括：Rosetta、SegLink、TextBoxes++；\n基于像素分割：首先通过图像语义分割获得可能属于的文本区域的像素，之后通过像素点直接回归或者对文本像素的聚合得到最终的文本定位。类似的模型包括：TextSnake、SPCNet、MaskTextSpotter；\n不同方法的优缺点：\n基于候选框的文本检测对文本尺度本身不敏感，对小文本的检出率更高；但是对于倾斜角度较大的密集文本块，该方法很容易因为无法适应文本方向的剧烈变化以及对文本的包覆性不够紧密而检测失败。\n基于候选框的检测方法利用整体文本的粗粒度特征，而非像素级别的精细特征，因此其检测精度往往不如基于像素分割的文本检测。\n基于像素分割的文本检测往往具有更好的精确度，但是对于小尺度的文本，因为对应的文本像素过于稀疏，检出率通常不搞，除非以牺牲检测效率为代价对输入图像进行大尺度的放大。\n同时基于候选框和像素分割：将基于候选框的文本检测框架和基于像素分割的文本检测框架结合在一起，共享特征提取部分，并将像素分割的结果转换为候选框检测回归过程中的一种注意力机制，从而使文本检测的准确性和召回率都得到提高，例如云从科技公司提出的 Pixel-Anchor；","category":"page"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"检测文字所在位置（CTPN）和识别文本区域内容（CRNN）","category":"page"},{"location":"AI/CV/#EAST","page":"计算机视觉","title":"EAST","text":"","category":"section"},{"location":"AI/CV/","page":"计算机视觉","title":"计算机视觉","text":"一种高效准确的场景文本检测器（An Efficient and Accurate Scene Text Detector，EAST）：","category":"page"},{"location":"interview/#笔试","page":"面试笔试","title":"笔试","text":"","category":"section"},{"location":"interview/#面试：输入输出的处理","page":"面试笔试","title":"面试：输入输出的处理","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"Python 的输出输出处理：","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"# strip() 去掉两端的空白符，返回 str\n# split() 按照空白符分割，返回 List[str]\n# map(type, x) 把列表 x 中的元素映射成类型 type\n# 1. 题目没有告知多少组数据时，用 while True\nwhile True:\n    try:\n        # ...\n    except:\n        break\n        \n# 2. 题目告知有 T 组数据时，用 For loop\nT = int(input().strip())\nfor _ in range(T):\n    # ...\n    \n    \n# 3. 不同的输入\ns = input().strip()  # 输入一个字符\nnum = int(input().strip())  # 输入一个整数\nnums = list(map(int, input().strip().split()))  # 输入一个整数列表","category":"page"},{"location":"interview/#面试题-1：整数除法","page":"面试笔试","title":"面试题 1：整数除法","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"题目：输入 2 个 int 型整数，它们进行除法计算并返回商，要求不得使用乘号 *、除号 / 及求余符号 %。当发生溢出时，返回最大的整数值。假设除数不为 0。例如，输入 15 和 2，输出 15/2 的结果，即 7。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"用减法实现除法，时间复杂度 mathcalO(log n)\n对于含有负数的情况，可以先记录最终答案的是正是负，然后全部转化为绝对值计算，最后在返回时操作；\n由于是整数的除法，且除数不等于 0，因此商的绝对值一定小于等于被除数的绝对值。只有一种情况会导致溢出，即 (-2^31)(-1)。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"import sys\ndef divide(dividend: int, divisor: int) -> int:\n    # 0x80000000 为最小的 int 整数，即 -2^31\n    if (dividend == 0x80000000 and divisor == -1):\n        return sys.maxint  # 溢出时返回最大整数\n    \n    # 如果 negative == 1 则结果为 负数，否则为 正数\n    negative = 2\n    if dividend > 0:\n        negative -= 1\n        dividend = -dividend\n        \n    if divisor > 0:\n        negative -= 1\n        divisor = -divisor\n    \n    result = divideCore(dividend, divisor)\n    return -result if negative == 1 else result\n\ndef divideCore(dividend: int, divisor: int) -> int:\n    \"\"\"\n    使用减法实现两个负数的除法\n    \"\"\"\n    result = 0\n    while dividend <= divisor:\n        value = divisor\n        quotient = 1  # 商\n        while (value >= 0xc0000000) and (dividend <= value + value):\n            # 0xc0000000 是 0x80000000 的一半，即 -2^30（防止溢出）\n            quotient += quotient\n            value += value\n            \n        result += quotient\n        dividend -= value\n    \n    return result","category":"page"},{"location":"interview/#面试题-2：二进制加法","page":"面试笔试","title":"面试题 2：二进制加法","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"题目：输入两个表示二进制的字符串，请计算它们的和，并以二进制字符串的形式输出。例如，输入的二进制字符串分别是「11」和「10」，则输出「101」。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"def binaryAdd(a, b):\n    ans = \"\"\n    i = len(a) - 1\n    j = len(b) - 1\n    carry = 0\n    while (i >= 0) or (j >= 0):\n        digitA = a[i] - '0' if i >= 0 else 0\n        digitB = b[j] - '0' if j >= 0 else 0\n        i += 1\n        j += 1\n        sum_ = digitA + digitB + carry\n        carry = sum_ - 2 if sum_ >= 2 else sum_\n        ans.append(str(sum_))\n    \n    if carry == 1:\n        ans.append(\"1\")\n    \n    # 字符串翻转的两种方法\n    return ans[::-1]  # 使用字符串切片\n\t# return ''.join(reversed(ans))  # 使用 reversed()","category":"page"},{"location":"interview/#面试题-3：前-n-个数字二进制形式中-1-的个数","page":"面试笔试","title":"面试题 3：前 n 个数字二进制形式中 1 的个数","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"题目：输入一个非负数 n，请计算 0 到 n 之间每个数字的二进制形式中 1 的个数，并输出一个数组。例如，输入的 n 为 4，由于 0、1、2、3、4 的二进制形式中 1 的个数分别为 0、1、1、2、1，因此输出数组 [0, 1, 1, 2, 1]。","category":"page"},{"location":"interview/#计算每个整数的二进制形式中-1-的个数","page":"面试笔试","title":"计算每个整数的二进制形式中 1 的个数","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"def countBits(num):\n    result = [0 for _ in range(num + 1)]\n    for i in range(num + 1):\n        j = i\n        while j != 0:\n            result[i] += 1\n            # x & (x - 1) 的操作可以将整数 x 二进制的最右边的 1 变成 0\n            j = j & (j - 1)\n            \n    return result","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"上述代码的时间复杂度 mathcalO(nk)，k 为二进制中 1 的个数","category":"page"},{"location":"interview/#根据-x-and-(x-1)-计算其二进制形式中-1-的个数","page":"面试笔试","title":"根据 x & (x - 1) 计算其二进制形式中 1 的个数","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"整数 x 的二进制形式中 1 的个数比 x & (x - 1) 的多 1 个","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"def countBits(num):\n    result = [0 for _ in range(num + 1)]\n    for i in range(num + 1):\n        result[i] = result[i & (i + 1)] + 1\n        \n    return result","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"这个代码的时间复杂度 mathcalO(n). ","category":"page"},{"location":"interview/#根据-x-/-2-计算-x-的二进制形式中-1-的个数","page":"面试笔试","title":"根据 x / 2 计算 x 的二进制形式中 1 的个数","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"如果正整数 x 是一个偶数，那么 x 相当于将 x / 2 左移一位的结果，他们两个的二进制中的 1 个数是相同的。如果 x 是奇数，那么 x 相当于将 x / 2 左移一位之后再将最右边一位设为 1 的结果，因此奇数 x 的二进制形式中 1 的个数比 x / 2 的个数多 1 个。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"def countBits(num):\n    result = [0 for _ in range(num + 1)]\n    for i in range(num + 1):\n        # 位运算效率更高：\n        # 用 i >> 1 替代 i / 2\n        # 用 i & 1 替代 i % 2\n        result[i] = result[i >> 1] + (i & 1)\n    \n    return result","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"这种解法时间复杂度 mathcalO(n)","category":"page"},{"location":"interview/#面试题-4：只出现一次的数字","page":"面试笔试","title":"面试题 4：只出现一次的数字","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"题目：输入一个整数数组，数组中只有一个数字出现了一次，而其他数字都出现了 3 次。请找出那个只出现一次的数字。例如，如果输入的数组为 [0, 1, 0, 1, 0, 1, 100]，则只出现一次的数字是 100。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"简单版本：输入数组中除一个数字只出现一次之外其他数字都出现两次，请找出只出现一次的数字。因为任何一个数字异或它自己的结果都是 0，因此解法就是将数组中所有数字进行异或运算，最终的结果就是那个只出现一次的数字。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"这个题目与简单版本的不同就是，其他重复的数字是出现 3 次的。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"思路是将数组中所有数字的同一位置的数位相加，得到的结果中每个数位都除以 3，如果能够整除，那么只出现一次的数字，对应的数位就是 0，如果结果余 1，那么对应的数位就是 1.","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"def singleNumber(nums: List[int]):\n    bitSums = [0 for _ in range(32)]  # 一个整数是由 32 个 0 或 1 组成的\n    for num in nums:\n        for i in range(32):\n            bitSums[i] += (num >> (31 - i)) & 1  # 得到整数 num 的二进制形式中左数起第 i 个数位\n            \n    result = 0\n    for i in range(32):\n        result = (result << 1) + bitSums[i] % 3\n    \n    return result","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"进阶题目：输入一个整数数组，数组中只有一个数字出现 m 次，其他数字都出现 n 次。请找出那个唯一出现 m 次的数字。假设 m 不能被 n 整除。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"如果数组中所有数字的第 i 个数位相加之和能被 n 整除，那么出现 m 次的数字的第 i 个数位一定是 0；否则出现 m 次的数字的第 i 个数位一定是 1","category":"page"},{"location":"interview/#面试题-5：单词长度的最大乘积","page":"面试笔试","title":"面试题 5：单词长度的最大乘积","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"题目：输入一个字符串数组 words，请计算不包含相同字符的两个字符串 words[i] 和 words[j] 的长度乘积的最大值。如果所有字符串都包含至少一个相同字符，那么返回 0。假设字符串中只包含英文小写字母。例如，输入的字符串数组 words 为 [\"abcw\"，\"foo\"，\"bar\"，\"fxyz\"，\"abcdef\"]，数组中的字符串 \"bar\" 与 \"foo\" 没有相同的字符，它们长度的乘积为 9。\"abcw\" 与 \"fxyz\" 也没有相同的字符，它们长度的乘积为 16，这是该数组不包含相同字符的一对字符串的长度乘积的最大值。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"暴力法：对于 str1 中的每个字符 ch，扫描字符串 str2 判断字符 ch 是否出现在 str2 中。如果两个字符串的长度分别为 p 和 q，那么暴力法时间复杂度 mathcalO(pq).","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"哈希表记录字符串中出现的字符：题目假设字符串只包含英文小写字母，用长度为 26 的数组模拟哈希表","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"def maxProduct(words: List[str]):\n    n = len(words)\n    flags = [[0 for _ in range(26)] for _ in range(len(words))]\n    # Step 1. 初始化每个字符串对应的哈希表\n    for i in range(n):\n        word = words[i]\n        for ch in word:\n            flags[i][ord(ch) - ord('a')] = 1\n    \n    result = 0\n    for i in range(n):\n        # Step 2. 根据哈希表判断每对字符串是否包含相同的字符\n        for j in range(i + 1, n):\n            k = 0\n            while k < 26:\n                if flags[i][k] and flags[j][k]:\n                    break\n                k += 1\n            \n            # Step 3. 如果所有字符都不相同，计算乘积\n            if k == 26:\n                prod = len(words[i]) * len(words[j])\n                result = max(result, prod)\n    \n    return result","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"第 1 步如果 words 的长度为 n，平均每个字符串长度为 k，那么时间复杂度为 mathcalO(nk)；第 2 步总共有 n^2 对字符串，时间复杂度为 mathcalO(n^2)，总的时间复杂度为 mathcalO(nk+n^2).","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"用整数的二进制数位记录字符串中出现的字符：int 型整数的有 32 个数位，将二进制的最右边代表 ‘a’，倒数最右代表 ‘b’，对应的数位为 1 则代表 words[i] 包含该字母，否则不包含。如果两个字符串没有相同的字符，那么它们对应的整数的「与」运算结果等于 0.","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"def maxProduct(words: List[str]):\n    n = len(words)\n    flags = [0 for _ in range(n)]\n    for i in range(n):\n        word = word[i]\n        for ch in word:\n            flag[i] |= 1 << (ch - 'a')\n            \n    result = 0\n    for i in range(n):\n        for j in range(i + 1, n):\n            if (flags[i] & flags[j] == 0):\n                prod = len(words[i]) * len(words[j])\n                result = max(result, prod)\n    \n    return result","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"这种解法的时间复杂度也是 mathcalO(nk+n^2)，空间复杂度 mathcalO(n)，但是这种解法在判断两个字符串是否包含相同字符时只需要 1 次运算，而前面的需要 26 次。","category":"page"},{"location":"interview/#面试题-6：排序数组中的两个数字之和","page":"面试笔试","title":"面试题 6：排序数组中的两个数字之和","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"题目：输入一个递增排序的数组和一个值 k，请问如何在数组中找出两个和为 k 的数字并返回它们的下标？假设数组中存在且只存在一对符合条件的数字，同时一个数字不能使用两次。例如，输入数组 [1，2，4，6，10]，k 的值为 8，数组中的数字 2 与 6 的和为 8，它们的下标分别为 1 与 3。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"双指针：初始下指针 p1 指向数组下标 0，指针 p2 指向数组末尾，如果两个指针指向的数字之和小于 k，可以把指针 p1 向右移动增加和的大小；如果两个指针指向的数字之和大于 k，可以把指针 p2 向左移动减小和的大小；如果两个指针指向的数字之和等于 k，那么就找到了符合条件的两个数字。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"def twoSum(numbers: int, target: int):\n    i = 0\n    j = len(numbers) - 1\n    while i < j and numbers[i] + numbers[j] != target:\n        if numbers[i] + numbers[j] < target:\n            i += 1\n        else:\n            j -= 1\n    \n    return [i, j]","category":"page"},{"location":"interview/#面试题-7：数组中和为-0-的-3-个数字","page":"面试笔试","title":"面试题 7：数组中和为 0 的 3 个数字","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"题目：输入一个数组，如何找出数组中所有和为 0 的 3 个数字的三元组？需要注意的是，返回值中不得包含重复的三元组。例如，在数组 [-1，0，1，2，-1，-4] 中有两个三元组的和为 0，它们分别是 [-1，0，1] 和 [-1，-1，2]。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"先对数组进行排序，在固定用变量 i 指向的数字之后，用函数 twoSum 在排序后的数组中找出所有下标大于 i 并且和为 -nums[i] 的两个数字（下标分别为 j 和 k）。如果 nums[i], nums[j], nums[k] 的和大于 0，那么下标 k 向左移动；如果 nums[i], nums[j], nums[k] 的和小于 0，那么下标 j 向右移动；如果 3 个数字之和正好等于 0，那么向右移动下标 j，以便找到其他和为 -nums[i] 的两个数字。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"def threeSum(nums: List[int]) -> List[List[int]]:\n    n = len(nums)\n    result = []\n    if n >= 3:\n        nums.sort()\n        i = 0\n        while (i < n - 2):\n            twoSum(nums, i, result)\n            temp = nums[i]\n            while(i < n and nums[i] == temp):\n                i += 1\n    return result\n\ndef twoSum(nums: List[int], i: int, result: List[List[int]]) -> None:\n    n = len(nums)\n    j = i + 1\n    k = n - 1\n    while j < k:\n        if nums[i] + nums[j] + nums[k] == 0:\n            result.append([i, j, k])\n            \n            temp = nums[j]\n            while (nums[j] == temp and j < k):\n                j += 1\n            elif nums[i] + nums[j] + nums[k] < 0:\n                j += 1\n            else:\n                k -= 1","category":"page"},{"location":"interview/#面试题-8：和大于或等于-k-的最短子数组","page":"面试笔试","title":"面试题 8：和大于或等于 k 的最短子数组","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"题目：输入一个正整数组成的数组和一个正整数 k，请问数组中和大于或等于 k 的连续子数组的最短长度是多少？如果不存在所有数字之和大于或等于 k 的子数组，则返回 0。例如，输入数组 [5，1，4，3]，k 的值为 7，和大于或等于 7 的最短连续子数组是 [4，3]，因此输出它的长度 2。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"子数组由数组中一个或连续的多个数字组成。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"指针 p1 和 p2 初始指向数组的第 1 个元素，由于数组中的数字都是正整数：","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"如果两个指针之间的子数组中所有数字之和大于等于 k，那么 p1 向右移动，相当于子数组删除最左边的元素；\n如果两个指针之间的子数组中所有数字之和小于 k，那么 p2 向右移动，相当于子数组在最右边添加一个数字；","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"def minSubArrayLen(k: int, nums: List[int]) -> int:\n    left = 0\n    sum_ = 0 \n    minLength = float('inf')\n    for right in range(len(nums)):\n        sum_ += nums[right]\n        while left <= right and sum_ >= k:\n            minLength = min(minLength, right - left + 1)\n            sum_ -= nums[left]\n            left += 1\n    \n    return minLength if minLength < float('inf') else 0","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"时间复杂度：mathcalO(n)","category":"page"},{"location":"interview/#面试题-9：乘积小于-k-的子数组","page":"面试笔试","title":"面试题 9：乘积小于 k 的子数组","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"题目：输入一个由正整数组成的数组和一个正整数 k，请问数组中有多少个数字乘积小于 k 的连续子数组？例如，输入数组 [10，5，2，6]，k 的值为 100，有 8 个子数组的所有数字的乘积小于 100，它们分别是 [10]、[5]、[2]、[6]、[10，5]、[5，2]、[2，6] 和 [5，2，6]。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"用指针 p1 和 p2 指向数组中的两个数字，初始时都指向数组的第一个元素，指针 p1 永远不会走到指针 p2 的右边：","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"如果两个指针之间的子数组中数字的乘积小于 k，指针 p2 向右移动；\n如果两个指针之间的子数组中数字的乘积大于等于 k，指针 p1 向右移动；","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"找出所有数字乘积小于 k 的子数组的个数：一旦 p1 向右移动到某个位置时子数组的乘积小于 k，只需要保持 p2 不懂，向右移动 p1 形成的所有子数组的数字乘积就一定小于 k","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"def numSubarrayProductLessThanK(nums: List[int], k: int) -> int:\n    product = 1\n    left = 0\n    count = 0\n    for right in range(len(nums)):\n        product *= nums[right]\n        while left <= right and product >= k:\n            product /= nums[left]\n            left += 1\n    \n        count += right - left + 1 if right >= left else 0\n    \n    return count","category":"page"},{"location":"interview/#面试题-10：和为-k-的子数组","page":"面试笔试","title":"面试题 10：和为 k 的子数组","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"题目：输入一个整数数组和一个整数 k，请问数组中有多少个数字之和等于 k 的连续子数组？例如，输入数组 [1，1，1]，k 的值为 2，有 2 个连续子数组之和等于 2。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"注意：该问题与前面两个问题的区别在于，数组中的数字不是正整数，如果使用双指针的方式，无法保证指针右移，数组中的和就变大（或变小）。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"from collections import defaultdict\ndef subarraySum(nums: List[int], k; int) -> int:\n    sumToCount = defaultdict(int)  # 哈希表保存从第 1 个数到当前扫描到的数字之间的数字之和\n    sumToCount[0] = 1\n    \n    sum_ = 0\n    count = 0\n    for num in nums:\n        sum_ += num\n        count += sumToCount[sum_ - k]\n        sumToCount[sum_] = sumToCount[sum_] + 1\n    \n    return count","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"时间复杂度：mathcalO(n)；空间复杂度：mathcalO(n)","category":"page"},{"location":"interview/#面试题-11：0-和-1-的个数相同的子数组","page":"面试笔试","title":"面试题 11：0 和 1 的个数相同的子数组","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"题目：输入一个只包含 0 和 1 的数组，请问如何求 0 和 1 的个数相同的最长连续子数组的长度？例如，在数组 [0，1，0] 中有两个子数组包含相同个数的 0 和 1，分别是 [0，1] 和 [1，0]，它们的长度都是 2，因此输出 2。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"分析：把输入数组中所有的 0 替换成 -1，在一个只包含数字 1 和 -1 的数组中，子数组中的 -1 和 1 的数目相同，那么子数组的所有数字之和就是 0，题目变成求数字之和为 0 的最长子数组的长度。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"如果数组中前 i 个数字之和为 m，前 j 个数字（j > i）之和也为 m，那么从第 i+1 个数字到第 j 个数字的子数组的数字之和为 0，这个数组的长度是 j-i","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"把数组从第 1 个数字开始到当前扫描到的数字累加之和保存到一个哈希表中，哈希表的 key 是从第 1 个数字开始累加到当前扫描到的数字之和，value 是当前扫描的数字的下标。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"def findMaxLength(nums: List[int]) -> int:\n    sumToIndex = dict()\n    sumToIndex[0] = -1\n    sum_ = 0\n    maxLength = 0\n    for i in range(len(nums)):\n        sum_ += -1 if nums[i] == 0 else 1\n        if sum_ in sumToIndex.keys():\n            maxLength = max(maxLength, i - sumToIndex(sum_))\n        else:\n            sumToIndex[sum] = i\n            \n    return maxLength","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"时间复杂度：mathcalO(n)，空间复杂度：mathcalO(n)","category":"page"},{"location":"interview/#面试题-12：左右两边子数组的和相等","page":"面试笔试","title":"面试题 12：左右两边子数组的和相等","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"题目：输入一个整数数组，如果一个数字左边的子数组的数字之和等于右边的子数组的数字之和，那么返回该数字的下标。如果存在多个这样的数字，则返回最左边一个数字的下标。如果不存在这样的数字，则返回 -1。例如，在数组 [1，7，3，6，2，9] 中，下标为 3 的数字（值为 6）的左边 3 个数字 1、7、3 的和与右边两个数字 2 和 9 的和相等，都是 11，因此正确的输出值是 3。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"在第 i 个元素左边的子数组的和是从第 1 个数字累加到第 i-1 个数字的和，右边的子数组（不包括第 i 个）的数字之和就是从第 i+1 数字开始累加到最后一个数字的和，这个和也等于数组所有数字之和减去第 1 个数字累加到第 i 个数字的和。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"先得到整个数组的和；\n记录数组累加到第 i 个数字的和；\n遍历到第 i 个数字时，用整个数组的和减去累加到第 i 个数字，就是右边的子数组数字之和；","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"def pivotIndex(nums: List[int]) -> int:\n    total = 0\n    for num in nums:\n        total += num\n    \n    sum_ = 0\n    for i in range(len(nums)):\n        sum_ += nums[i]\n        if sum_ - nums[i] == total - sum_:\n            return i\n    \n    return -1","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"时间复杂度：mathcalO(n)；空间复杂度：mathcalO(1)","category":"page"},{"location":"interview/#面试题-13：二维子矩阵的数字之和","page":"面试笔试","title":"面试题 13：二维子矩阵的数字之和","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"题目：输入一个二维矩阵，如何计算给定左上角坐标和右下角坐标的子矩阵的数字之和？对于同一个二维矩阵，计算子矩阵的数字之和的函数可能由于输入不同的坐标而被反复调用多次。例如，输入图 2.1 中的二维矩阵，以及左上角坐标为（2，1）和右下角坐标为（4，3）的子矩阵，该函数输出 8。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"# +---+---+---+---+---+\n# | 3 | 0 | 1 | 4 | 2 |\n# +---+---+---+---+---+\n# | 5 | 6 | 3 | 2 | 1 |\n# +---┌───┬───┬───┐---+\n# | 1 │ 2 │ 0 │ 1 │ 5 |\n# +---├───┼───┼───┤---+\n# | 4 │ 1 │ 0 │ 1 │ 7 |\n# +---├───┼───┼───┤---+\n# | 1 │ 0 │ 3 │ 0 │ 5 |\n# +---└───┴───┴───┘---+\n# \n# 图 2.1 一个 5x5 的二维数组","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"左上角坐标为 (r1, c1)，右下角坐标为 (r2, c2) 的子矩阵的数字之和可以用左上角都为 (0, 0)，右下角为 (r2, c2) 的子矩阵减去右下角为 (r1-1, c2) 的子矩阵，减去右下角为 (r2, c1-1) 的子矩阵，再加上右下角为 (r1-1, c1-1) 的子矩阵。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"假设矩阵 sums[i][j] 是左上角 (0, 0) 到右下角 (i, j) 的子矩阵的数字之和，那么问题的答案就是 sums[r2][c2] + sums[r1-1][c2] - sums[r2][c1-1] + sums[r1-1][c1-1]","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"class NumMatrix:\n    def __init__(self):\n        pass\n    \n    def NumMatrix(self, matrix: List[List[int]]) -> None:\n        m = len(matrix)\n        if m == 0: return\n        n = len(matrix[0])\n        if n == 0: return \n        \n        sums = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n        for i in range(n):\n            rowSum = 0\n            for j in range(m):\n                rowSum += matrix[i][j]\n                sums[i + 1][j + 1] = sums[i][j + 1] + rowSum\n        \n    def sumRegion(self, row1: int, col1: int, row2: int, col2: int) -> int:\n        return sums[row2 + 1][col2 + 1] - sums[row1][col2 + 1] - sums[row2 + 1][col1] + sums[row1][col1]","category":"page"},{"location":"interview/#面试题-14：字符串中的变位词","page":"面试笔试","title":"面试题 14：字符串中的变位词","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"题目：输入字符串 s1 和 s2，如何判断字符串 s2 中是否包含字符串 s1 的某个变位词？如果字符串 s2 中包含字符串 s1 的某个变位词，则字符串 s1 至少有一个变位词是字符串 s2 的子字符串。假设两个字符串中只包含英文小写字母。例如，字符串 s1 为 \"ac\"，字符串 s2 为 \"dgcaf\"，由于字符串 s2 中包含字符串 s1 的变位词 \"ca\"，因此输出为 true。如果字符串 s1 为 \"ab\"，字符串 s2 为 \"dgcaf\"，则输出为 false。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"变位词：指组成各个单词的字母及每个字母出现的次数完全相同，只是字母排列的顺序不同。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"def checkInclusion(s1: str, s2: str) -> bool:\n    n1 = len(s1)\n    n2 = len(s2)\n    if len(s2) < len(s1): return False\n    \n    # 用哈希表来记录字符串中每个字母的出现次数\n    counts = [0 for _ in range(26)]\n\n    for i in range(n1):\n        counts[s1[i] - 'a'] += 1\n        counts[s2[i] - 'a'] -= 1\n    \n    if areAllZero(counts): return True\n\t\n    for i in range(n1, n2):\n        counts[s2[i] - 'a'] -= 1\n        counts[s2[i - n1] - 'a'] += 1\n        if areAllZero(counts): return True\n    \n    return False\n\ndef areAllZero(counts: List[int]) -> bool:\n    for count in counts:\n        if count != 0: return False\n    \n    return True","category":"page"},{"location":"interview/#面试题-15：字符串中的所有变位词","page":"面试笔试","title":"面试题 15：字符串中的所有变位词","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"题目：输入字符串 s1 和 s2，如何找出字符串 s2 的所有变位词在字符串 s1 中的起始下标？假设两个字符串中只包含英文小写字母。例如，字符串 s1 为 \"cbadabacg\"，字符串 s2 为 \"abc\"，字符串 s2 的两个变位词 \"cba\" 和 \"bac\" 是字符串 s1 中的子字符串，输出它们在字符串 s1 中的起始下标 0 和 5。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"def findAnagrams(s1: str, s2: str) -> List[int]:\n    indices = list()\n    n1, n2 = len(s1), len(s2)\n    if n1 < n2: return indices\n    \n    counts = [0 for _ in range(26)]\n    i = 0\n    while i < n2:\n        counts[s2[i] - 'a'] += 1\n        counts[s1[i] - 'a'] -= 1\n        i += 1\n    \n    if areAllZero(counts):\n        indices.append(0)\n    \n    while i < n1:\n        counts[s1[i] - 'a'] -= 1\n        counts[s1[i - n2] - 'a'] += 1\n        if areAllZero(counts):\n            indices.append(i - n2 + 1)\n        i += 1\n        \n    return indices","category":"page"},{"location":"interview/#面试题-16：-不含重复字符的最长子字符串","page":"面试笔试","title":"面试题 16： 不含重复字符的最长子字符串","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"题目：输入一个字符串，求该字符串中不含重复字符的最长子字符串的长度。例如，输入字符串 \"babcca\"，其最长的不含重复字符的子字符串是 \"abc\"，长度为 3。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"用一个哈希表记录字符串出现的次数、如果子字符串中不含重复字符，那么它对应的哈希表中没有比 1 大的值。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"def lengthOfLongestSubstring(s: str) -> int:\n    n = len(s)\n    if n == 0: return 0\n    \n    # ASCII 码总共有 256 个字符\n    counts = [0 for _ in range(256)]\n    \n    i, j = 0, -1  # i 是右指针，j 是左指针\n    longest = 1\n    while i < n:\n        counts[s[i]] += 1\n        while hasGreaterThan1(counts):\n            j += 1\n            counts[s[j]] -= 1\n        \n        longest = max(i - j, longest)\n    \n    return longest\n\ndef hasGreaterThan1(counts: int) -> bool:\n    for count in counts:\n        if count > 1:\n            return True\n    \n    return False","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"优化：定义一个变量 countDup 来存储哈希表中大于 1 的数字的个数。当一个字符对应的数字从 1 变成 2 时，countDup 加 1；当一个字符对应的数字由 2 变成 1 时，countDup 减 1.","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"def lengthOfLongestSubstring(s: str) -> int:\n    n = len(s)\n    if n == 0: return 0\n    \n    counts = [0 for _ in range(256)]\n    i, j = 0, -1\n    longest = 1\n    countDup = 0\n    while i < n:\n        counts[s[i]] += 1\n        if counts[s[i]] == 2:\n            countDup += 1\n        \n        while countDup > 0:\n            j += 1\n            counts[s[j]] -= 1\n            if counts[s[j]] == 1:\n                countDup -= 1\n        \n        longest = max(i - j, longest)\n    \n    return longest","category":"page"},{"location":"interview/#面试题-17：包含所有字符的最短字符串","page":"面试笔试","title":"面试题 17：包含所有字符的最短字符串","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"题目：输入两个字符串 s 和 t，请找出字符串 s 中包含字符串 t 的所有字符的最短子字符串。例如，输入的字符串 s 为 \"ADDBANCAD\"，字符串 t 为 \"ABC\"，则字符串 s 中包含字符 'A'、'B' 和 'C' 的最短子字符串是 \"BANC\"。如果不存在符合条件的子字符串，则返回空字符串 \"\"。如果存在多个符合条件的子字符串，则返回任意一个。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"如果一个字符串 s 中包含另一个字符串 t 的所有字符，那么字符串 t 的所有字符在字符串 s 中都出现，并且同一个字符在字符串 s 中出现的次数不少于在字符串 t 中出现的次数。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"from collections import defaultdict\ndef minWindow(s: str, t: str):\n    charToCount = defarltdict(int)\n    for ch in t:\n        charToCount[ch] = charToCount[ch] + 1\n    \n    count = len(charToCount)\n    start, end = 0, 0\n    minStart, minEnd = 0, 0\n    minLength = float('inf')\n    while end < len(s) or (count == 0 and end == len(s)):\n        if count > 0:\n            endCh = s[end]\n            if endCh in charToCount:\n                charToCount[endCh] -= 1\n                if charToCound[endCh] == 0:\n                    count -= 1\n            \n            end += 1\n        else:\n            if end - start < minLength:\n                minLength = end - start\n                minStart = start\n                minEnd = end\n            \n            startCh = s[start]\n            if startCh in charToCount:\n                charToCount[startCh] += 1\n                if charToCount[startCh] == 1:\n                    count += 1\n            \n            start += 1\n\n    return s[minStart, minEnd] if minLength < float('inf') else \"\"","category":"page"},{"location":"interview/#LeetCode-微软模拟笔试-1","page":"面试笔试","title":"LeetCode 微软模拟笔试 1","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"给定两个字符串 a 和 b，寻找重复叠加字符串 a 的最小次数，使得字符串 b 成为叠加后的字符串 a 的子串，如果不存在则返回 -1。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"注意：字符串 \"abc\" 重复叠加 0 次是 \"\"，重复叠加 1 次是 \"abc\"，重复叠加 2 次是 \"abcabc\"。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"class Solution:\n    def repeatedStringMatch(self, a: str, b: str) -> int:\n        # 看看 b 中的字母是不是 a 都有\n        if not set(b).issubset(set(a)): return -1\n        start = -1\n        n = len(a)\n        for i in range(n):\n            if a[i] == b[0]:\n                start = i\n                cnt = 1\n                for j in range(len(b)):\n                    if start == n:\n                        start = 0\n                        cnt += 1\n                    if a[start] == b[j]:\n                        if j == len(b) - 1:\n                            return cnt\n                        start += 1\n                    else: break\n\n        return -1","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"给定一个二叉树的 根节点 root，请找出该二叉树的 最底层 最左边 节点的值。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"假设二叉树中至少有一个节点。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"# Definition for a binary tree node.\n# class TreeNode:\n#     def __init__(self, val=0, left=None, right=None):\n#         self.val = val\n#         self.left = left\n#         self.right = right\nclass Solution:\n    def __init__(self):\n        self.leftistDepth = 0  # 当前最左的节点的深度\n        self.ans = -1  # 最深最左的节点的值\n\n    def pretrack(self, root, depth):\n        if not root: return\n        if not root.left and depth > self.leftistDepth:  # 当前的节点是最左的 并且是 目前为止最深的\n            self.ans = root.val\n            self.leftistDepth = depth\n        self.pretrack(root.left, depth + 1)\n        self.pretrack(root.right, depth + 1)\n\n    def findBottomLeftValue(self, root: Optional[TreeNode]) -> int:\n        self.pretrack(root, 1)\n        return self.ans","category":"page"},{"location":"interview/#下一个排列","page":"面试笔试","title":"下一个排列","text":"","category":"section"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"给你一个整数数组 nums ，找出 nums 在字典序中的下一个排列。必须 原地 修改，只允许使用额外常数空间。","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"需要找到一个左边的「较小数」和右边的「较大数」，同时要求「较小数」尽量靠右，「较大数」尽可能小。具体做法如下：","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"从后向前查找第一个顺序对 (i, i + 1)，满足 a[i] < a[i + 1]，a[i] 即为想要的「较小数」，此时 (i + 1, n) 必然是下降序列；\n在区间 [i + 1, n - 1] 中从后向前查找第一个元素 j 满足 a[i] < a[j]，a[j] 即为想要找的「较大数」；\n交换 a[i] 与 a[j]，可以证明区间 [i + 1, n - 1] 必为降序，我们可以直接使用双指针反转区间 [i + 1, n - 1] 使其变为升序；","category":"page"},{"location":"interview/","page":"面试笔试","title":"面试笔试","text":"如果步骤 1 找不到满足条件的顺序对，说明当前序列已经是一个降序序列，是最大的序列，可以直接跳过步骤 2 执行步骤 3。","category":"page"},{"location":"AI/GNN/#通用框架","page":"图神经网络","title":"通用框架","text":"","category":"section"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"除了图神经网络的不同变体，人们还提出了一些通用框架，旨在将不同的模型集成到单一的框架中。^1","category":"page"},{"location":"AI/GNN/#消息传递神经网络","page":"图神经网络","title":"消息传递神经网络","text":"","category":"section"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"消息传递神经网络^2（MPNN, Message Passing Neural Network）包含两个阶段：消息传递阶段和读出阶段。\t","category":"page"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"","category":"page"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"参考：","category":"page"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"[1] 刘知远，周界，《图神经网络导论》","category":"page"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"[2]  J. Gilmmer, S. S. Schoenholz, P. F. Riley, et al. Neural message passing for quantum chemistry. In Proc. of ICML, 2018: 1263-1272. ","category":"page"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"[3] 刘忠雨，李彦霖，周洋，《深入浅出图神经网络》","category":"page"},{"location":"AI/Transformer/#Transformer-知识总结","page":"Transformer","title":"Transformer 知识总结","text":"","category":"section"},{"location":"AI/Transformer/#原理","page":"Transformer","title":"原理","text":"","category":"section"},{"location":"AI/Transformer/","page":"Transformer","title":"Transformer","text":"Transformer 整个网络结构由 Attention 机制组成。在 RNN（包括 LSTM、GRU 等）中计算是顺序的，只能从左向右或者从右向左依次计算，这种机制带来的 2 个问题：","category":"page"},{"location":"AI/Transformer/","page":"Transformer","title":"Transformer","text":"时间片 t 的计算依赖 t-1 时刻的计算结果，限制了模型的并行能力；\n顺序计算的过程中信息会丢失。尽管 LSTM 使用门机制的结构来缓解长期依赖的问题，但是在特别长期时依旧表现不好；","category":"page"},{"location":"AI/Transformer/","page":"Transformer","title":"Transformer","text":"Transformer 通过以下方式来解决上面的问题：","category":"page"},{"location":"AI/Transformer/","page":"Transformer","title":"Transformer","text":"使用 Attention 机制，讲序列中的任意两个位置之间的距离缩小为一个常量；\n因为不是类似 RNN 的顺序结构，因此具有更好的并行性。也更为符合现有的 GPU 框架；","category":"page"},{"location":"AI/Transformer/#Encoder-和-Decoder-模块","page":"Transformer","title":"Encoder 和 Decoder 模块","text":"","category":"section"},{"location":"AI/Transformer/","page":"Transformer","title":"Transformer","text":"Encoder 模块将 Backbone 输出的 feature map 转换成一维表征，然后结合 positional encoding 作为 Encoder 的输入。每个 Encoder 都由 Multi-Head Self-Attention 和 FFN 组成。和 Transformer Encoder 不同的是，因为 Encoder 具有位置不变性，DETR 将 positional encoding 添加到每一个 Multi-Head Self-Attention 中，来保证目标检测的位置敏感性。","category":"page"},{"location":"AI/Transformer/","page":"Transformer","title":"Transformer","text":"Decoder 也具有位置不变性，Decoder 的 n 个 object query（可以理解为学习不同 object 的 positional embedding）必须是不同的，以便产生不同的结果，并且同时把它们添加到每一个 Multi-Head Attention 中。n 个 object queries 通过 Decoder 转换成一个 output embedding，然后 output embedding 通过 FFN 独立解码出 n 个预测结果，包含 box 和 class。对输入 embedding 同时使用 Self-Attention 和 Encoder-Decoder Attention，模型可以利用目标的相互关系来进行全局推理。","category":"page"},{"location":"AI/Transformer/","page":"Transformer","title":"Transformer","text":"和 Transformer Decoder 不同的是，DETR 的每个 Decoder 并行输出 n 个对象，Transformer Decoder 使用的是自回归模型，串行输出 n 个对象，每次只能预测一个输出序列的一个元素。","category":"page"},{"location":"AI/Transformer/#多头注意力（Multi-Head-Attention）","page":"Transformer","title":"多头注意力（Multi-Head Attention）","text":"","category":"section"},{"location":"AI/Transformer/","page":"Transformer","title":"Transformer","text":"多头注意力的提出是为了对同一 key、value、query，希望抽取不同的信息，例如短距离和长距离，类似于 CV 中的感受野（field）。","category":"page"},{"location":"AI/Transformer/#参考","page":"Transformer","title":"参考","text":"","category":"section"},{"location":"AI/Transformer/","page":"Transformer","title":"Transformer","text":"[1] 知乎专栏：计算机视觉面试题 - Transformer 相关问题总结，作者：爱者之贻","category":"page"},{"location":"AI/DeepLearn/#逐层归一化","page":"-","title":"逐层归一化","text":"","category":"section"},{"location":"AI/DeepLearn/","page":"-","title":"-","text":"逐层归一化（Layer-wise Normalization）是将传统机器学习中的数据归一化应用到深度神经网络中，对神经网络中隐藏层的输入进行归一化，从而使得网络更容易训练。","category":"page"},{"location":"AI/DeepLearn/#批量归一化","page":"-","title":"批量归一化","text":"","category":"section"},{"location":"AI/DeepLearn/","page":"-","title":"-","text":"批量归一化（Batch Normalization, BN）中，如果 input batch 的 shape 为 (B, C, H, W)，统计出的 mean 和 variance 的 shape 为 (1, C, 1, 1). ","category":"page"},{"location":"AI/RNN/#循环神经网络","page":"循环神经网络","title":"循环神经网络","text":"","category":"section"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"记录网络的输入序列为 x_1x_2cdotsx_n，一个循环神经网络（RNN）展开后可以看做一个 n 层的前馈神经网络，第 t 层对应着 t 时刻的状态（t=12cdotsn），记第 t 层（时刻）的输入状态、隐藏状态、输出状态分别为 x_t h_t o_t，训练时的目标输出值为 y_t，则有：隐藏状态 h_t 由当前时刻的输入状态 x_t 和上一时刻的隐藏状态 h_t-1 共同确定，即","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"h_t=sigma(Ux_t+Wh_t-1+b)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中，U 是输入层到隐藏层的权重矩阵，W 是不同时刻的隐藏层之间的连接权重，b 是偏置向量，sigma(cdot) 是激活函数（通常使用 textttTanh 函数）。循环神经网络最大的特点就是当前时刻的隐藏状态不仅与当前时刻的输入状态有关，还受上一时刻的隐藏状态影响。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"输出状态 o_t 的计算公式为：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"o_t=g(Vh_t+c)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中，V 是隐藏层到输出层的权重矩阵，c 是偏置向量，g(cdot) 是输出层的激活函数（对于分类任务可以采用 textttSoftmax 函数）。在训练时，网络在整个序列上的损失可以定义为不同时刻的损失之和：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"mathcalL=sum_tmathcalL_t=sum_t Loss(o_ty_t)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"上述的权重矩阵 U W V 是所有时刻共享参数的，这种机制不仅可以极大地减少网络需要学习的参数数量，而且使得网络可以处理长度不固定的输入序列。在 RNN 的训练过程中，由于不同时刻的状态是相互依赖的，因此需要存储各个时刻的状态信息，而且无法进行并行计算，这导致整个训练过程内存消耗大，并且速度较慢。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"RNN 之所以能够在序列数据的处理上获得出色的表现，是因为它拥有长期记忆功能，能够压缩并获得长期数据的表示。实际上，在 RNN 训练过程中，为了防止梯度爆炸（或弥散）的问题，通常采用带截断的反向传播算法，即仅反向传播 k 个时间步的梯度。理论上的无限记忆优势在实际中几乎不存在。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"实际上，在序列任务中，卷积神经网络（CNN）在空洞卷积的帮助下（例如 TextCNN），具有更好的并行化和可训练性。只不过长短期记忆网络（LSTM）和 Seq2Seq 网络依然是序列数据处理中最为通用的架构。还有人很多工作对卷积神经网络和循环神经网络进行组合使用，提升序列数据处理能力（如 TrellisNet）。","category":"page"},{"location":"AI/RNN/#随时间反向传播算法（BPTT）","page":"循环神经网络","title":"随时间反向传播算法（BPTT）","text":"","category":"section"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"随时间反向传播算法（BackPropagation Through Time, BPTT）将 RNN 看做一个展开的多层前馈网络，每一层对应 RNN 中的每个时刻。在展开的前馈网络中，所有层的参数是共享的，因此参数的真实梯度是所有展开层的参数梯度之和。","category":"page"},{"location":"AI/RNN/#长短期记忆网络（LSTM）","page":"循环神经网络","title":"长短期记忆网络（LSTM）","text":"","category":"section"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"LSTM 在标准 RNN 基础上做了改进，解决 RNN 训练过程中的梯度消失问题。增加了控制门单元：遗忘门、输入门、输出门，这些门控单元组合成 cell 状态，可以保证 LSTM 在长序列场景下的信息保持。","category":"page"},{"location":"AI/RNN/#长程依赖问题","page":"循环神经网络","title":"长程依赖问题","text":"","category":"section"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"LSTM 是如何实现长短期记忆功能的？","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（1）一般的 RNN 中，只有一个隐藏状态（hidden state）单元 h_t，不同时刻隐藏状态单元的参数是相同（共享）的。LSTM 在普通 RNN 的基础上增加了一个元胞状态（cell state）单元 c_t，其在不同时刻有着可变的连接权重，可解决普通循环神经网络中的梯度消失或爆炸问题。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（2）LSTM 引入了门控单元，是神经网络学习到的用于控制信号的存储、利用和舍弃的单元。对于每一个时刻 t，LSTM 有输入门 i_t、遗忘门 f_i 和输出门 o_t 共 3 个门控单元。每个门控单元的输入包括当前时刻的序列信息 x_t 和上一时刻的隐藏状态单元 h_t-1，具体公式：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"i_t=sigmaleft( W_i x_t + U_i h_t-1 + b_i right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"f_t=sigmaleft( W_f x_t + U_f h_t-1 + b_f right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"o_t=sigmaleft( W_o x_t + U_o h_t-1 + b_o right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"3 个门控单元都相当于一个全连接层，激活函数 sigma(cdot) 的取值范围是 0 1，常用 textttSigmoid 作为激活函数。当门控单元的状态为 0 时，信号会被全部丢弃；当状态为 1 时，信号会被全部保留。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（3）元胞状态单元从上一个时刻 c_t-1 到当前时刻 c_t 的转移是由输入门和遗忘门共同控制的。输入门决定了当前时刻输入信息 tildec_t 有多少被吸收，遗忘门决定了上一时刻元胞状态单元 c_t-1 有多少不被遗忘，最终的元胞状态单元 c_t 由两个门控处理后的信号取和产生。具体公式：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"tildec_t = textttTanhleft( W_c x_t + U_c h_t-1 + b_c right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"c_t = f_t odot c_t-1 + i_t odot tildec_t","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中 odot 为逐元素点乘操作。LSTM 的隐藏状态单元 h_t 则由输出门 c_t 决定：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"h_t = o_t odot textttTanh(c_t)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"不仅隐藏状态单元 h_t-1 和 h_t 之间有着较为复杂的循环连接，内部的元胞状态单元 c_t-1 和 c_t 之间还具有线性自循环关系，这个关系可以看作是在滑动处理不同时刻的信息。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（4）LSTM 中遗忘门和输出门的激活函数十分重要。删除遗忘门的激活函数会导致之前的元胞状态不能很好地被抑制；而删除输出门的激活函数则可能会出现非常大的输出状态。","category":"page"},{"location":"AI/RNN/#门控循环单元（GRU）","page":"循环神经网络","title":"门控循环单元（GRU）","text":"","category":"section"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（1）GRU 只有两个门控单元，分别为重置门 r_t 和 更新门 z_t，一个控制短期记忆，另一个控制长期记忆。重置门控制前一状态有多少信息被写入到当前的候选集上，其值越小，前一个状态的信息被写入的就越少；更新门用于控制前一个时刻的状态信息被代入到当前状态中的程度，其值越大，说明前一个时刻的状态信息带入越多。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（2）GRU 中每个门控单元的输入包括当前时刻和序列信息 x_t 和上一时刻的隐藏状态单元 h_t-1，具体计算公式为：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"r_t = sigmaleft( W_r x_t + U_r h_t-1 right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"z_t = sigmaleft( W_z x_t + U_z h_t-1 right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中 sigma(cdot) 是激活函数，一般用 textttSigmoid。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（3）GRU 中重置门决定先前的隐藏状态单元是否被忽略，而更新门则控制当前隐藏状态单元是否需要被新的隐藏状态单元更新，具体公式：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"tildeh_t = textttTanh left( W_h x_t + U_h (r_t odot h_t-1) right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"h_t = (1 - z_t) h_t - 1 + z_t tildeh_t","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中，(1 - z_t)h_t-1 表示上一时刻保留下来（没被遗忘）的信息，z_t tildeh_t 是当前时刻记忆下来的信息。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"用 1 - z_t z_t  作为系数，表明对上一时刻遗忘多少权重的信息，就会在这一时刻记忆多少权重的信息以作为弥补。GRU 就是用这样的一种方式用一个更新门 z_t 实现遗忘和记忆两个功能。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（4）GRU 只有一个隐藏状态单元 h_t，而 LSTM 有隐藏状态单元 h_t 和元胞状态单元 c_t。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（5）GRU 具有更少的参数，更易于计算和实现。在不同数据集、不同超参配置下，可以取得与 LSTM 相当甚至更好的性能，并且具有更快地收敛速度。","category":"page"},{"location":"AI/RNN/#序列到序列（Seq2Seq）","page":"循环神经网络","title":"序列到序列（Seq2Seq）","text":"","category":"section"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（1）Seq2Seq 的映射架构能够将一个可变长序列映射到另一个可变长序列。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（2）Seq2Seq 框架由于输入序列和输出序列是不等长的因此整个处理过程需要拆分为对序列的理解和翻译，也就是编码和解码。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（3）采用一个固定尺寸的状态向量 C 作为编码器与解码器之间的「桥梁」。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（4）假设输入序列为 X=(x_1x_2cdotsx_T)，编码器可以是一个简单的循环神经网络，其隐藏状态 h_t 的计算公式为：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"h_t = f(h_t-1 x_t)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中，f(cdot) 是非线性激活函数，可以是简单的 textttSigmoid 函数，也可以是复杂的门控函数（LSTM、GRU 等）。将上述循环神经网络（编码器）最后一个时刻的隐藏状态 h_T 作为状态向量，并输入到解码器。C 是一个尺寸固定的向量，并且包含了整个序列的所有信息。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（5）解码器需要根据固定尺寸的状态向量 C 来生成长度可变的解码序列 Y=(y_1 y_2 cdots y_T)。这里解码序列的长度 T^prime 和编码长度 T 可以是不同的。解码器也可以用一个简单的循环神经网络来实现，其隐藏状态 h_t 可以按照如下公式计算：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"h_t = f(h_t-1y_t-1C)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"解码器的输出由如下公式决定：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"P(y_tmid y_t-1y_t-2cdotsy_1C) = g(h_t y_t-1C)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中，g(cdot) 会产生一个概率分布（例如用 textttSoftmax 函数产生概率分布）。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（6）解码器的工作流程：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"首先在收到一个启动信号（如 y_0=text start gt）后开始工作，根据 h_t y_t-1 C 计算出 y_t  的概率分布；\n然后对 y_t 进行采样获得具体取值；\n循环上述操作，直到遇到结束信号（如 y_t=text eos gt；","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（7）解码器的实现还能够用一种更加简单的方式，仅在初始时刻需要状态向量 C，其他时刻仅接收隐藏状态和上一时刻的输出信息 P(y_t)=g(h_ty_t-1)。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（8）在训练时，需要让模型输出的序列尽可能正确，这可以通过最大化对数似然概率来实现：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"max_theta frac1Nsum_n=1^N log p_theta(Y_n mid X_n)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中 theta 为模型参数，X_n 是一个输入序列，Y_n 是对应的输出序列， (X_nY_n) 构成一个训练样本对。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（9）因为是序列到序列的转换，实际应用中可以通过贪心法求解 Seq2Seq，当度量标准、评估方式确定后，解码器每次根据当前的状态和已解码的序列选择一个最佳的解码结果，直至结束。","category":"page"},{"location":"AI/FE/#格莱姆角场（GAF）","page":"特征工程","title":"格莱姆角场（GAF）","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"利用 GAF 算法将一维时间序列数据转为二维图片数据的代码，在 GitHub 项目 Series2Image 上可以找到（记得把 star 点上呀！）","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"原文地址：Encoding Time Series as Images","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"虽然现在深度学习在计算机视觉和语音识别上发展得很好，但是碰到时间序列时，构建预测模型是很难的。原因包括循环神经网络较难训练、一些研究比较难以应用，而且没有现存与训练网络，1D-CNN 不方便。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"但是如果使用 Gramian Angular Field (GAF)，可以把时间序列转成图片，充分利用目前机器视觉上的优势。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"这篇文章会包括下面这些内容：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"数学先验知识；\nGram Matrix 为何可以为单变量的时间序列构建一个好的二维表示；\nGram Matrix 点积为何不能表示 CNN 的数据；\n为 CNN 准备好 Gram Matrix 结构的操作是什么；","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"还会包括 Python 代码：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"用于 GAF 计算的 NumPy 工具；","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"下面的动图展示了对数据进行极坐标编码，然后对生成的角度进行类似于 Gram 矩阵的操作：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(Image: )","category":"page"},{"location":"AI/FE/#.-数学先验知识","page":"特征工程","title":"1. 数学先验知识","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"GAF 的数学方法与内积与相应的 Gram 矩阵有很深的联系。","category":"page"},{"location":"AI/FE/#.1.-点积（Dot-product）","page":"特征工程","title":"1.1. 点积（Dot product）","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"内积是两个向量之间的运算，用来度量它们的「相似性」。它允许使用来自传统 Euclidian Geometry 的概念：长度、角度、第二维度和第三维度的正交性。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"在二维空间上，两个向量 u 和 v 之间的内积定义为：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"langle uvrangle=u_1 cdot v_1+u_2 cdot v_2","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"或者：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"langle uvrangle=ucdot vcdotcos(theta)","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"如果 u 和 v 的范数为 1，我们就得到：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"langle uvrangle=cos(theta)","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"因此，如果处理的是单位向量，他们的内积就只由角度 theta 决定了，这个角度可以用弧度来表示。计算出来的值会在 [-1,1] 内。记住这些定理，在本文的其他位置会用到。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"注意：在 Euclidian 集合（n 维）中，两个向量的内积的正式定义为：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"langle uvrangle=sum_i=1^nu_icdot v_i","category":"page"},{"location":"AI/FE/#.2.-Gram-矩阵（Gram-Matrix）","page":"特征工程","title":"1.2. Gram 矩阵（Gram Matrix）","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"在线性代数和几何中，Gram 矩阵是一个有用的工具，它经常用于计算一组向量的线性相关关系。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"定义：一组 n 个向量的 Gram 矩阵是由每一对向量的点积定义的矩阵。从数学上讲，这可以解释为：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"G=beginpmatrix \n langle u_1v_1rangle  langle u_1v_2rangle  cdots  langle u_1v_nrangle  \n langle u_2v_1rangle  langle u_2v_2rangle  cdots  langle u_2v_nrangle \nvdots  vdots  ddots  vdots \n langle u_nv_1rangle  langle u_nv_2rangle  cdots  langle u_nv_nrangle \nendpmatrix","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"再有，假设所有的二维向量都是单位向量，我们会得到：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"G=beginpmatrix \n cos(phi_11)  cos(phi_12)  cdots  cos(phi_1n)  \n cos(phi_21)  cos(phi_22)  cdots  cos(phi_2n)  \nvdots  vdots  ddots  vdots \n cos(phi_n1)  cos(phi_n2)  cdots  cos(phi_nn)  \nendpmatrix","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"其中 Phi(ij) 是两个向量的夹角。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"关键结论：为什么要用 Gram 矩阵？Gram 矩阵保留了时间依赖性。由于时间随着位置从左上角到右下角的移动而增加，所以时间维度被编码到矩阵的几何结构中。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(Image: )","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"注：单变量时间序列在某种程度上无法解释数据的共现和潜在状态；我们的目标应该是找到替代的和更丰富的表示。","category":"page"},{"location":"AI/FE/#.-实现方式","page":"特征工程","title":"2. 实现方式","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"假设有一时间序列 X=x_1cdotsx_n：","category":"page"},{"location":"AI/FE/#.1.-缩放","page":"特征工程","title":"2.1. 缩放","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"使用一个限定在 [-1,1] 的最小-最大定标器（Min-Max scaler）来把时间序列缩放到 [-1,1] 里，这样做的原因是为了使内积不偏向于值最大的观测。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(Image: )","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"在这个用例中，标准缩放器 不是合适的候选者，因为它的输出范围和产生的内部积都可能超过 [- 1,1]。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"然而，与最小-最大定标器结合，内积确实保留了输出范围：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"langlecdotcdotrangle-11times-11rightarrow-11","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(xy)rightarrow xcdot y","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"在 [-1,1] 中进行点积的选择并不是无害的。如非必要，把 [-1,1] 作为输入范围是非常可取的。","category":"page"},{"location":"AI/FE/#.2.-噪声图片","page":"特征工程","title":"2.2. 噪声图片","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"缩放完时间序列之后，我们计算每一对的点积并把它们放进 Gram 矩阵里：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"G=beginpmatrix \nx_1cdot x_1  x_1cdot x_2  cdots  x_1cdot x_n  \nx_2cdot x_1  x_2cdot x_2  cdots  x_2cdot x_n \nvdots  vdots  ddots  vdots \nx_ncdot x_1  x_ncdot x_2  cdots  x_ncdot x_n \nendpmatrix","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"我们查看一下 G 的值来看一下这个图片：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(Image: )","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"可以看到：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"输出似乎遵循以 0 为中心的高斯分布。\n得到的图片是有噪声的。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"前者解释了后者，因为数据的高斯分布越多，就越难将其与高斯噪声区分开来。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"这对我们的神经网络来说是个问题。此外，CNN 在处理稀疏数据方面表现更好（CNN work better with sparse data）已经得到证实。","category":"page"},{"location":"AI/FE/#.3.-非稀疏性","page":"特征工程","title":"2.3. 非稀疏性","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"高斯分布并不奇怪。当看三维内积值 z 的图像，对所有 (x y)R^² 的可能的组合,我们得到一个点积的三位表面：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(Image: )","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"假设时间序列的值服从均匀分布 [-1,1]，则矩阵的值服从高斯分布。下面是长度为 n 的不同时间序列的 Gram 矩阵值输出的直方图：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(Image: )","category":"page"},{"location":"AI/FE/#.-开始编码","page":"特征工程","title":"3. 开始编码","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"由于单变量时间序列是一维的，点积不能区分有价值的信息和高斯噪声，除了改变空间，没有其他利用「角」关系的方法。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"因此，在使用类似于 Gram 矩阵的构造之前，我们必须将时间序列编码为至少二维的空间。为此，我们将在一维时间序列和二维空间之间构造一个双射映射，这样就不会丢失任何信息。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"这种编码很大程度上是受到极坐标转换的启发，但是在这种情况下，半径坐标表示时间。","category":"page"},{"location":"AI/FE/#.1.-缩放序列","page":"特征工程","title":"3.1. 缩放序列","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"第一步：用 Min-Max scaler 把序列缩放到 [-1,1] 上","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"我们的过程与上面实现方式中类似。加上 Min-Max scaler，我们的极坐标编码将是双射的，使用 arccos函数双射（参见下一步)。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"第二步：将缩放后的时间序列转换到「极坐标」","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"需要考虑两个量，时间序列的值及其对应的时间戳。这两个变量分别用角度和半径表示。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(Image: )","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"假设我们的时间序列由 N 个时间戳 t 和对应的 x 组成，那么:","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"角度是用 arccos(x) 计算的，值在 0pi 之间。\n首先计算半径变量，我们把区间 [0,1] 分成 N 等份。因此，我们得到 N+1 个分隔点 0cdots1 。然后我们丢弃 0，并连续地将这些点与时间序列关联起来。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"数学定义为：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"begincases\nphi_i=arccos(x_i) \nr_i=fraciN\nendcases","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"这些编码有几个优点：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"整个编码是双射的（作为双射函数的组合）。\n它通过 r 坐标保持时间依赖性。这个优点很有用。","category":"page"},{"location":"AI/FE/#.-时间序列的内积","page":"特征工程","title":"4. 时间序列的内积","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"在二维空间中，接下来的问题是我们如何使用内积运算来处理稀疏性。","category":"page"},{"location":"AI/FE/#.1.-为什么不是极坐标编码值的内积呢？","page":"特征工程","title":"4.1. 为什么不是极坐标编码值的内积呢？","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"二维极坐标空间的内积有几个限制，因为每个向量的范数都根据时间依赖性进行了调整。更准确地说应该是：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"两个截然不同的观察结果之间的内积将偏向于最近的一个（因为范数随时间增加）；\n当计算观测值与自身的内积时，得到的范数也是有偏差的。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"因此，如果存在一个像这样的内积运算，它应该只依赖于角度。","category":"page"},{"location":"AI/FE/#.2.-使用角度","page":"特征工程","title":"4.2. 使用角度","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"由于任何类似于内积的操作都不可避免地将两个不同观测值的信息转换成一个值，所以我们不能同时保留两个角度给出的信息。我们必须做出一些让步。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"为了最好地从两个角度解释个体和连接信息，作者定义了内积的另一种操作：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"xoplus y=cos(theta_1+theta_2)","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"其中 theta 表示 x 和 y 的角度。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"注意：我选择了不同的符号而不是使用内积，因为这个操作不满足内积的要求（线性，正定）。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"这就产生了如下的类 Gram 矩阵:","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"G=beginpmatrix \n cos(phi_1+phi_1)  cos(phi_1+phi_2)  cdots  cos(phi_1+phi_n)  \n cos(phi_2+phi_1)  cos(phi_2+phi_2)  cdots  cos(phi_2+phi_n)  \nvdots  vdots  ddots  vdots \n cos(phi_n+phi_1)  cos(phi_n+phi_2)  cdots  cos(phi_n+phi_n)  \nendpmatrix","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"作者的选择这样做的动机是：相对于笛卡尔坐标，极坐标保留绝对的时间关系。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"优势","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"对角线由缩放后的时间序列的原始值构成（我们将根据深度神经网络学习到的高层特征来近似重构时间序列）；\n时间相关性是通过时间间隔 k 的方向叠加，用相对相关性来解释的。","category":"page"},{"location":"AI/FE/#.3.-稀疏表示","page":"特征工程","title":"4.3. 稀疏表示","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"现在我们来画出格拉姆角场（Gramian Angular Field）的值的密度分布：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(Image: )","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"从上图中我们可以看出，格拉姆角场要稀疏得多。为了解释这一点,让我们用 uoplus v 在笛卡尔坐标重新表示：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"beginmatrix \ncos(theta_1+theta_2)  =  cos(arccos(x)+arccos(y)) \n =  cos(acrccos(x))cdot cos(arccos(y))-sin(arccos(x))cdot sin(arccos(y)) \n =  xcdot y+sqrt1-x^2cdot sqrt1-y^2 \n =  langle xy rangle-sqrt1-x^2cdot sqrt1-y^2\nendmatrix","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"我们在上一项中注意到，新构造的运算对应于传统内积的惩罚版本：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"xoplus y=xcdot y-rangle-sqrt1-x^2cdot sqrt1-y^2","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"为了了解一下这种惩罚的作用。让我们先来看看整个操作的 3D 图：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(Image: )","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"可以看到：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"惩罚将平均输出移向 -1；\n如果 x 和 y 越接近0，惩罚越大。主要的原因是，这些点点更接近高斯噪声；\n对于 x=y：会转换为 -1；\n输出很容易与高斯噪声区分开。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"缺点","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"主对角线,然而,生成的 GAM 很大是由于 nmapsto n^2，而原始时间序列的长度为 n。作者建议通过使用分段聚合近似（Piecewise Aggregation Approximation）减少大小。\n这个操作不是真正意义上的内积。","category":"page"},{"location":"AI/FE/#.-代码","page":"特征工程","title":"5. 代码","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"我自己利用 pyts 包写了一个可以使用的 demo，放在自己的 GitHub 上，详情可以参考另一篇博文 —— Python：使用 pyts 把一维时间序列转换成二维图片","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"总结：这篇博文的灵感主要来自王志光和 Tim 的一篇详细的论文，他们利用平铺的卷积神经网络将时间序列编码为图像进行视觉检查和分类。论文中还提到了另一种有趣的编码技术：马尔科夫转换域。","category":"page"},{"location":"AI/FE/#「类别型特征」的编码方式","page":"特征工程","title":"「类别型特征」的编码方式","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"序号编码（Ordinal Encoding）：通常用于处理类别间具有大小关系的数据，转换后依然保留相对的大小关系。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"独热编码（One-hot Encoding）：通常用于处理类别间不具有大小关系的特征。对于类别取值较多的情况下使用 One-hot Encoding 需要注意以下问题：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"使用稀疏向量来节省空间。\n配合特征选择来降低维度。高维度特征会带来几方面的问题：\n在 K 近邻算法中，高维度空间下两点之间的距离很难得到有效的衡量；\n在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合的问题；\n通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度；","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"二进制编码（Binary Encoding）：先用序号编码给每个类别赋予一个类别 ID，然后将类别 ID 对应的二进制编码作为结果。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"Helmert Contrast","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"Sum Contrast","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"Polynomial Contrast","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"Backward Difference Contrast","category":"page"},{"location":"AI/FE/#组合特征","page":"特征工程","title":"组合特征","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。在实际问题中，需要面对多种高维特征，简单地两两组合，依然容易存在参数过多、过拟合等问题。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"怎样有效地找到组合特征？可以利用决策树来寻找特征组合方式。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"例如，影视推荐问题有两个低阶特征「语言」和「类型」，其中有语言分为中文和英文，类型分为电影和电视剧，那么这两个特征的高阶组合特征有（中文，电影）、（英文，电视剧）、（英文，电影）、（中文，电视剧）四种。下表的数据，就可以变为新的数据：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"是否点击 语言 类型\n0 中文 电影\n1 英文 电影\n1 中文 电视剧\n0 英文 电视剧","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"是否点击 语言 = 中文，类型 = 电影 语言 = 英文，类型 = 电影 语言 = 中文，类型 = 电视剧 语言 = 英文，类型 = 电视剧\n0 1 0 0 0\n1 0 1 0 0\n1 0 0 0 1\n0 0 0 0 1","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"以逻辑回归为例，假设数据的特征向量为 X=(x_1x_2dotsx_k)，则有：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"Y=textsigmoid(sum_isum_jw_ijlangle x_ix_jrangle)","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"其中 langle x_ix_jrangle 表示 x_i 和 x_j 的组合特征，w_ij 的维度等于第 i 和第 j 个特征不同取值的个数。在上例中，「语言」这个特征有中文和英文两个选择，「类型」这个特征有电影和电视剧两个选择，那么 w_ij 的维度就为 2times 2=4. 当组合之前的两个特征不同取值的个数都不大时，用这种方式不会有太大的问题。但是对于某些问题，有用户 ID 和物品 ID，而用户和物品的数量动辄几千万，几千万乘几千万 mtimes n，这么大的参数量，无法进行学习。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"对于这种「高维组合特征」要如何处理？假设用户和物品的数量分别为 m 和 n，一种行之有效的方法是将两个特征分别用 k 维的低维向量表示（kll mkll n），这样原本 mtimes n 的学习参数就降低为 mtimes k + ntimes k，这其实等价于推荐算法中的矩阵分解。","category":"page"},{"location":"AI/FE/#文本表示模型","page":"特征工程","title":"文本表示模型","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"最基础的文本表示模型是词袋模型，就是将每篇文章看成一袋子词，并忽略每个词出现的顺序。每篇文章可以表示成一个长向量，向量中的每一维度代表一个单词，而该维对应的权重则反映了这个词在原文章中的重要程度。常用 TF-IDF（Term Frequency-Inverse Document Frequency）来计算权重：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"textTF-IDF(td)=textTF(td)times textIDF(t)","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"其中 textTF(td) 为单词 t 在文档 d 中出现的频率，textIDF(t) 是逆文档频率，用来衡量单词 t 对表达语义所起的重要性，表示为：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"textIDF(t) = logfractextNum of articlestextNum of articles containing word t+1","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"直观解释为，如果一个单词在非常多的文章里面都出现，那么它可能是一个比较通用的词汇，对于区分谋篇文章特殊语义的贡献比较小，因此对权重做一定惩罚。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"有的时候，多个不同的单词组合起来会有特殊的含义，比如 natural language processing 组合起来就有「自然语言处理」的意思，但是把这三个单词拆开，就没有组合起来的特别。将类似这样的连续出现的 n 个词（nle N）组成的词组（N-gram）也作为一个单独的特征放到向量表示中去，构成 N-gram 模型。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"词干抽取（Word Stemming），同一个词可能有多种词性变化，却有相似的含义。在实际应用中，一般会对单词进行词干抽取，即将不同词性的单词统一成为同一词干的形式。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"主题模型用于从文本库中发现有代表性的主题（得到每个主题上面词的分布特性）。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"词嵌入是一类将词向量化的模型的统称，将每个词都映射成低维空间上的一个稠密向量（Dense Vector），通常维度 K=50sim 300。词嵌入将每个词映射成一个 K 维向量，如果一篇文章有 N 个词，就可以用一个 Ntimes K 的矩阵来表示这篇文章。但是这样的表示仅仅只是底层的表示，在实际应用中，如果仅仅把这个矩阵作为原文本的表示特征输入到机器学习模型当中，很难得到令人满意的结果。","category":"page"},{"location":"company_info/#旷视科技（MEGVII）","page":"-","title":"旷视科技（MEGVII）","text":"","category":"section"},{"location":"company_info/","page":"-","title":"-","text":"移动端高效卷积神经网络 ShuffleNet","category":"page"},{"location":"company_info/","page":"-","title":"-","text":"AI 生产力平台 Brain++：包括开源深度学习框架天元 MegEngine、深度学习云计算平台 MegCompute、数据管理平台 MegData","category":"page"},{"location":"company_info/","page":"-","title":"-","text":"旷视人才：孙剑，著有论文 Residual Network、Fast R-CNN、Faster R-CNN 等","category":"page"},{"location":"docker/#Docker-命令","page":"Docker","title":"Docker 命令","text":"","category":"section"},{"location":"docker/","page":"Docker","title":"Docker","text":"推荐阅读：Docker 命令大全；","category":"page"},{"location":"docker/","page":"Docker","title":"Docker","text":"docker run：命令是创建一个新的容器并且运行一个命令，完整的语法为","category":"page"},{"location":"docker/","page":"Docker","title":"Docker","text":"docker run [OPTIONS] IMAGE [COMMAND] [ARG...]","category":"page"},{"location":"docker/","page":"Docker","title":"Docker","text":"其中 OPTION 可以选参数：","category":"page"},{"location":"docker/","page":"Docker","title":"Docker","text":"-i 以交互模式运行容器，通常与 -t 同时使用；","category":"page"},{"location":"docker/","page":"Docker","title":"Docker","text":"-t 为容器分配一个伪输入终端，通常与 -t 同时使用；","category":"page"},{"location":"docker/","page":"Docker","title":"Docker","text":"通常都是 -it 这样的","category":"page"},{"location":"docker/","page":"Docker","title":"Docker","text":"--volume, -v 是绑定一个卷；或者说是目录映射；-v 本地目录:容器目录","category":"page"},{"location":"docker/","page":"Docker","title":"Docker","text":"-e 是环境变量；","category":"page"},{"location":"docker/","page":"Docker","title":"Docker","text":"例如 -v /data:/data 就是主机的目录 /data 映射到容器的 /data；","category":"page"},{"location":"docker/","page":"Docker","title":"Docker","text":"--env-file=[] 从指定文件读入环境变量；","category":"page"},{"location":"docker/","page":"Docker","title":"Docker","text":"--ipc=host 是容器间都共享宿主机的内存；","category":"page"},{"location":"docker/","page":"Docker","title":"Docker","text":"登录：docker login 仓库地址 -u 用户名 -p 密码（如果不指定地址则为登录官方仓库）","category":"page"},{"location":"docker/","page":"Docker","title":"Docker","text":"登出：docker logout","category":"page"},{"location":"docker/","page":"Docker","title":"Docker","text":"拉取镜像：docker pull 镜像仓库地址","category":"page"},{"location":"docker/","page":"Docker","title":"Docker","text":"上传镜像：docker push 镜像仓库地址","category":"page"},{"location":"docker/","page":"Docker","title":"Docker","text":"列出容器：docker ps","category":"page"},{"location":"git/#第一章-Git-基础","page":"Git","title":"第一章 Git 基础","text":"","category":"section"},{"location":"git/#.-常用命令","page":"Git","title":"1. 常用命令","text":"","category":"section"},{"location":"git/","page":"Git","title":"Git","text":"1.1. 配置 user 信息","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"配置自己的用户名为 strongnine，邮箱为 strongnine@163.com，实际用的时候请将此换成自己的用户名和邮箱。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"$ git config --global user.name 'strongnine'\n$ git config --global user.email 'strongnine@163.com'","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"1.2. config 的三个作用域","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"$ git config --global\n$ git config --local","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"可以显示自己目前的局部（local）或者全局（global）的配置。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"# 显示 config 设置\n$ git config --list --local\n$ git config --list --global","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"1.3. git 命令","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"创建仓库可以在 GitHub 上创建仓库，然后再通过 git clone 克隆到自己的本地，也可以现在本地新建的文件夹里用 git init 初始化创建仓库。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"# git 仓库的初始化\n$ git init","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"查看当前仓库状态：git status 可以查看当前仓库的状态。能够看到文件的修改、删除、添加、以及重命名（重命名的逻辑就是删除一个文件并且添加一个文件），并且还能够看到当前存在的冲突啥的。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"添加文件：git add 可以将某个文件的更新添加到暂存区区里；","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"git add -u：将文件的修改、文件的删除，添加到暂存区。git add .：将文件的修改，文件的新建，添加到暂存区。git add -A：将文件的修改，文件的删除，文件的新建，添加到暂存区。git add -A 相对于 git add -u 命令的优点 ： git add -A 可以提交所有被删除、被替换、被修改和新增的文件到数据暂存区，而 git add -u 只能操作跟踪过的文件。git add -A 等同于 git add -all. ","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"撤销添加（版本回退）：有的时候我们 add 了一个文件，想要撤销，可以用 git reset","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"撤销添加：git reset HEAD 将绿字变成红字；","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"提交修改：git commit 将当前暂存区里的更新提交，会用默认编辑器跳出信息，可以在第一行添加提交的备注信息，例如 \"add README.md\". ","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"git commit -m\"add README.md\" 可以直接将备注信息一起提交。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"删除文件：git mv <文件名> 是正确删除文件的方法。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"重命名的文件：git mv oldname newname","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"当你重命名了一个文件之后，用 git status 会提示有一个文件被删除，有一个文件是新的 Untracked 文件。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"重置文件：git reset --hard 用来对暂存区的文件进行重置。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"warning: Warning\n注意：git reset 是一条相对危险的命令。","category":"page"},{"location":"git/#.-版本管理","page":"Git","title":"2. 版本管理","text":"","category":"section"},{"location":"git/","page":"Git","title":"Git","text":"2.1. 分支管理","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"查看历史：git log 可以查看当前分支的提交历史记录日志，命令 gitk 可以调出图形界面查看历史版本。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"git log --graph 可以有比较图形化的界面；git log --oneline 只显示每次提交的备至；git log -n4 --online 指定查看最近 4 个 commit；git log --all 查看全部分支的日志；git log --all --graph 用图形化的方式显示所有分支的日志；","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"查看分支：git branch -v 可以查看本地有多少分支。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"git branch -av：查看所有分支；git branch -d 分支名：删除分支；git branch -D 分支名：强制删除分支；","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"warning: Warning\n如果分支还未被 merged 的时候要用强制删除，请确保该分支无用。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"创建分支：git checkout ","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"git checkout -b 可以创建新分支并且切换到该新的分支；","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"有的时候需要加上 --decorate 参数才可以显示（master）（temp）等分支信息。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"2.2. .git 目录的内容","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"cat 命令主要用来查看文件内容，创建文件，文件合并，追加文件内容等功能。 cat HEAD 查看 HEAD 文件的内容 git cat-file 命令 显示版本库对象的内容、类型及大小信息。 git cat-file -t b44dd71d62a5a8ed3 显示版本库对象的类型 git cat-file -s b44dd71d62a5a8ed3 显示版本库对象的大小 git cat-file -p b44dd71d62a5a8ed3 显示版本库对象的内容","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"HEAD：指向当前的工作路径 config：存放本地仓库（local）相关的配置信息。 refs/heads：存放分支 refs/tags：存放tag，又叫里程牌 （当这次 commit 是具有里程碑意义的，比如项目 1.0 的时候 就可以打 tag） objects：存放对象 .git/objects/ 文件夹中的子文件夹都是以哈希值的前两位字符命名 每个 object 由 40 位字符组成，前两位字符用来当文件夹，后 38 位做文件。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"commit、tree、blob 的关系","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"(Image: relations)","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"一个 commit 对应一颗 tree，tree 相当于文件夹，blob 相当于具体的文件（数据）。git 里面，文件内容相同， 就是视为同一个文件。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"当创建了新的空文件夹时，使用 status 不会检测到这个空的文件夹。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"2.3. 分离头指针","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"变更没有基于某个 branch，在分离头指针进行的 commit，如果没有及时合并到某个 branch，可能会被 git 当作垃圾清掉。如果这种变更是重要的，就要将其与某个 branch 绑在一起。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"git checkout -b 可以创建新分支并且切换到该新的分支。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"HEAD 指针可以指向某个分支的最后一次提交，也可以不和某个分支挂钩，当处于分离头指针时，可以直接指向某个 commit。它只能够定位到某个 commit。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"对比提交之间的差异：git diff [commit1] [commit2] 可以比较两个具体的 commit 的差异。git diff HEAD HEAD^1 将当前结点与其父亲结点进行对比。HEAD^1, HEAD~1, HEAD~, HEAD^ 都一样。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"一个节点，可以包含多个子节点（checkout 出多个分支）\n一个节点可以有多个父节点（多个分支合并）\n^ 和 ~ 都是父节点，区别是跟随数字时候，^2 是第二个父节点，而 ~2 是父节点的父节点\n^ 和 ~ 可以组合使用,例如 HEAD~2^2","category":"page"},{"location":"git/#第二章-独自使用-Git","page":"Git","title":"第二章 独自使用 Git","text":"","category":"section"},{"location":"git/#.-commit-的操作","page":"Git","title":"1. commit 的操作","text":"","category":"section"},{"location":"git/","page":"Git","title":"Git","text":"1.1. 修改 commit 的 message","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"# 修改最新 commit 的信息\n$ git commit --amend\n# 想要修改旧 commit 的信息，需要先选择其父节点\n# 运行后会弹出一个交互界面，在里面修改、保存之后\n# 还会继续弹出一个交互界面，提示要把 message 如何修改\n$ git rebase -i 父节点","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"warning: Warning\n这种操作只适用于还未合并到「主线」 的分支上，否则会影响到合作者的工作。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"1.2. 整理多个 commit ","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"# 和上一个操作相似\n# 在弹出的交互界面进行不同的修改就行（会有提示）\n$ git rebase -i 父节点\n\n# 上面的是把「连续的」commit 合并，还有一种是把「间隔的」合并\n$ git rebase -i 父节点","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"1.3. 对比差异","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"# 对比暂存区和 HEAD 里面内容的差异（看看做了哪些改动）\n$ git diff --cached\n\n# 对比工作区和暂存区的不同\n$ git diff\n\n# 只比较某个文件\n$ git diff -- <文件名>\n\n# 查看不同提交的指定文件的差异\n$ git diff <指针 1> <指针 2> -- <文件名>","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"1.4. 恢复变更","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"# 把暂存区里面的文件全部恢复成和 HEAD 一样的\n$ git reset HEAD\n\n# 让工作区的文件恢复为暂存区一样（变更工作区）\n$ git checkout -- index.html\n\n# 取消暂存区部分文件的更改\n$ git reset HEAD -- <文件名>...","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"1.5. 消除最近几次提交","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"# 将头指针指向特定的某次提交，并且删除这之前的提交\n# <危险指令> 慎用！！！\n$ git reset --hard <指针>","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"1.6. 删除文件","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"# 正确删除文件的方法\n$ git rm <文件名>","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"1.7. 临时加塞的紧急任务 —— stash 的使用","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"# 把当前状态存放\n$ git stash\n# 列出状态区\n$ git stash list\n# 恢复暂存区（弹出之前放进 stash 顶的），但是 stash 堆栈里的信息还会在\n$ git stash apply\n# 恢复的基础上还会丢掉 stash 里面的信息\n$ git stash pop","category":"page"},{"location":"git/#.-Git-管理","page":"Git","title":"2. Git 管理","text":"","category":"section"},{"location":"git/","page":"Git","title":"Git","text":"2.1. 指定不需要 Git 管理的文件","category":"page"},{"location":"git/","page":"Git","title":"Git","text":".gitignore 文件上的内容就是表示指定类型的文件不给 Git 进行管理。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"2.2. Git 的备份","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"(Image: git_learning_fig2)","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"哑协议传输进度不可看见，智能协议可见。智能协议比哑协议快。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"# --bare 代表不带工作区的裸仓库\n# 哑协议\n$ git clone --bare /path/to/repo.git <拷贝路径.git>\n# 智能协议\n$ git clone --bare / file:///path/to/repo.git <拷贝路径.git>\n\n# 把本地的变更同步到远端\n$ git remote -v\n$ git remote add <名称> <协议地址>\n# 查看分支\n$ git branch -av\n$ git push <名称>\n$ git push --set-upstream <  > <  >","category":"page"},{"location":"git/#第三章-Github-同步","page":"Git","title":"第三章 Github 同步","text":"","category":"section"},{"location":"git/#配置公私钥","page":"Git","title":"配置公私钥","text":"","category":"section"},{"location":"git/","page":"Git","title":"Git","text":"在 Github 首页上，寻找 help，在上面有关于如何 connecting to github with SSH 的做法。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"# 打开 git bash 在里面输入下面命令\n# 若干存在 id_rsa 和 id_rsa.pub 文件则代表已经有公私钥\n# 否则应该要根据 Help 上的提示进行生成\n$ ls - al ~/.ssh\n# 查看 id_rsa.pub 的内容\n$ cd ~/.ssh\n$ cat id_rsa.pub\n# 复制里面的内容，前往 github 账户设置里面添加 SSH keys","category":"page"},{"location":"git/#**把本地仓库同步到-Github**","page":"Git","title":"把本地仓库同步到 Github","text":"","category":"section"},{"location":"git/","page":"Git","title":"Git","text":"# 添加一个新的 remote\n$ git remote add <名称> <SSH>\n# 查看已有的 remote\n$ git remote -v\n\n# 把所有内容 push\n$ git push <name> --all\n# 如果远端有某些文件是本地未包含的，这个分支会被拒绝 push\n# 需要把远端的先「拉」下来\n$ git fetch <name> master\n# 切换到 master 分支\n$ git checkout master\n# 与远端的 .../master 的分支合并\n$ git merge <远端分支>\n# 但注意如果两个分支都是独立的，没有共同的历史，那么会拒绝合并\n# 查看 merge 帮助\n$ git merge -h\n$ git merge --allow-unrelated-histories <远端分支>\n# 现在进行 push 就不会报错了\n$ git push <name> master","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"注：在之后为了方便学习，将一些命令与视频里面的进行同步，<name> 会用 github 来代替，因为我们把远端的仓库 fetch 下来并且命名为 gitHub 了","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"个人笔记总结git remote -v 查看远程版本库信息\ngit remote add <name> <url>添加 githup 远程版本库\ngit fetch <name> 拉取远程版本库\ngit merge -h 查看合并帮助信息\ngit merge --allow-unrelated-histories githup/master 合并 <name> 上的 master 分支（两分支不是父子关系，所以合并需要添加 –allow-unrelated-histories）\ngit push <name> 推送同步到 <name> 仓库—— by DriveMan_邱佳源","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"fast forward 到底是什么？举个例子，本地分支往远端分支做 push，如果远端分支不是本地分支的祖先，那它俩就不是 fast forward 了。反之，它俩就是 fast forward 的关系。","category":"page"},{"location":"git/#第四章-Git-多人单分支集成协作","page":"Git","title":"第四章 Git 多人单分支集成协作","text":"","category":"section"},{"location":"git/#.-多个人对文件修改","page":"Git","title":"1. 多个人对文件修改","text":"","category":"section"},{"location":"git/","page":"Git","title":"Git","text":"1.1. 不同人修改了不同文件","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"# \n# 会出现 Non fast_forwards 的报错，远端仓库会拒绝这个 push\n# 先把远端的 fetch 下来\n$ git fetch <name>(github)\n# 然后查看 branch 会发现有 [ahead 1, behind 1] 这样的信息，\n# 代表远端有的这里没有和这里有的远端没有\n$ git branch -av\nfeature/add_git_commands     07c85df [ahead 1, behind 1] ......\n\n# 有时候会考虑合并\n$ git merge (github/feature/add_git_commands)","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"老师你好，我有个问题哈，clone 命令 git clone git@github.com:git2019/gitlearning.git 既然已经把远程仓库所有内容都克隆到本地了，为什么还需要 git checkout -b feature/addgitcommands origin/feature/addgit_command 命令基于远程分支在本地建立分支，不是从远程clone下来了嘛，为什么还要新建，难道 clone 命令不能克隆分支吗？作者回复：我们在本地无法直接在 clone 下来的远程分支上做变更的，只能基于远程分支建本地分支后，才能创建 commit。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"1.2. 不同人修改同一文件的不同区域","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"# pull 会把远端的内容拉下来，并且本地的也会进行更新\n# 简介的方法就是直接 pull，还有一种是 fetch + merge\n# 多查看 branch ，看看 ahead 和 behind 的数目\n$ git branch -av\n\n# 当只有 ahead 没有 behind 的时候，肯定是 fast-forward 可以提交","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"fast-forword 看了英语翻译为快进，结合 git branch -av 中的 ahead 和 behind，ahead 是本地仓库比远端仓库多 commit，behind 是本地仓库比远端仓库少 commit。对正常的备份系统来说，我本地只能比备份多，备份不可能比我本地多才是。然而，git 由于多用户提交原因出现备份比本地多了，本地滞后了，所以需要 pull 一下，让本地比备份相等或多，这种情况就是 fast forward ，也就是我本地要比备份快进。不知理解对否？作者回复：其实就是两个分支的关系为 0|n 或者 n|0 ，如果两个分支直接为 n|m 的关系就不是 fast forward 。A 分支比 B 分支多 5 个 commit，B 比 A 分支多 3 个 commit。A 和 B 就不是 fast forward。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"1.3. 不同人修改同一文件的同一区域","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"# 如果 push 不上去，使用 merge 又提示已经更新到最新了\n# 就说明远端变了，要及时更新\n$ git pull\nAuto-merging (index.html)\nCONFLICT(content): Merge conflict in (index.html)\n# 提示 CONFLICT(content) 说明文件有冲突，不能自动合并 index.html\n# 打开这个文件，会提示哪里出现冲突\n$ vi index.html\n# 编辑完成后查看状态\n$ git status\n\n# 如果这个分支有问题了，可以用 --abort 退出合并\n$ git merge --abort\n$ git commit -am'(commit text)'","category":"page"},{"location":"git/#.-更改了文件名","page":"Git","title":"2. 更改了文件名","text":"","category":"section"},{"location":"git/","page":"Git","title":"Git","text":"2.1. 同时变更了文件名和内容","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"# 其中有一个人变更了文件名\n# 另一个人只变更了文件内容\n# pull 的话会智能识别问题\n$ git pull","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"2.2. 同一文件改成不同的文件名","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"# 依旧是报冲突\n$ git pull\n# 查看工作目录，会出现未命名的文件，和两个重命名的文件\n# 如果使用 diff 查看两个文件的差异，不会显示差异\n$ diff <file-1> <file-2>\n# 使用 status，会提示：\nboth deleted:  <oldfilename>\nadded by us:   <filename-1>\nadded by them: <filename-2>\n# 可以先移除不要的文件，再加上想要保存的文件名\n$ git rm <filename-2>\n$ git add <filename-1>\n$ git commit -am'(commit text)'","category":"page"},{"location":"git/#第五章-集成使用禁忌","page":"Git","title":"第五章 集成使用禁忌","text":"","category":"section"},{"location":"git/#.-禁止向集成分支执行-push-f","page":"Git","title":"1. 禁止向集成分支执行 push -f","text":"","category":"section"},{"location":"git/","page":"Git","title":"Git","text":"-f, --force 是强制更新，即使不是 fast-forward 也可以 push。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"# 把历史 reset 到某个 log\n$ git reset --hard (b3f390c)\n# 强制 push，就会把在 b3f390c 后面做的改变都删除\n$ git push -f (origin) (feature/add_git_commands)","category":"page"},{"location":"git/#.-禁止向集成分支执行变更历史的操作","page":"Git","title":"2. 禁止向集成分支执行变更历史的操作","text":"","category":"section"},{"location":"git/#第六章-GitHub","page":"Git","title":"第六章 GitHub","text":"","category":"section"},{"location":"git/#.-核心功能","page":"Git","title":"1. 核心功能","text":"","category":"section"},{"location":"git/","page":"Git","title":"Git","text":"代码预览、项目管理、集成、团队管理、社交编码（开源）、文档、存放代码。","category":"page"},{"location":"git/#.-寻找开源项目","page":"Git","title":"2. 寻找开源项目","text":"","category":"section"},{"location":"git/","page":"Git","title":"Git","text":"使用高级搜索：光标放在搜索框里，按回车就会出现 advanced search 了。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"可以在 Help 上查看有哪些高级搜索的语法。  ","category":"page"},{"location":"git/#.-搭建个人博客","page":"Git","title":"3. 搭建个人博客","text":"","category":"section"},{"location":"git/","page":"Git","title":"Git","text":"通过高级搜索在搜索框中输入 blog easily start in:readme stars:>5000 找到 jekyll-now 仓库。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"第一步就是 fork 一个到自己的账号里去。fork 完后修改工程名称：<username>.github.io","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"第二步修改 _config.yml。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"在 _posts 里面新增文件格式为：2018-12-24-<title>.md","category":"page"},{"location":"git/#第七章-团队协作","page":"Git","title":"第七章 团队协作","text":"","category":"section"},{"location":"git/#.-创建团队项目","page":"Git","title":"1. 创建团队项目","text":"","category":"section"},{"location":"git/#.-挑选合适的分支集成策略","page":"Git","title":"3. 挑选合适的分支集成策略","text":"","category":"section"},{"location":"git/","page":"Git","title":"Git","text":"在仓库的 Insights => Network 里可以看到特性分支演变历史。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"在 Options 的 Merge button 可以设置允许哪种合并。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"merge 把分支最后合并到 master 上去；\nsquash merging 把分支的所有 commits 变成一个，再放到主线上去。（在当前主线后面加上）\nrebase merging 把分支的所有 commits 添加到主线后面去。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"后面两种适合于线性开发的情况。","category":"page"},{"location":"git/#.-issue-跟踪","page":"Git","title":"4. issue 跟踪","text":"","category":"section"},{"location":"git/","page":"Git","title":"Git","text":"issue 上有标签管理，对不同的问题进行分类。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"还可以对 issue 进行模型管理，自定义一些问题报告的模板。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"有 Bug report、Feature request 等。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"使用 Projects 的看板来管理 issue","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"点击 Pojects 进行看板（Board）的设置。","category":"page"},{"location":"git/#.-Code-review","page":"Git","title":"5. Code review","text":"","category":"section"},{"location":"git/","page":"Git","title":"Git","text":"在 Settings 的 Branches 上可以设置特殊分支的保护规则。比如对于 master 分支进行 push 保护，每次 push 都要有特定人数去检查才能通过。","category":"page"},{"location":"git/#.-多分支的集成","page":"Git","title":"6. 多分支的集成","text":"","category":"section"},{"location":"git/","page":"Git","title":"Git","text":"特性分支往主干合，要发 Pull requests。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"git rerere 是一个隐藏功能，允许你让 Git 记住解决一个块冲突的方法，在下一次看到相同冲突时，自动解决。","category":"page"},{"location":"git/#第八章-GitLab","page":"Git","title":"第八章 GitLab","text":"","category":"section"},{"location":"git/","page":"Git","title":"Git","text":"这两章先看视频过一遍，等到应用的时候可以复习。目前不知道具体的使用需求，先大概看个印象。","category":"page"},{"location":"git/#其它问题","page":"Git","title":"其它问题","text":"","category":"section"},{"location":"git/#.-在-Windows-上如何支持中文","page":"Git","title":"1. 在 Windows 上如何支持中文","text":"","category":"section"},{"location":"git/","page":"Git","title":"Git","text":"参考解决 Git 在 windows 下中文乱码的问题.md。","category":"page"},{"location":"git/","page":"Git","title":"Git","text":"有一个注意的点：目前无法解决输入中文字符会显示乱码的问题解决方案：git commit 时，不用 -m 参数，直接回车让 vim 来处理\n进 vim 后按 i 进入编辑模式，完成后再保存退出","category":"page"},{"location":"vim/#vim-快捷键总结","page":"Vim","title":"vim 快捷键总结","text":"","category":"section"},{"location":"vim/","page":"Vim","title":"Vim","text":"vim 最常用的两种模型：「普通模式」和「输入模式」","category":"page"},{"location":"vim/#普通模式下的快捷键","page":"Vim","title":"普通模式下的快捷键","text":"","category":"section"},{"location":"vim/","page":"Vim","title":"Vim","text":":w 保存文本","category":"page"},{"location":"vim/","page":"Vim","title":"Vim","text":":q 退出文本（如果文本做了改动会退不出去）","category":"page"},{"location":"vim/","page":"Vim","title":"Vim","text":":q! 强制退出不保存文本","category":"page"},{"location":"vim/","page":"Vim","title":"Vim","text":":wq 保存文件并退出","category":"page"},{"location":"vim/","page":"Vim","title":"Vim","text":"i 在光标位置开始输入（左下角会有 --- INSERT --- 的提示），在插入模型下","category":"page"},{"location":"vim/","page":"Vim","title":"Vim","text":"在10行和112行之间插入4空格：10,112 s/^/    /","category":"page"},{"location":"vim/#vim-配置","page":"Vim","title":"vim 配置","text":"","category":"section"},{"location":"AI/RS/#大规模分段线性模型（LS-PLM）","page":"-","title":"大规模分段线性模型（LS-PLM）","text":"","category":"section"},{"location":"AI/RS/","page":"-","title":"-","text":"早在 2012 年，大规模分段线性模型（Large Scale Piece-wise Linear Model）就是阿里巴巴的主流推荐模型，又被称为混合逻辑回归（Mixed Logistics Regression），可以看作在逻辑回归的基础上采用分而治之的思路，先对样本进行分片，再在样本分片中应用逻辑回归进行 CTR（Click Through Rate，点击率）预估。","category":"page"},{"location":"AI/RS/#Embedding-技术","page":"-","title":"Embedding 技术","text":"","category":"section"},{"location":"AI/RS/","page":"-","title":"-","text":"Embedding，中文译为「嵌入」，常被翻译为「向量化」或者「向量映射」。形式上讲，Embedding 就是用一个低维稠密的向量「表示」一个对象，可以是词、商品、电影。","category":"page"},{"location":"AI/RS/#搜索相关性","page":"-","title":"搜索相关性","text":"","category":"section"},{"location":"AI/RS/","page":"-","title":"-","text":"搜索相关性旨在计算 Query 和返回 Doc 之间的相关程度，也就是判断 Doc 中的内容是否满足用户 Query 的需求，对应 NLP 中的语义匹配任务（Semantic Maching）。","category":"page"},{"location":"AI/RS/","page":"-","title":"-","text":"早期文本匹配：仅仅考虑 Query 与 Doc 的字面匹配程度，通过 TF-IDF、BM25 等基于 Term 的匹配特性来计算相关性。","category":"page"},{"location":"AI/RS/","page":"-","title":"-","text":"优点：线上计算效率高\n缺点：（1）基于 Term 的关键词匹配泛化性能较差，缺少语义和词序信息；（2）无法处理一词多义或多词一义的问题，漏匹配和误匹配现象严重。","category":"page"},{"location":"AI/RS/","page":"-","title":"-","text":"传统语义匹配模型：主要包括（1）隐式空间的匹配：将 Query 和 Doc 都映射到同一个空间的向量，再用向量距离或相似度作为匹配分，如 Partial Least Square (PLS)；（2）基于翻译模型的匹配：将 Doc 映射到 Query 空间后进行匹配或计算 Doc 翻译成 Query 的概率。","category":"page"},{"location":"AI/RS/","page":"-","title":"-","text":"深度语义匹配模型：实现方法上分为基于表示（Representation-based）和基于交互（Interaction-based）的方法。","category":"page"},{"location":"AI/RS/","page":"-","title":"-","text":"(Image: )","category":"page"},{"location":"AI/RS/","page":"-","title":"-","text":"基于表示的深度语义匹配模型：分别学习 Query 和 Doc 的语义向量表示，再基于两个向量计算相似度。","category":"page"},{"location":"AI/RS/","page":"-","title":"-","text":"DSSM 模型 [微软]：提出经典的双塔结构的文本匹配模型，分别使用相互独立的两个网络结构构建 Query 和 Doc 的向量表示，用余弦相似度衡量两个向量的相关程度。\nNRM [微软 Bing 搜索]：针对 Doc 表征问题，除了基础的 Doc 标题和内容，还考虑了其他多源信息（每类信息称为一个域 Field），如外链、用户点击过的 Query 等，考虑一个 Doc 中有多个 Field，每个 Field 内又有多个实例（Instance），每个 Instance 对应一个文本，如一个 Query 词。模型首先学习 Instance 向量，将所有 Instance 的表示向量聚合起来就得到一个 Field 的表示向量，将多个 Field 的表示向量聚合起来得到最终 Doc 的向量。\nSentenceBERT：将预训练模型 BERT 引入到双塔的 Query 和 Doc 的编码层，采用不同的 Pooling 方式获取双塔的句向量，通过点乘、拼接等方式对 Query 和 Doc 进行交互。","category":"page"},{"location":"AI/RS/","page":"-","title":"-","text":"基于交互的深度语义匹配模型：不直接学习 Query 和 Doc 的语义表示向量，而是在底层输入阶段就让 Query 和 Doc 进行交互，建立一些基础的匹配信号，再将基础匹配信号融合成一个匹配分。","category":"page"},{"location":"AI/RS/","page":"-","title":"-","text":"ESIM：是预训练模型引入之前被业界广泛使用的经典模型，首先对 Query 和 Doc 进行编码得到初始向量，再用 Attention 机制进行交互加权后与初始向量进行拼接，最终分类得到相关性得分。引入预训练模型 BERT 进行交互计算时，通常将 Query 和 Doc 拼接作为 BERT 句间关系任务的输入，通过 MLP 网络得到最终的相关性得分。\nCEDR：在 BERT 句间关系任务获得 Query 和 Doc 向量之后，对 Query 和 Doc 向量进行拆分，进一步计算 Query 与 Doc 的余弦相似矩阵。","category":"page"},{"location":"AI/RS/","page":"-","title":"-","text":"","category":"page"},{"location":"AI/RS/","page":"-","title":"-","text":"参考","category":"page"},{"location":"AI/RS/","page":"-","title":"-","text":"[1] 王喆，《深度学习推荐系统》2020","category":"page"},{"location":"lang/Julia/","page":"Julia","title":"Julia","text":"查看 dict 里是否有某个 key：haskey(dick, key)","category":"page"},{"location":"lang/Julia/#Distributions.jl","page":"Julia","title":"Distributions.jl","text":"","category":"section"},{"location":"lang/Julia/","page":"Julia","title":"Julia","text":"离散分布（Discrete Distributions）的类型","category":"page"},{"location":"lang/Julia/","page":"Julia","title":"Julia","text":"按照给定分布进行采样","category":"page"},{"location":"lang/Julia/#CurveFit.jl","page":"Julia","title":"CurveFit.jl","text":"","category":"section"},{"location":"lang/Julia/","page":"Julia","title":"Julia","text":"CurveFit.jl 是 Julia 中实现曲线拟合的包。","category":"page"},{"location":"lang/Julia/#线性最小二乘法","page":"Julia","title":"线性最小二乘法","text":"","category":"section"},{"location":"lang/Julia/","page":"Julia","title":"Julia","text":"线性最小二乘法（Linear Least Square）常用于寻找离散数据集的近似值。给定点集 x[i] 和 y[i] 以及一系列函数 f_i(x)，最小二乘法通过最小化与 y[i] 相关的误差平方来找到对应的系数 a[i]，例如 a[1]*f_1(x) + a[2]*f_2(x) + ... + a[n]*f_n(x).","category":"page"},{"location":"lang/Julia/","page":"Julia","title":"Julia","text":"基础功能用 QR 分解来实现：A \\ y：coefs = A \\ y，其中 A[:. i] = f_i(x). 通常 x 是单个变量，如果需要多个自变量，可以使用相同的过程，类似于：A[:, i] = f_i(x1, x2, ..., xn).","category":"page"},{"location":"lang/Julia/","page":"Julia","title":"Julia","text":"不同的拟合方式：","category":"page"},{"location":"lang/Julia/","page":"Julia","title":"Julia","text":"linear_fit(x, y) finds coefficients a and b for y[i] = a + b*x[i]\npower_fit(x, y) finds coefficients a and b for y[i] = a *x[i]^b\nlog_fit(x, y) finds coefficients a and b for y[i] = a + b*log(x[i])\nexp_fit(x, y) finds coefficients a and b for y[i] = a*exp(b*x[i])\nexpsum_fit(x, y, 2, withconst = true) finds coefficients k, p, and λ for y[i] = k + p[1]*exp(λ[1]*x[i]) + p[2]*exp(λ[2]*x[i])\npoly_fit(x, y, n) finds coefficients a[k] for y[i] = a[1] + a[2]*x[i] + a[3]*x[i]^2 + a[n+1]*x[i]^n\nlinear_king_fit(E, U), find coefficients a and b for E[i]^2 = a + b*U^0.5\nlinear_rational_fit(x, y, p, q) finds the coefficients for rational polynomials: y[i] = (a[1] + a[2]*x[i] + ... + a[p+1]*x[i]^p) / (1 + a[p+1+1]*x[i] + ... + a[p+1+q]*x[i]^q)","category":"page"},{"location":"lang/Julia/#非线性最小二乘法","page":"Julia","title":"非线性最小二乘法","text":"","category":"section"},{"location":"lang/Julia/","page":"Julia","title":"Julia","text":"有时拟合函数相对于拟合系数是非线性的。 在这种情况下，给定系数的近似值，拟合函数围绕该近似值线性化，并且线性最小二乘法用于计算对近似系数的校正。 重复此迭代直到达到收敛。 拟合函数具有以下形式：f(x_1, x_2, x_3, ..., x_n, a_1, a_2, ..., a_p) = 0，其中 xi 是已知的数据点，ai 是要拟合的系数。","category":"page"},{"location":"lang/Julia/","page":"Julia","title":"Julia","text":"当模型公式在拟合系数上不是线性时，非线性算法是必要的。 这个库实现了一个不明确需要导数的牛顿型算法。 这是在函数中实现的：coefs, converged, iter = nonlinear_fit(x, fun, a0, eps=1e-7, maxiter=200)","category":"page"},{"location":"lang/Julia/","page":"Julia","title":"Julia","text":"在这个函数中，x 是一个数组，其中每一列代表数据集的一个不同变量，fun 是可调用的（callable），它返回拟合误差，要求可以使用以下签方式调用：residual = fun(x, a)，其中 x 是一个表示一行参数数组 x 的向量，a 是拟合系数的估计值，这些系数都不能为零（以提供比例）。eps  和 maxiter 是收敛参数。","category":"page"},{"location":"lang/Julia/","page":"Julia","title":"Julia","text":"nonlinear_fit 函数用于实现一下拟合函数：","category":"page"},{"location":"lang/Julia/","page":"Julia","title":"Julia","text":"king_fit(E, U) find coefficients a, b and n for E[i]^2 = a + b*U^n\nrational_fit Just like linear_rational_fit but tries to improve the results using nonlinear least squares (nonlinear_fit)","category":"page"},{"location":"lang/Julia/#通用接口","page":"Julia","title":"通用接口","text":"","category":"section"},{"location":"lang/Julia/","page":"Julia","title":"Julia","text":"CurveFit.jl 开发了方便使用不同曲线拟合的通用接口：fit = curve_fit(::Type{T}, x, y...)，其中 T 是 curve fitting type，The following cases are implemented:","category":"page"},{"location":"lang/Julia/","page":"Julia","title":"Julia","text":"curve_fit(LinearFit, x, y)\ncurve_fit(LogFit, x, y)\ncurve_fit(PowerFit, x, y)\ncurve_fit(ExpFit, x, y)\ncurve_fit(Polynomial, x, y, n=1)\ncurve_fit(LinearKingFit, E, U)\ncurve_fit(KingFit, E, U)\ncurve_fit(RationalPoly, x, y, p, q)","category":"page"},{"location":"lang/Julia/","page":"Julia","title":"Julia","text":"curve_fit 通用函数返回一个对象，该对象可用于使用 apply_fit 计算模型的估计值。 call 被重载，以便对象可以用作函数。","category":"page"},{"location":"lang/Julia/#用例","page":"Julia","title":"用例","text":"","category":"section"},{"location":"lang/Julia/","page":"Julia","title":"Julia","text":"using PyPlot\nusing CurveFit\n\nx = 0.0:0.02:2.0\ny0 = @. 1 + x + x*x + randn()/10\nfit = curve_fit(Polynomial, x, y0, 2)\ny0b = fit.(x) \nplot(x, y0, \"o\", x, y0b, \"r-\", linewidth=3)","category":"page"},{"location":"AI/ML/#验证方法","page":"机器学习","title":"验证方法","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"Holdout 检验","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"交叉检验","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"自助法（Bootstrap）：有放回地从 N 个样本中抽样 n 个样本。当样本规模比较小的时候，将样本集进行划分会让训练集进一步减小，这可能会影响模型训练效果。自助法是基于自助采样的检验方法。在 n 次采样过程中，有的样本会被重复采样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集，进行模型验证，这就是自助法的验证过程。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"交并比（Intersection over Union，IoU）：交并比 IoU 衡量的是两个区域的重叠程度，是两个区域的交集比上并集。在目标检测任务重，如果模型输出的矩形框与人工标注的矩形框 IoU 值大于某个阈值（通常为 0.5）时，即认为模型输出正确。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"精准率与召回率（Precision & Recall）：在目标检测中，假设有一组图片，Precision 代表我们模型检测出来的目标有多少是真正的目标物体，Recall 就是所有真实的目标有多少比例被模型检测出来了。目标检测中的真正例（True Positive）、真负例（True Negative）、假正例（False Positive）、假负例（False Positive）的定义如下：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":" 实际为正 实际为负\n预测为正 TP FP\n预测为负 FN TN","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"对于这四个指标可以这样去理解，后面的 Positive 和 Negative 以预测的结果为主，因为我们关注的是模型的预测，如果模型的预测与实际的标注不一样，那么这个预测就是「假的」，比如预测为负那么就称为 Negative，但是实际为正，与预测的不一样，那么就是「假的」False，因此这个预测就是 False Negative，这是一个「假的正例」是「错误的正例」。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"精准率，就是在预测为正样本中实际为正样本的概率，也就是所有的 Positive 中 True Positive 的概率","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"Precision = fracTPTP+FP","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"召回率，就是在实际为正样本中预测为正样本的概率，就是所有的实际标注为正样本的（TP + FN）预测为正样本的概率（TP）","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"Recall = fracTPTP+FN","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"准确率，就是模型预测正确的（所有的 True：TP + TN）占全部的比例","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"Accuracy = fracTP+TNTP+TN+FP+FN","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"平均精度（Average precision，AP）：是主流的目标检测模型评价指标，它的意思是不同召回率上的平均精度。我们希望训练好的模型 Precision 和 Recall 都越高越好，但是这两者之间有个矛盾，当 Recall 很小的时候 Precision 可能会很高，当 Recall 很大的时候，Precision 可能会很低。我们将不同 Recall 对应的 Precision 做一个曲线（PR 曲线），然后在这个曲线上计算 Precision 的均值。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"曲线下面积（Area Under Curve，AUC）：","category":"page"},{"location":"AI/ML/#优化算法","page":"机器学习","title":"优化算法","text":"","category":"section"},{"location":"AI/ML/#损失函数总结","page":"机器学习","title":"损失函数总结","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"为了刻画模型输出与样本标签的匹配程度，定义损失函数 L(cdotcdot)Ytimes Yrightarrow mathbbR_ge 0，L(f(x_itheta)y_i) 越小，表明模型在该样本点匹配得越好。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"为了具有更加简介的表达，将网络的输出表示为 f，而实际标签表达为 y。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"在分类问题上常用的损失函数：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（1）0-1 损失函数：最常用于二分类问题，Y=1-1，我们希望 textttsign f(x_itheta)=y_i，所以 0-1 损失函数为","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"L_0-1(fy) = 1_fyle 0","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中 1_P 是指示函数（Indicator Function），当且仅当 P 为真时取值为 1，否则取值为 0。0-1 损失的优点是可以直观地刻画分类的错误率，缺点是由于其非凸、非光滑的特点，算法很难对该函数进行优化。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（2）Hinge 损失函数：是 0-1 损失函数相对紧的凸上界，且当 fyge 1 时，函数不对其做任何惩罚。它在 fy=1 处不可导，不能够用梯度下降法进行优化，而是用次梯度下降法（Subgradient Descent Method）。适用于 Maximum-Margin 分类，主要用于支持向量机（SVM）中，用来解间距最大化的问题。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"L_texthinge(fy)=max01-fy","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（3）感知损失函数（Perceptron Loss）：是 Hinge 损失函数的一个变种。Hinge 对判定边界附近的点（正确端）惩罚力度很高，但是 Perceptron 只要样本的判定类别正确就行，不管其判定边界的距离。它比 Hinge 更加简单，不是 Max-margin Boundary，所以模型的泛化能力没有 Hinge 强。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"L_textPerceptron=max(0 -f)","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（4）Logistic 损失函数：是 0-1 损失函数的凸上界，该函数处处光滑，对所有的样本点都有所惩罚，因此对异常值相对更敏感一点。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"L_textlogistic(fy)=log_2(1+exp(-fy))","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（5）Log 对数损失函数：即对数似然损失（Log-likelihood Loss），它的标准形式","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"L_textlog(f(boldsymbolxtheta)y)=-log f_y(boldsymbolxtheta)","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中 f_y(boldsymbolxtheta) 可以看作真实类别 y 的似然函数。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（6）交叉熵（Cross Entropy）损失函数：对于两个概率分布，一般可以用交叉熵去衡量它们的差异。标签的真实分布 boldsymboly 和模型预测分布 f(boldsymbolxtheta) 之间的交叉熵为","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"mathcalL(f(boldsymbolxtheta)boldsymboly)=-boldsymboly^toplog f(boldsymbolxtheta)=-sum_c=1^Cy_clog f_c(boldsymbolxtheta)","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"因为 boldsymboly 为 one-hot 向量，因此交叉熵可以写为","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"mathcalL(f(boldsymbolxtheta)boldsymboly)=-log f_y(boldsymbolxtheta)","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中 f(boldsymbolxtheta) 可以看作真实类别 y 的似然函数。因此交叉熵损失函数也就是负对数似然函数（Negative Log-Likelihood）。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"在回归问题中常用的损失函数：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（1）平方损失（Mean Squared Error）函数：在回归问题中最常用的损失函数。对于 Y=mathbbR，我们希望 f(x_itheta)approx y_i","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"L_textMSE(fy)=(f-y)^2","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（2）绝对损失（Mean Absolute Error）函数：当预测值距离真实值较远的时候，平方损失函数的惩罚力度大，也就是说它对于异常点比较敏感。如果说平方损失函数是在做均值回归的话，那么绝对损失函数就是在做中值回归，对于异常点更加鲁棒一点。只不过绝对损失函数在 f=y 处无法求导。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"L_textMAE(fy)=f-y","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（3）Huber 损失函数：也称为 Smooth L1 Loss， 综合考虑可导性和对异常点的鲁棒性。在 f-y 较小的时候为平方损失，比较大的时候为线性损失","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"L_textHuber(fy)=begincases(f-y)^2qquad f-yle delta 2deltaf-y-delta^2quadf-ydeltaendcases","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（4）Log-Cosh 损失函数：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（5）分位数损失函数：","category":"page"},{"location":"AI/ML/#随机梯度算法","page":"机器学习","title":"随机梯度算法","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"随机梯度下降法本质上是采用迭代方式更新参数，每次迭代在当前位置的基础上，沿着某一方向迈一小步抵达下一位置，不断地重复这个步骤，它的更新公式为","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"theta_t+1=theta_t - eta g_t","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中 eta 是学习率。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"动量（Momentum）方法：类比中学物理知识，当前梯度就好比当前时刻受力产生的加速度，前一次步长 v_t-1 好比前一时刻的速度，当前步长 v_t 好比当前加速度共同作用的结果。这就好比小球有了惯性，而刻画惯性的物理量是动量。模型参数的迭代公式为：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"v_t = gamma v_t-1 + eta g_t","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"theta_t+1 = theta_t - v_t","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"在这里当前更新步长 v_t 直接依赖于前一次步长 v_t-1 和当前梯度 g_t，衰减系数 gamma 扮演了阻力的作用。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"AdaGrad 方法：在应用中，我们希望更新频率低的参数可以拥有较大的更新步幅，而更新频率高的参数的步幅可以减小，AdaGrad 方法采用「历史梯度平方和」来衡量不同参数的梯度的稀疏性，取值越小表明越稀疏。AdaGrad 借鉴了 mathscrl_2 正则化的思想，每次迭代时自适应地调整每个参数的学习率。这样的方式保证了不同的参数有具有自适应学习率。具体的更新公式表示为：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"在第 t 次迭代时，先计算每个参数梯度平方的累计值","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"G_t = sum_tau=1^t boldsymbolg_tau odot boldsymbolg_tau","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中 odot 为按元素乘积，boldsymbolg_tauin mathbbR^theta 是第 tau 次迭代时的梯度。参数更新差值为","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"Deltatheta_t=-fracetasqrtG_t+epsilonodotboldsymbolg_t","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中 alpha 是初始学习率，epsilon 是为了保持数值稳定性而设定的非常小的常数，一般取值为 e^-7sim e^-10。分母中求和的形式实现了退火过程，意味着随着时间推移，学习速率 fracetasqrtG_t+epsilon 越来越小，保证算法的最终收敛。在 AdaGrad 算法中，如果某个参数的偏导数积累比较大，其学习率相对较小；相反如果其偏导数积累较小，其学习率相对较大，但整体是随着迭代次数的增加，学习率逐渐变小。","category":"page"},{"location":"AI/ML/#Adam-算法","page":"机器学习","title":"Adam 算法","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"Adam 算法的全称是自适应动量估计算法（Adaptive Moment Estimation Algorithm），它将惯性保持和自适应两个优点结合，可以看作是动量法和 RMSprop 算法（或者 AdaGrad 算法）的结合。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"它一方面记录梯度的一阶矩（First Moment）M_t，即过往梯度与当前梯度的平均，理解为「惯性」，是梯度 boldsymbolg_t 的指数加权平均。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"另一方面记录梯度的二阶矩（Second Moment）G_t，即过往梯度平方与当前梯度平方的平均，理解为「自适应部分」，是梯度 boldsymbolg_t^2 的指数加权平均。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"一阶矩可以理解为均值；二阶矩可以理解为未减去均值的方差","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"M_t = beta_1 M_t-1 + (1 - beta_1)boldsymbolg_t","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"G_t = beta_2 G_t-1 + (1 - beta_2)boldsymbolg_todotboldsymbolg_t","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中 beta_1 和 beta_2 分别为两个移动平均的衰减率，通常取值为 beta_1 = 09, beta_2 = 099. ","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"Adam 算法考虑了 M_t G_t 在零初始情况下的偏置矫正。假设 M_0=0 G_0=0，那么在迭代初期 M_t 和 G_t 的值会比真实的均值和方差要小，特别是当 beta_1 和 beta_2 都接近于 1 时，偏差会很大。具体来说，Adam 算法的更新公式为：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"hatM_t = fracM_t1 - beta_1^t","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"hatG_t = fracG_t1 - beta_2^t","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"Deltatheta_t = -fracalphasqrthatG_t + epsilon hatM_t","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中学习率 alpha 通常设为 0.001，并且也可以进行衰减，比如 alpha_t=alpha_0sqrtt. ","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"Adam 算法的物理意义：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"《百面机器学习》163 页","category":"page"},{"location":"AI/ML/#逐层归一化","page":"机器学习","title":"逐层归一化","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"逐层归一化（Layer Normalization）是将传统机器学习中的数据归一化方法应用到深度神经网络中，对神经网络中隐藏的输入进行归一化，使得网络更容易训练。常用的逐层归一化方法有：批量归一化、层归一化、权重归一化和局部响应归一化。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"内部协变量偏移（Internal Covariate Shift）：当使用随机梯度下降来训练网络时，每次参数更新都会导致该神经层的输入分布发生改变，越高的层，其输入分布会改变得越明显。从机器学习角度来看，如果一个神经层的输入分布发生了改变，那么其参数需要重新学习。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"逐层归一化的能够提高训练效率的原因：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（1）更好的尺度不变性：把每个神经层的输入分布都归一化为标准正态分布，可以使得每个神经层对其输入具有更好的尺度不变性。不论低层的参数如何变化，高层的输入保持相对稳定。另外，尺度不变性可以使得我们更加高效地进行参数初始化以及超参选择。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（2）更平滑的优化地形：逐层归一化一方面可以使得大部分神经层的输入处于不饱和区域，从而让梯度变大，避免梯度消失问题；另一方面还可以使得神经网络的优化地形（Optimization Landscape）更加平滑，以及使梯度变得更加稳定，从而允许我们使用更大的学习率，并提高收敛速度。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"批量归一化（Batch Normalization，BN）方法 是一种有效的逐层归一化方法，可以对神经网络中任意的中间层进行归一化操作。假设神经网络第 l 层的净输入为 boldsymbolz^(l)，神经元输出为 boldsymbola^(l)，即","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"boldsymbola^(l) = f(boldsymbolz^(l))=fleft( boldsymbolWboldsymbola^(l) + boldsymbolb right)","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中 f(cdot) 是激活函数，boldsymbolW boldsymbolb 是神经网络的参数。为了提高优化效率，就要使得净输入 boldsymbolz^(l) 的分布一致，比如都归一化到标准正态分布。归一化操作一般应用在仿射变换（Affine Transformation）boldsymbolWboldsymbola^(l)+boldsymbolb 之后，激活函数之前。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"为了提高归一化效率，一般使用标准化将净输入 boldsymbolz^(l) 的每一维都归一化到标准正态分布","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"hatboldsymbolz^(l) = fracboldsymbolz^(l)-mathbbEboldsymbolz^(l)sqrttextvar(boldsymbolz^(l))+epsilon","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中 mathbbEboldsymbolz^(l) 和 textvar(boldsymbolz^(l)) 是当前参数下 boldsymbolz^(l) 的每一维在整个训练集上的期望和方差。","category":"page"},{"location":"AI/ML/#集成学习","page":"机器学习","title":"集成学习","text":"","category":"section"},{"location":"AI/ML/#Boosting-与-Bagging","page":"机器学习","title":"Boosting 与 Bagging","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"机器学习问题的两种策略：一种是研发人员尝试各种模型，选择其中表现最好的模型，做重点调参优化；另一种是将多个分类器的结果统一成一个最终的决策，其中每个单独的分类器称为基分类器，使用这类策略的机器学习方法统称为集成学习。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"集成学习分为 Boosting 和 Bagging 两种。Boosting 方法训练基分类器时采用串行方式，各个基分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。Bagging 与 Boosting 的串行训练方式不同，Bagging 方法在训练过程中，各基分类器之间无强依赖，可以进行并行训练。最著名的算法之一就是基于决策树基分类器的随机森林（Random Forest）。Bagging 方法更像是一个集体决策的过程，每个个体都进行单独学习，在最终做决策时，每个个体单独做出判断，再通过投票的方式做出最后的集体决策。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"基分类器，有时候又被称为弱分类器。基分类器的错误，是偏差和方差两种错误之和。偏差主要是由于分类器的表达能力有限导致的系统性错误，表现在训练误差不收敛，方差是由于分类器对于样本分布过于敏感，导致在训练样本数较少时，产生过拟合。而 Boosting 方法通过逐步聚焦于基分类器分错的样本，减小集成分类器的偏差。Bagging 方法则是采取分而治之的策略，通过对训练样本多次采样，并分别训练出多个不同模型，然后做综合，来减小集成分类器的方差。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"最常用的基分类器是决策树：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"决策树可以较为方便地将样本的权重整合到训练过程当中，而不需要使用过采样的方法来调整样本权重；\n决策树的表达能力和泛化能力，可以通过调节树的层数来做折中；\n数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性较大，这样的「不稳定学习期」更适合作为基分类器。（在这个点上，神经网络也因为不稳定性而适合作为基分类器，可以通过调节神经元数量、连接方式、网络层数、初始权值等方式引入随机性）；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"集成学习的基本步骤。集成学习一般可以分为以下 3 个步骤：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（1）找到误差互相独立的基分类器；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（2）训练基分类器；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（3）合并基分类器的结果；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"合并基分类器的方法有 voting 和 stacking 两种，前者对应 Bagging 方法，后者对应 Boosting 方法。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"以 Adaboost 为例，其基分类器的训练和合并的基本步骤如下：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（1）确定基分类器：可以选择 ID3 决策树作为基分类器。虽然任何分类模型都可以作为基分类器，但树形模型由于结构简单且较为容易产生随机性所以比较常用。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（2）训练基分类器：假设训练集为 x_iy_ii=1dotsN，其中 y_iin-11，并且有 T 个基分类器，则可以按照如下过程来训练基分类器：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"初始化采样分布 D_1(i)=1N；\n令 t=12dotsT 循环：\n从训练集中，按照 D_t 分布，采样出子集 S_i=x_iy_ii=1dotsN；\n用 S_i 训练出基分类器 h_t；\n计算基分类器 h_t 的错误率：\nvarepsilon_t=fracsum_i=1^N_tIh_t(x_i)neq y_iD_i(x_i)N_t\n其中 Icdot 为判别函数；\n计算基分类器 h_t 权重 a_t=logfrac(1-varepsilon_t)varepsilon_t，这里可以看到错误率 varepsilon_t 越大，基分类器的权重 a_t 就越小；\n设置下一次采样：\nD_t+1=begincasesD_t(i) text or  fracD_t(i)(1-varepsilon_t)varepsilon_t  h_t(x_i)neq y_i\nfracD_t(i)varepsilon_t(1-varepsilon_t)  h_t(x_i)= y_iendcases","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（3）合并基分类器：给定一个未知样本 z，输出分类结果为加权投票的结果 textSign(sum_t=1^Th_t(z)a_t).","category":"page"},{"location":"AI/ML/#梯度提升决策树（GBDT）","page":"机器学习","title":"梯度提升决策树（GBDT）","text":"","category":"section"},{"location":"AI/ML/#XGBoost","page":"机器学习","title":"XGBoost","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"XGBoost 是陈天奇等人开发的一个开源机器学习项目，高效地实现了 GBDT 算法并进行了算法和工程上的许多改进，被广泛应用在 Kaggle 竞赛以及其他许多机器学习竞赛中。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"XGBoost 本质上还是一个 GBDT（Gradient Boosting Decision Tree），只是把速度和效率发挥到极致，所以前面加上了 X（代表 Extreme）。原始的 GBDT 算法基于经验损失函数的负梯度来构造新的决策树，只是在决策树构建完成后再进行剪枝。XGBoost 在决策树构建阶段就加入了正则项，即","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"L_t=sum_i lleft(y_i F_t-1(x_i)+f_t(x_i)right)+Omega(f_t)","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中 F_t-1(x_i) 表示现有的 t-1 棵树最优解，树结构的正则项定义为","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"Omega(f_t)=gamma T+frac12lambdasum_j=1^Tw^2_j","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中 T 为叶子节点个数，w_j 表示第 j 个叶子节点的预测值。对该损失函数在 F_t-1 处进行二阶泰勒展开可以推导出","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"L_tapproxoversetsimL_t=sum_j=1^TleftG_jw_j+frac12(H_j+lambda)w^2_jright+gamma T","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"从所有的树结构中寻找最优的树结构是一个 NP-hard 问题，在实际中往往采用贪心法来构建出一个次优的树结构，基本思想是根据特定的准则选取最优的分裂。不同的决策树算法采用不同的准则，如 IC3 算法采用信息增益，C4.5 算法为了克服信息增益中容易偏向取值较多的特征而采用信息增益比，CART 算法使用基尼指数和平方误差，XGBoost 也有特定的准则来选取最优分裂。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"XGBoost 与 GBDT 的区别和联系：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（1）GBDT 是机器学习算法，XGBoost 是该算法的工程实现；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（2）在使用 CART 作为基分类器时，XGBoost 显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（3）GBDT 在模型训练时只使用了代价函数的一阶导数信息，XGBoost 对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（4）传统的 GBDT 采用 CART 作为基分类器，XGBoost 支持多种类型的基分类器，比如线性分类器；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（5）传统的 GBDT 在每轮迭代时使用全部的数据，XGBoost 则采用了与随机森林相似的策略，支持对数据进行采样；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（6）传统的 GBDT 没有设计对缺失值进行处理，XGBoost 能够自动学习出缺失值的处理策略；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"XGBoost 的并行化：boosting 是一种串行结构，它的并行不是在 tree 粒度上的，而是在特征粒度上的并行。决策树学习最耗时的一个步骤就是对特征的值进行排序（为了确定最佳分割点）。XGBoost 训练之前，预先对数据进行排序，保存为 block 结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"XGBoost 的特点：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"传统的 GBDT 以 CART 作为基函数，而 XGBoost 相当于有 L1/L2 正则化项的分类或者回归\n传统的 GBDT 在优化的时候只用到一阶导数，XGBoost 对代价函数进行了二阶泰勒展开，同时用到一阶和二阶导数。并且 XGBoost 工具支持自定义代价函数，只要函数可以一阶和二阶求导；\nXGBoost 在代价函数里加入了正则项，控制模型复杂度。正则项里包含了树的叶节点个数、每个叶子节点上输出 score 的 L2 模的平方和。从 Bias-variance tradeoff 角度来讲，正则项降低了模型 variance，使学习出来的模型更加简单，防止过拟合，这也是 XGBoost 优于传统 GBDT 的一个特性。 剪枝是都有的，叶子节点输出 L2 平滑是新增的；\nshrinkage 缩减和 column subsampling。shrinkage 缩减：类似于学习速率，在每一步 tree boosting 之后增加了一个参数 n（权重），通过这种方式来减小每棵树的影响力，给后面的树提供空间去优化模型。column subsampling：列（特征）抽样，随机森林那边学习来的，防止过拟合的效果比传统的行抽样还好（行抽样功能也有），并且有利于后面提到的并行化处理算法；\nsplit finding algorithms（划分点查找算法），树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法 greedy algorithm 枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以 XGBoost 还提出了一种可并行的近似直方图算法（Weighted Quantile Sketch），用于高效地生成候选的分割点；\n对缺失值的处理。对于特征的值有缺失的样本，XGBoost 可以自动学习出它的分裂方向。 稀疏感知算法 Sparsity-aware Split Finding；\n内置交叉验证（Built-in Cross-Validation），XGBoost 可以在 boosting 过程的每次迭代中运行交叉验证，因此很容易在一次运行中获得准确的最佳 boosting 迭代次数；\nXGBoost 支持并行，提高计算速度；","category":"page"},{"location":"AI/ML/#LightGBM","page":"机器学习","title":"LightGBM","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"LightGBM 是 XGBoost 的更高效实现，由微软发布。LightGBM 相比于 Xgboost，添加了很多新的方法来改进模型，包括：并行方案、基于梯度的单边检测（GOSS）、排他性特征捆绑等。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"LightGBM 的设计思路主要是两点：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"减小数据对内存的使用，保证单个机器在不牺牲速度的情况下，尽可能地用上更多的数据；\n减小通信的代价，提升多机并行时的效率，实现在计算上的线性加速。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"由此可见，LightGBM 的设计初衷就是提供一个快速高效、低内存占用、高准确度、支持并行和大规模数据处理的数据科学工具。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"LightGBM 并没有垂直的切分数据集，而是每个 worker 都有全量的训练数据，因此最优的特征分裂结果不需要传输到其他 worker 中，只需要将最优特征以及分裂点告诉其他 worker，worker 随后本地自己进行处理。处理过程如下：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"每个 worker 在基于局部的特征集合找到最优分裂特征；\nworker 间传输最优分裂信息，并得到全局最优分裂信息；\n每个 worker 基于全局最优分裂信息，在本地进行数据分裂，生成决策树；","category":"page"},{"location":"AI/ML/#其他知识","page":"机器学习","title":"其他知识","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"HMM：EM 算法、维特比算法、前向后向算法、极大似然估计","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"在 HMM 中，如果已知观察序列和产生观察序列的状态序列，可以用极大似然估计进行阐述估计。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"EM 算法只有观测序列，无状态序列时来学习模型参数，即 Baum-Welch 算法。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"维特比算法是用动态规划解决 HMM 的预测问题的，不是参数估计。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"前向后向算法是用来计算概率的。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"极大似然估计是观测序列和相应序列都存在时的监督学习算法，用来进行阐参数估计。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"序列模式挖掘算法、AprioriAll 算法、GSP 算法、FreeSpan 算法、PrefixSpan 算法","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"Apriori 算法：关联分析原始算法，用于从候选项集中发现频繁项集。两个步骤：进行自连接、进行剪枝。缺点：无时序先后性。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"AprioriAll 算法：AprioriAll 算法与 Apriori 算法的执行过程是一样的，不同点在于候选集的产生，需要区分最后两个元素的前后。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"AprioriSome 算法：可以看做是 AprioriAll 算法的改进","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"AprioriAll 算法和 AprioriSome 算法的比较：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（1）AprioriAll 用  去计算出所有的候选 Ck，而 AprioriSome 会直接用  去计算所有的候选 ，因为 包含 ，所以 AprioriSome 会产生比较多的候选。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（2）虽然 AprioriSome 跳跃式计算候选，但因为它所产生的候选比较多，可能在回溯阶段前就占满内存。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（3）如果内存占满了，AprioriSome 就会被迫去计算最后一组的候选。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（4）对于较低的支持度，有较长的大序列，AprioriSome 算法要好些。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"GPS算法：类Apriori算法。用于从候选项集中发现具有时序先后性的频繁项集。两个步骤：进行自连接、进行剪枝。缺点：每次计算支持度，都需要扫描全部数据集；对序列模式很长的情况，由于其对应的短的序列模式规模太大，算法很难处理。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"SPADE算法：改进的GPS算法，规避多次对数据集D进行全表扫描的问题。与GSP算法大体相同，多了一个IDLIST记录，使得每一次的IDLIST根据上一次的IDLIST得到（从而得到支持度）。而IDLIST的规模是随着剪枝的不断进行而缩小的。所以也就解决了GSP算法多次扫描数据集D问题。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"FreeSpan算法：即频繁模式投影的序列模式挖掘。核心思想是分治算法。基本思想为：利用频繁项递归地将序列数据库投影到更小的投影数据库集中，在每个投影数据库中生成子序列片断。这一过程对数据和待检验的频繁模式集进行了分割，并且将每一次检验限制在与其相符合的更小的投影数据库中。 优点：减少产生候选序列所需的开销。缺点：可能会产生许多投影数据库，开销很大，会产生很多的","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"PrefixSpan 算法：从FreeSpan中推导演化而来的。收缩速度比FreeSpan还要更快些。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"模型过拟合：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"原因：（1）训练数据太少；（2）模型太复杂；（3）参数过多；（4）噪声过多。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"解决办法：（1）获得更多的训练数据；（2）降低特征维度；（3）正则化；（4）Dropout；（5）早停 Early Stop；（6）数据清洗。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"参考","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"[1] GitHub 项目：ML-NLP；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"[2] XGBoost 特点、调参、讨论；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"[3] 诸葛越，葫芦娃，《百面机器学习》，中国工信出版集团，人民邮电出版社","category":"page"},{"location":"lang/Python/#基础知识","page":"Python","title":"基础知识","text":"","category":"section"},{"location":"lang/Python/","page":"Python","title":"Python","text":"关于 __name__：一个模块中，","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"如果直接运行文件 __name__ 为 __main__；\n如果该模块被调用，__name__ 为被调用模块的模块名；","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"# print_func.py 的代码如下\nprint('Hello World!')\nprint('__name__ value: ', __name__)\n \ndef main():\n    print('This message is from main function')\n \nif __name__ == '__main__':\n   main()\n\n\n# print_module.py 的代码如下\nimport print_func\nprint(\"Done!\")\n\n\n# 运行 print_module.py 的结果\n>>> Hello World! __name__ value: print_func  Done! ","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"py 文件执行完保持交互界面：在终端用命令行 python file.py 执行 py 文件的时候，有时候想要继续测试代码，那么可以在文件的最后添加上下面的两行代码，这样在执行完 py 文件之后就会保持命令行交互界面不退出。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"# 执行完不退出 Python 交互\nimport code\ncode(banner=\"\", local=locals())","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"为了提高内存利用效率，对于一些简单的对象，如一些数值较小的 int 对象（范围在 [-5, 257)），字符串对象等，Python 采用重用对象内容的方法。在 Python 3.6 中小整数对象池的范围会更大。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"a = [1, 2, 3]\nb = [1, 2, 4]\nid(a[1]) == id(b[1])  # 结果为 True\n\na[1] is b[1]  # 结果也为 True\n# 1. is 比较两个对象的 id 值是否相等，是否指向同一个内存地址；\n# 2. == 比较两个对象的内容是否相等，值是否相等","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"# 对于以下代码\n# 1. 两个整数相除，结果为整数\n# 2. 操作数之一是浮点，则两个数都转化为浮点计算\nprint type(1/2)  # Python 2.x\n>>> <type 'int'>\n\n# 无论是什么类型，都是按照正常的除法进行\nprint(type(1/2)) # Python 3.x\n>>> <type 'float'>","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"filter(object)：将迭代器的数据代入函数中，返回使函数返回值为 True 的值","category":"page"},{"location":"lang/Python/#函数","page":"Python","title":"函数","text":"","category":"section"},{"location":"lang/Python/","page":"Python","title":"Python","text":"Python 函数的闭包：如果在函数中定义的 lambda 或者 def 嵌套在一个循环之中，而这个内嵌函数又引用了一个外层作用域的变量，该变量被循环所改变，那么所有在这个循环中产生的函数会有相同的值 —— 也就是在最后一次循环中完成时被引用变量的值。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"def fn():\n    t = []\n    i = 0  # 外层作用域\n    while i < 2:\n        t.append(lambda x: print(i * x, end=\",\"))\n        i += 1\n    return t\n\nfor f in fn():  # fn() 执行完之后 i = 2\n    f(2)  # t = [lambda x: print(2 * x, end=\",\"), lambda x: print(2 * x, end=\",\")]\n>>> 4,4,","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"*args 和 **kwargs 是 Python 中方法的可变参数。*args 表示任何多个无名参数，是一个 Tuple；**kwargs 表示多个关键字参数，是一个 Dict。同时使用时 *args 参数要在 **kwargs 前面。当方法的参数不确定时，可以使用 *args 和 **kwargs.","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"a, *b, c = range(5)  # *b: 剩下的参数会分配给 b\nprint(a, b, c)\n>>> 0 [1, 2, 3] 4\n\n*a, *b, c = range(5)  # 这种表达会报错\n>>> SyntaxError: two starred expressions in assignment","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"Python 参数传递采用的是「传对象引用」的方式，这种方式相当于传值和传引用的一种综合。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"如果函数收到的是一个可变对象（比如 字典 或者 列表）的引用，就能修改对象的原始值 —— 相当于通过「传引用」来传递对象。\n如果函数收到的是一个不可变对象（比如 数字、字符 或者 元组）的引用，就不能直接修改原始对象 —— 相当于通过「传值」来传递对象。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"def changeList(nums):\n    nums.append('c')\n    print(\"nums\", nums)\n\nstr1 = ['a', 'b']\n# 调用函数\nchangeList(str1)\nprint(\"str1\", str1)\n>>> nums ['a', 'b', 'c'], str1 ['a', 'b', 'c']","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"Python 的默认参数只在函数定义时被赋值一次，而不会每次调用函数时又创建新的引用，函数定义完成后，默认参数已经存在固定的内存地址。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"如果使用一个可变的默认参数并对其进行改变，那么以后对该函数的调用都会改变这个可变对象\n默认参数如果是不可变对象，不存在该问题，每次调用都会将其变为默认值","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"def fun(a = (), b = []):\n    a += (1,)\n    b.append(1)\n    return a, b\n\nfun()\nprint(fun())\n>>> ((1,), [1, 1])\n# !!! 注意 a == (1,)，而 b = [1, 1]","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"函数修饰符 @：可以理解为引用、调用它修饰的函数","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"def test(f):\n    print(\"before ...\")\n    f()\n    print(\"after ...\")\n    \n@test\ndef func():\n    print(\"func was called\")\n\n# 运行之后的输出结果为\n>>> before ...\n>>> func was called\n>>> after ...","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"当 Python 解释器读到函数修饰符 @ 的时候，执行的步骤为：","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"调用 test 函数，test 函数的入口参数就是 func 函数；\ntest 函数被执行，入口参数的函数（func 函数）也会被调用；","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"def dec(f):\n    n = 3\n    \n    def wrapper(*args, **kw):\n        return f(*args, **kw) * n\n    \n    return wrapper\n\n@dec\ndef foo(n):\n    return n * 2\n\nfoo(2) == 12  # True\nfoo(3) == 18  # True\n\n# *args, **kw 是参数，用的是我们调用函数 foo(n) 时候的参数 n，\n# 注意与 dec(f) 里的 n = 3 作区分\n# 当我们利用了修饰符之后，就相当于\nfoo = dec(foo)","category":"page"},{"location":"lang/Python/#列表","page":"Python","title":"列表","text":"","category":"section"},{"location":"lang/Python/","page":"Python","title":"Python","text":"列表的切片一般指创造新的对象，是浅拷贝，不会有索引越界的情况，如果超出了列表的索引范围不会报错，会输出空列表。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"lists = [1, 2, 3, 4, 5, 6]\nprint(lists[6:])\n>>> []","category":"page"},{"location":"lang/Python/#字符串","page":"Python","title":"字符串","text":"","category":"section"},{"location":"lang/Python/","page":"Python","title":"Python","text":"str.upper()  # 把所有字符中的小写字母转化成大写字母\nstr.lower()  # 把所有字符中的大写字母转化成小写字母\nstr.capitalize()  # 把第一个字母转化为大写字母，其余小写\nstr.title()  # 把每个单词的第一个字母转化为大写，其余小写\n\nstr.find(str, beg=0, end=len(strs))  # 表示在 strs 中返回第一次出现 str 的位置下标\n# beg 表示在 strs 中的开始索引，默认为 0，end 为结束索引，默认为 strs 的长度。\n\nstr.rfind()  # 与 find 不同的在于它返回最后一次匹配的位置，如果匹配不到返回 -1\n\nstr.endswith(suffix[, start[, end]]) # 用于判断字符串是否以指定后缀结尾\n# 如果以指定后缀结尾返回 True，否则返回 False\n# start 与 end 为可选参数，代表检索字符串的开始和结束位置\n\nstr = \"Hello, Python\"\nsuffix = \"Python\"\nprint(str.endswith(suffix, 2))  # 从位置 2（'l'）开始判断字符串 str 是否以 suffix 结尾\n>>> True","category":"page"},{"location":"lang/Python/#集合","page":"Python","title":"集合","text":"","category":"section"},{"location":"lang/Python/","page":"Python","title":"Python","text":"集合（set）是一个无序的不重复元素序列","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"# 集合 set 用大括号 {x, y,...} 或者 set() 来创建\n# 注意：空集合的创建只能用 set(), {} 是创建空字典\ns.add(x)  # 添加元素\ns.update(x)  # 添加的元素可以是列表、元祖、字典等\ns.remove(x)  # 移除元素, 如果元素不存在会报错\ns.discard(x)  # 移除元素，不存在不会报错\ns.pop()  # 随机删除集合中的一个元素\nlen(s)  # 计算集合元素的个数\ns.clear()  # 清空集合\nx in s  # 判断 x 是不是在集合中\n\n# 如果集合 A 是集合 B 的子集，方法 issubset() 返回 True\n# The issubset() method returns True if set A is the subset of B\nA.issubset(B)","category":"page"},{"location":"lang/Python/#字典","page":"Python","title":"字典","text":"","category":"section"},{"location":"lang/Python/","page":"Python","title":"Python","text":"字典是 Python里唯一的映射类型，它存储了键值对的关联，是由键到键值的映射关系。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"# 字典里面有一个 get 方法\ndict.get(key, default)  # 当 key 对应的值存在时返回其本身，当 key 对应的值不存在时返回给定的 default 作为替代","category":"page"},{"location":"lang/Python/#默认字典","page":"Python","title":"默认字典","text":"","category":"section"},{"location":"lang/Python/","page":"Python","title":"Python","text":"defaultdict 中，当字典里的 key 不存在但被查找时，返回的不是 keyError 而是一个默认值。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"from collections import defaultdict  # 需要先导入\n# 用法 dict_type = defaultdict( factory_function)\ndict_int  = defaultdict(int)  # 不存在时返回 整数 0\ndict_set  = defaultdict(set)  # 不存在时返回 空集 {}\ndict_str  = defaultdict(str)  # 不存在时返回 空字符 \"\"\ndict_list = defaultdict(list) # 不存在时返回 空列表 []","category":"page"},{"location":"lang/Python/#Python-列表的生成","page":"Python","title":"Python 列表的生成","text":"","category":"section"},{"location":"lang/Python/","page":"Python","title":"Python","text":"注意：在生成列表的时候，最好用 [0 for _ in range(n)] 的方式而不是 [0] * n 的方式生成，原因如下。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"假设我们想要创建一个长度为 n = 2 列表 a 的时候，一般的做法有两种：a = [0] * n 和 a = [0 for _ in range(n)]，在一维的时候两种方法没有区别。但是如果我们想要创建一个列表，列表中的每个元素都是长度为 m = 3 的列表时用两种方法出来的结果就是不同的：","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"n, m = 2, 3\na = [[0] * n] * m  # 用第一种方法生成的\n# a = [[0, 0], [0, 0], [0, 0]]\nb = [[0 for _ in range(n)] for _ in range(m)]  # 用第二种方法生成的\n# b = [[0, 0], [0, 0], [0, 0]]\n\na[0][0] = 1  # 令 a 列表的第一个元素为 1\n# a = [[1, 0], [1, 0], [1, 0]]\nb[0][0] = 1\n# b = [[1, 0], [0, 0], [0, 0]]","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"我们的预期结果是像 b 这样的，也就是第一个列表的第一个元素等于 1，但是列表 a 是将每一个列表的第一个元素都设为 1 了。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"导致这种情况的原因在于，用第一种生成方法是类似于 = 的方式去生成的，也就是假设 a, b 都是列表，我们令 a = b，如果他们其中一个的元素改变了，另一个也会跟着变，例如：","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"a = [0, 0, 0]\nb = a\nb[0] = 1\n# a = [1, 0, 0]\n# b = [1, 0, 0]","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"要想避免这样的情况，就应该这样写 b = list(a). ","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"回到一开始的问题，如果用第一种方法 a = [[0] * n] * m，那么 a 中的 m 个列表存储的地址都是一样的，那么你改变其中的一个列表，其他的列表都会跟着改变，就会出现上面的情况。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"c = [[0] * n for _ in range(m)]  # 没问题\nd = [[0 for _ in range(n)]] * m  # 有问题","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"所以，以后要生成的列表时候，一律用 [0 for _ in range(n)] 会更好。","category":"page"},{"location":"lang/Python/#迭代器","page":"Python","title":"迭代器","text":"","category":"section"},{"location":"lang/Python/","page":"Python","title":"Python","text":"推荐阅读：Python: Built-in Types – Iterator Types;\nPython：可迭代对象、迭代器、生成器函数、生成器的解析举例代码说明；","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"迭代是 Python 范围集合元素的一种方法。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"可迭代对象（Iterable）：Python 中某对象实现 __iter__() 方法或者 __getitem__() 方法，且其参数从 0 开始索引，那么该对象就是可迭代对象。可以用 for 循环的对象，或者说序列（Sequence）都是可迭代对象，比如列表（list）、字典（dict）、元祖（tuple）、集合（set)、字符串（string）这些序列都是可迭代对象。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"使用 iter() 方法可以将可迭代对象变成迭代器，如果可迭代对象实现了 __iter__() 方法，那么调用该方法会返回一个迭代器对象。调用迭代器的 __next__() 方法返回每一次迭代的内容，直到迭代完成后抛出 StopIteration 异常。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"当使用 for 循环的时候，解释器会检查对象是否有 __iter__() 方法，有的话就调用它来获取一个迭代器；\n如果没有 __iter__() 方法但是实现了 __getitem__()，解释器会创建一个迭代器，尝试从 0 开始按顺序遍历元素；\n如果尝试失败，就会抛出一个 TypeError；\n字符串、列表、元祖、字典、集合等均不是迭代器，但是他们是可迭代对象，在迭代中本质上就是对调用 __iter__() 后得到的迭代器通过不断使用 next() 函数来实现的；\n使用  isinstance() 函数可以判断某个对象是否为某一类型，因此可以使用 isinstance(nums, Iterable) 来判断对象 nums 是否为可迭代对象；","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"迭代器（Iterator）：可以记住遍历对象的位置，其内部有一个状态用于记录迭代所在的位置，以便下次迭代时能够取出正确的元素。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"迭代器有两个基本方法 iter() 和 next()，前者可以将可迭代对象变成迭代器，后者可以返回下一个值；\n要定义一个迭代器必须实现 __iter__()  和 __next__() 方法。在 Python 2 中则要求类内包含有 next() 方法；\n迭代器只能往前不能后退，从集合的第一个元素开始访问，直到所有的元素被访问完之后结束；\n迭代器一定是可迭代对象，但是可迭代对象不一定是迭代器，例如字符串、字典、元祖、集合等；","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":">>> nums = [1, 2, 3]   # 创建一个列表\n>>> nums_iterator = iter(nums)  # 得到一个迭代器\n>>> print(nums_iterator)\n<list_iterator object at 0x7fe1a014e250>\n>>> print(next(nums_iterator))\n1\n>>> print(next(nums_iterator))\n2\n>>> print(next(nums_iterator))\n3\n# next() 方法调用到末尾时会跳出 StopIteration\n>>> print(next(nums_iterator))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nStopIteration","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"创建一个迭代器的具体方式如下所示，该代码实现的迭代器功能为迭代从 0 每次递增 1 到 9 的所有数字：","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"class my_inter:\n    def __iter__(self):\n        self.length = 10 \n        self.index = 0\n        return self\n\n\n    def __next__(self):\n        if self.index < self.length:\n            x = self.index\n            self.index += 1\n            return x\n        else:\n            raise StopIteration\n\n\nnums = my_inter()  # 实例化对象\nprint(nums)\n# 对实例化对象使用 iter() 返回迭代器\nnums_iter = iter(nums)\n\nfor x in nums_iter:  # 对迭代器进行迭代\n    print(x, end=\" \")\n\n>>> 0 1 2 3 4 5 6 7 8 9 ","category":"page"},{"location":"lang/Python/#生成器","page":"Python","title":"生成器","text":"","category":"section"},{"location":"lang/Python/","page":"Python","title":"Python","text":"在 Python 中使用了 yield 的函数称为生成器函数（Generator Function）。调用一个生成器函数，返回的是一个实例化迭代器对象。生成器函数返回一个只能用于迭代操作的生成器（Generator），生成器是一种特殊的迭代器，自动实现了「迭代器协议」，即实现 __iter__() 和 __next__() 两个方法，无需再手动实现。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"生成器因为有 send() 方法，因此在迭代的过程中可以改变当前迭代值，而这在普通迭代器上会引发异常。在调用生成器运行的过程中，每次遇到 yield 时函数会暂停并保存当前所有的运行信息，返回 yield 语句表达式的值，并在下一次执行 next() 或者 send() 方法时从当前位置继续运行。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"注意：在 Python 3 开始，生成器的 next() 方法变成 __next__()","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"使用以下方法判断是否为生成器函数或者是否为生成器：","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"from inspect import isgeneratorfunction\nisgeneratorfunction(x)  # 判断 x 是否为生成器函数\n\nimport types\nisinstance(x, types.GeneratorType)  # 判断 x 是否为生成器","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"创建一个生成器函数的具体方式如下所示：","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"def my_list(num):   # 定义生成器\n    now = 0   # 当前迭代值，初始为 0\n    while now < num:\n        val = yield now  # 返回当前迭代值，并接受可能的 send 发送值\n        # val 如果为空，迭代值自增 1；否则重新设定当前迭代值为 val\n        now = now + 1 if val is None else val\n\n\naddOneGenera = my_list(5)  # 得到一个生成器对象\nprint(\"下一个迭代值：{}\".format(addOneGenera.__next__()))\nprint(\"下一个迭代值：{}\".format(addOneGenera.__next__()))\nprint(\"重新设定当前迭代值为 3\")\naddOneGenera.send(3)\nprint(\"下一个迭代值：{}\".format(addOneGenera.__next__()))\n\n>>> 下一个迭代值：0\n>>> 下一个迭代值：1\n>>> 重新设定当前迭代值为 3\n>>> 下一个迭代值：4","category":"page"},{"location":"lang/Python/#虚拟环境-virtualenv","page":"Python","title":"虚拟环境 virtualenv","text":"","category":"section"},{"location":"lang/Python/","page":"Python","title":"Python","text":"pip, virtualenv, fabric 统称为 Python 的三大神器。其中 virtualenv 的作用是建立一个虚拟的 Python 环境。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"通过 pip 安装 virtualenv：pip install virtualenv，如果输入 virtualenv --version 能够输出版本号就代表安装成功了。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"为项目搭建新的虚拟环境：virtualenv nine-py，执行完之后会在当前的目录中创建一个相对应名字的文件夹，是独立的 Python 运行环境，包含了 Python 可执行文件，以及 pip 库的一份拷贝，在这个环境中安装的库都是独立的，不会影响到其他的环境。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"如果想要指定 Python 解释器：virtualenv -p /usr/bin/python2.7 nine-py","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"激活虚拟环境：source nine-py/bin/activate","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"停用虚拟环境：deactivate（停用之后会回到系统默认的 Python 解释器）","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"查看当前安装版本：pip freeze","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"将当前环境输出为文件：pip freeze > requirements.txt，会创建一个 requirements.txt 文件，其中包含当前环境所有包以及对应版本的简单列表。","category":"page"},{"location":"lang/Python/","page":"Python","title":"Python","text":"安装环境文件：pip install -r requirements.txt","category":"page"},{"location":"lang/Python/#文件操作","page":"Python","title":"文件操作","text":"","category":"section"},{"location":"lang/Python/#读取文件","page":"Python","title":"读取文件","text":"","category":"section"},{"location":"lang/Python/","page":"Python","title":"Python","text":"# 读取文件的不同方法\nread(size)  # 从文件当前位置起读取 size 个字节，若不给定参数则读取至文件末尾\nreadline()  # 每次读出一行内容，占用内存小，适合读取大文件\nreadlines()  # 读取文件所有行，保存在一个 list 中","category":"page"},{"location":"#Docs","page":"Home","title":"9Docs","text":"","category":"section"},{"location":"AI/GAN/#生成对抗网络","page":"生成对抗网络","title":"生成对抗网络","text":"","category":"section"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"2014 年，加拿大蒙特利尔大学的 Ian Goodfellow 和他的导师 Yoshua Bengio 提出生成对抗网络（Generative Adversarial Networks, GANs）。在 GANs 被提出来之后，发展迅速，出现了各种变种网络，包括 WGAN、InfoGAN、f-GANs、BiGAN、DCGAN、IRGAN 等。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"对于 GANs 的理解，可以想象成假币者与警察间展开的一场猫捉老鼠游戏，造假币者试图造出以假乱真的假币，警察试图发现这些假币，对抗使得二者的水平都得到提高。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"GANs 包括生成器（Generator）和判别器（Discriminator）两个部分。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（1）生成器的作用是合成「假」样本。它从先验分布中采样随机信号，通过神经网络得到模拟样本。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（2）判别器的作用是判断输入的样本是真实的还是合成的。它同时接收来自生成器的模拟样本和实际数据集的真实样本，并且判断当前接收的样本是「真」还是「假」。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"GANs 实际上是一个二分类问题，判别器 D 试图识别实际数据为真实样本，识别生成器生成的数据为模拟样本。它的损失函数写成负对数似然 （Negative Log-Likelihood），也称为 Categorical Cross-Entropy Loss，即：","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"mathcalL(D) = -int p(x) left p(data mid x) log D(x) + p(g mid x) log(1-D(x))  righttextdxqquad text(1)","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"其中 D(x) 表示判别器预测 x 为真实样本的概率，p(data mid x) 和 p(g mid x) 表示 x 分属真实数据集和生成器这两类的概率。即理解为，在给定样本 x 的条件下，该样本来自真实数据集 data 的概率和来自生成器的概率。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"样本 x 的来源应该各占实际数据集和生成器一半，即 p_textsrc(data)=p_textsrc(g)= 05。用 p_textdata(x)doteq p(xmid data) 表示从实际数据集得到 x 的概率，p_textg(x)doteq p(xmid g) 表示从生成器得到 x 的概率，有 x 的总概率：","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"p(x) = p_textsrc(data)p(xmid data) + p_textsrc(g)p(xmid g)","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"注：doteq 和 approx 是等价的，都是表达约等于的意思。一般写完等号之后，发现不是等于，而是约等于，所以就懒得涂抹写成 approx，所以就添加一个点。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"将损失函数 (1) 式中的 p(x)p(datamid x) 替换为 p_textsrc(data)p_textdata(x)，以及将 p(x)p(gmid x) 替换为 p_textsrc(g)p_textg(x)，就可以得到最终的目标函数","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"mathcalD=-frac12left( mathbbE_xsim p_textdata(x)left log D(x) right + mathbbE_xsim p_textg(x)left log (1 - D(x)) rightright)","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"在此基础上可以得到值函数","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"V(GD) = mathbbE_xsim p_textdata(x)left log D(x) right + mathbbE_xsim p_textg(x)left log (1 - D(x)) right","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"在训练的时候，判别器 D 的目标就是最大化上述值函数，生成器 G 的目标就是最小化它，因此整个 MinMax 问题可以表示为 undersetGminundersetDmax V(GD)。","category":"page"},{"location":"AI/GAN/#GANs-的训练方式","page":"生成对抗网络","title":"GANs 的训练方式","text":"","category":"section"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"我们知道 GANs 的值函数为","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"V(GD) = mathbbE_xsim p_textdata(x)left log D(x) right + mathbbE_xsim p_textg(x)left log (1 - D(x)) right","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"在训练的时候，判别器 D 的目标就是最大化上述值函数，生成器 G 的目标就是最小化它，因此整个 MinMax 问题可以表示为 undersetGminundersetDmax V(GD)。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"GANs 在训练的时候是采用生成器和判别器交替优化的方式进行的。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"判别器 D 的训练：","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（1）先固定生成器 G(cdot)；","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（2）利用生成器随机模拟产生样本 G(z) 作为负样本（z 是一个随机向量），并从真实数据集中采样获得正样本 X；","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（3）将正负样本输入到判别器 D(cdot) 中，根据判别器的输出 D(X) 和 D(G(z)) 和样本标签来计算误差；","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（4）最后利用误差反向传播算法来更新判别器 D(cdot) 的参数；","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"判别器的训练是这样的一个问题：给定生成器 G，寻找当前情况下的最优判别器 D^*_G 。对于单个样本 x，最大化 undersetDmax p_textdata(x)log D(x) + p_textg(x)log(1-D(x)) 的解为 hatD(x)=p_textdata(x)p_textdata(x)+p_textg(x)，外面套上对 x 的积分就得到 undersetDmax V(GD)，解由单点变成一个函数解：","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"D^*_G=fracp_textdatap_textdata+p_textg","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"此时 undersetGminV(GD^*_G)=undersetGminleft-log 4 + 2cdot textJSD(p_textdata p_textg)right，其中 textJSD(cdot) 是 JS 距离。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"优化生成器 G 实际上是在最小化生成样本分布与真实样本分布的 JS 距离。最终达到的均衡点是 textJSD(p_textdata p_textg) 的最小值点，即 p_textg=p_textdata 时，textJSD(p_textdata p_textg) 取到零，最优解 G^*(z)=xsim p_textdata(x)，D^*(x)equiv frac12，值函数 V(G^*D^*)=-log 4。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"生成器 G 的训练：","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（1）先固定判别器 D(cdot)；","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（2）然后利用当前生成器 G(cdot) 随机模拟产生样本 G(z)，输入到判别器 G(cdot) 中；","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（3）根据判别器的输出 D(G(z)) 和样本标签来计算误差；","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（4）最后利用误差反向传播算法来更新生成器 G(cdot) 的参数；","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"假设 G^prime 表示前一步的生成器，D 是 G^prime 下的最优判别器 D^*_G^prime。那么求解最优生成器 G 的过程为：","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"undersetGargminV(GD^*_G^prime)=undersetGargmintextKLleft( p_textg  fracp_textdata+p_textg^prime2 right) - textKL(P_textg P_textg^prime)","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"由此可以知道（1）优化 G 的过程是让 G 远离前一步的 G^prime，同时接近分布 (p_textdata+p_textg^prime)2；（2）达到均衡点时 p_textg^prime=p_textdata，有 undersetGargminV(GD^*_G^prime)=undersetGargmin()，如果用这时的判别器去训练一个全新的生成器 G_textnew，理论上可能啥也训练不出来。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"参考：","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"[1] 诸葛越，葫芦娃，《百面机器学习》，中国工信出版集团，人民邮电出版社","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"[2] Goodfellow I. J., Pouget-Abadie J., Mirza M., et al. Generative adversarial networks[J]. Advances in Neural Information Processing Systems, 2014, 3: 2672-2680. ","category":"page"}]
}
