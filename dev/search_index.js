var documenterSearchIndex = {"docs":
[{"location":"AI/CNN/#卷积神经网络","page":"卷积神经网络","title":"卷积神经网络","text":"","category":"section"},{"location":"AI/CNN/#深度卷积神经网络","page":"卷积神经网络","title":"深度卷积神经网络","text":"","category":"section"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"感受野（Receptive Field），指的是神经网络中神经元「看到的」输入区域，在卷积神经网络中，feature map 上某个元素的计算受输入图像上某个区域的影响，这个区域即该元素的感受野。感受野是个相对概念，某层 feature map 上的元素看到前面不同层上的区域范围是不同的，通常在不特殊指定的情况下，感受野指的是看到输入图像上的区域。","category":"page"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"例如两个级联的卷积核大小为 3times 3，stride = 2 的卷积层的感受野为 7times 7，如图所示","category":"page"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"(Image: 感受野)","category":"page"},{"location":"AI/CNN/#卷积详解","page":"卷积神经网络","title":"卷积详解","text":"","category":"section"},{"location":"AI/CNN/#卷积","page":"卷积神经网络","title":"卷积","text":"","category":"section"},{"location":"AI/CNN/#因果卷积（Causal-Convolution）","page":"卷积神经网络","title":"因果卷积（Causal Convolution）","text":"","category":"section"},{"location":"AI/CNN/#空洞卷积","page":"卷积神经网络","title":"空洞卷积","text":"","category":"section"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"空洞卷积具有更大的感受野，有助于构建长期记忆功能。","category":"page"},{"location":"AI/GNN/#通用框架","page":"图神经网络","title":"通用框架","text":"","category":"section"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"除了图神经网络的不同变体，人们还提出了一些通用框架，旨在将不同的模型集成到单一的框架中。^1","category":"page"},{"location":"AI/GNN/#消息传递神经网络","page":"图神经网络","title":"消息传递神经网络","text":"","category":"section"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"消息传递神经网络^2（MPNN, Message Passing Neural Network）包含两个阶段：消息传递阶段和读出阶段。\t","category":"page"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"","category":"page"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"参考：","category":"page"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"[1] 刘知远，周界，《图神经网络导论》","category":"page"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"[2]  J. Gilmmer, S. S. Schoenholz, P. F. Riley, et al. Neural message passing for quantum chemistry. In Proc. of ICML, 2018: 1263-1272. ","category":"page"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"[3] 刘忠雨，李彦霖，周洋，《深入浅出图神经网络》","category":"page"},{"location":"git/git_notebook/#第一章-Git-基础","page":"Git 学习笔记","title":"第一章 Git 基础","text":"","category":"section"},{"location":"git/git_notebook/#.-常用命令","page":"Git 学习笔记","title":"1. 常用命令","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.1. 配置 user 信息","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"配置自己的用户名为 strongnine，邮箱为 strongnine@163.com，实际用的时候请将此换成自己的用户名和邮箱。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"$ git config --global user.name 'strongnine'\n$ git config --global user.email 'strongnine@163.com'","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.2. config 的三个作用域","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"$ git config --global\n$ git config --local","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"可以显示自己目前的局部（local）或者全局（global）的配置。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 显示 config 设置\n$ git config --list --local\n$ git config --list --global","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.3. git 命令","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"创建仓库可以在 GitHub 上创建仓库，然后再通过 git clone 克隆到自己的本地，也可以现在本地新建的文件夹里用 git init 初始化创建仓库。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# git 仓库的初始化\n$ git init","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"查看当前仓库状态：git status 可以查看当前仓库的状态。能够看到文件的修改、删除、添加、以及重命名（重命名的逻辑就是删除一个文件并且添加一个文件），并且还能够看到当前存在的冲突啥的。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"添加文件：git add 可以将某个文件的更新添加到暂存区区里；","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"git add -u：将文件的修改、文件的删除，添加到暂存区。git add .：将文件的修改，文件的新建，添加到暂存区。git add -A：将文件的修改，文件的删除，文件的新建，添加到暂存区。git add -A 相对于 git add -u 命令的优点 ： git add -A 可以提交所有被删除、被替换、被修改和新增的文件到数据暂存区，而 git add -u 只能操作跟踪过的文件。git add -A 等同于 git add -all. ","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"撤销添加（版本回退）：有的时候我们 add 了一个文件，想要撤销，可以用 git reset","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"撤销添加：git reset HEAD 将绿字变成红字；","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"提交修改：git commit 将当前暂存区里的更新提交，会用默认编辑器跳出信息，可以在第一行添加提交的备注信息，例如 \"add README.md\". ","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"git commit -m\"add README.md\" 可以直接将备注信息一起提交。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"删除文件：git mv <文件名> 是正确删除文件的方法。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"重命名的文件：git mv oldname newname","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"当你重命名了一个文件之后，用 git status 会提示有一个文件被删除，有一个文件是新的 Untracked 文件。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"重置文件：git reset --hard 用来对暂存区的文件进行重置。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"warning: Warning\n注意：git reset 是一条相对危险的命令。","category":"page"},{"location":"git/git_notebook/#.-版本管理","page":"Git 学习笔记","title":"2. 版本管理","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"2.1. 分支管理","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"查看历史：git log 可以查看当前分支的提交历史记录日志，命令 gitk 可以调出图形界面查看历史版本。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"git log --graph 可以有比较图形化的界面；git log --oneline 只显示每次提交的备至；git log -n4 --online 指定查看最近 4 个 commit；git log --all 查看全部分支的日志；git log --all --graph 用图形化的方式显示所有分支的日志；","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"查看分支：git branch -v 可以查看本地有多少分支。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"git branch -av：查看所有分支；git branch -d 分支名：删除分支；git branch -D 分支名：强制删除分支；","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"warning: Warning\n如果分支还未被 merged 的时候要用强制删除，请确保该分支无用。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"创建分支：git checkout ","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"git checkout -b 可以创建新分支并且切换到该新的分支；","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"有的时候需要加上 --decorate 参数才可以显示（master）（temp）等分支信息。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"2.2. .git 目录的内容","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"cat 命令主要用来查看文件内容，创建文件，文件合并，追加文件内容等功能。 cat HEAD 查看 HEAD 文件的内容 git cat-file 命令 显示版本库对象的内容、类型及大小信息。 git cat-file -t b44dd71d62a5a8ed3 显示版本库对象的类型 git cat-file -s b44dd71d62a5a8ed3 显示版本库对象的大小 git cat-file -p b44dd71d62a5a8ed3 显示版本库对象的内容","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"HEAD：指向当前的工作路径 config：存放本地仓库（local）相关的配置信息。 refs/heads：存放分支 refs/tags：存放tag，又叫里程牌 （当这次 commit 是具有里程碑意义的，比如项目 1.0 的时候 就可以打 tag） objects：存放对象 .git/objects/ 文件夹中的子文件夹都是以哈希值的前两位字符命名 每个 object 由 40 位字符组成，前两位字符用来当文件夹，后 38 位做文件。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"commit、tree、blob 的关系","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"(Image: relations)","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"一个 commit 对应一颗 tree，tree 相当于文件夹，blob 相当于具体的文件（数据）。git 里面，文件内容相同， 就是视为同一个文件。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"当创建了新的空文件夹时，使用 status 不会检测到这个空的文件夹。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"2.3. 分离头指针","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"变更没有基于某个 branch，在分离头指针进行的 commit，如果没有及时合并到某个 branch，可能会被 git 当作垃圾清掉。如果这种变更是重要的，就要将其与某个 branch 绑在一起。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"git checkout -b 可以创建新分支并且切换到该新的分支。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"HEAD 指针可以指向某个分支的最后一次提交，也可以不和某个分支挂钩，当处于分离头指针时，可以直接指向某个 commit。它只能够定位到某个 commit。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"对比提交之间的差异：git diff [commit1] [commit2] 可以比较两个具体的 commit 的差异。git diff HEAD HEAD^1 将当前结点与其父亲结点进行对比。HEAD^1, HEAD~1, HEAD~, HEAD^ 都一样。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"一个节点，可以包含多个子节点（checkout 出多个分支）\n一个节点可以有多个父节点（多个分支合并）\n^ 和 ~ 都是父节点，区别是跟随数字时候，^2 是第二个父节点，而 ~2 是父节点的父节点\n^ 和 ~ 可以组合使用,例如 HEAD~2^2","category":"page"},{"location":"git/git_notebook/#第二章-独自使用-Git","page":"Git 学习笔记","title":"第二章 独自使用 Git","text":"","category":"section"},{"location":"git/git_notebook/#.-commit-的操作","page":"Git 学习笔记","title":"1. commit 的操作","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.1. 修改 commit 的 message","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 修改最新 commit 的信息\n$ git commit --amend\n# 想要修改旧 commit 的信息，需要先选择其父节点\n# 运行后会弹出一个交互界面，在里面修改、保存之后\n# 还会继续弹出一个交互界面，提示要把 message 如何修改\n$ git rebase -i 父节点","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"warning: Warning\n这种操作只适用于还未合并到「主线」 的分支上，否则会影响到合作者的工作。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.2. 整理多个 commit ","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 和上一个操作相似\n# 在弹出的交互界面进行不同的修改就行（会有提示）\n$ git rebase -i 父节点\n\n# 上面的是把「连续的」commit 合并，还有一种是把「间隔的」合并\n$ git rebase -i 父节点","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.3. 对比差异","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 对比暂存区和 HEAD 里面内容的差异（看看做了哪些改动）\n$ git diff --cached\n\n# 对比工作区和暂存区的不同\n$ git diff\n\n# 只比较某个文件\n$ git diff -- <文件名>\n\n# 查看不同提交的指定文件的差异\n$ git diff <指针 1> <指针 2> -- <文件名>","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.4. 恢复变更","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 把暂存区里面的文件全部恢复成和 HEAD 一样的\n$ git reset HEAD\n\n# 让工作区的文件恢复为暂存区一样（变更工作区）\n$ git checkout -- index.html\n\n# 取消暂存区部分文件的更改\n$ git reset HEAD -- <文件名>...","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.5. 消除最近几次提交","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 将头指针指向特定的某次提交，并且删除这之前的提交\n# <危险指令> 慎用！！！\n$ git reset --hard <指针>","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.6. 删除文件","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 正确删除文件的方法\n$ git rm <文件名>","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.7. 临时加塞的紧急任务 —— stash 的使用","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 把当前状态存放\n$ git stash\n# 列出状态区\n$ git stash list\n# 恢复暂存区（弹出之前放进 stash 顶的），但是 stash 堆栈里的信息还会在\n$ git stash apply\n# 恢复的基础上还会丢掉 stash 里面的信息\n$ git stash pop","category":"page"},{"location":"git/git_notebook/#.-Git-管理","page":"Git 学习笔记","title":"2. Git 管理","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"2.1. 指定不需要 Git 管理的文件","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":".gitignore 文件上的内容就是表示指定类型的文件不给 Git 进行管理。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"2.2. Git 的备份","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"(Image: git_learning_fig2)","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"哑协议传输进度不可看见，智能协议可见。智能协议比哑协议快。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# --bare 代表不带工作区的裸仓库\n# 哑协议\n$ git clone --bare /path/to/repo.git <拷贝路径.git>\n# 智能协议\n$ git clone --bare / file:///path/to/repo.git <拷贝路径.git>\n\n# 把本地的变更同步到远端\n$ git remote -v\n$ git remote add <名称> <协议地址>\n# 查看分支\n$ git branch -av\n$ git push <名称>\n$ git push --set-upstream <  > <  >","category":"page"},{"location":"git/git_notebook/#第三章-Github-同步","page":"Git 学习笔记","title":"第三章 Github 同步","text":"","category":"section"},{"location":"git/git_notebook/#配置公私钥","page":"Git 学习笔记","title":"配置公私钥","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"在 Github 首页上，寻找 help，在上面有关于如何 connecting to github with SSH 的做法。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 打开 git bash 在里面输入下面命令\n# 若干存在 id_rsa 和 id_rsa.pub 文件则代表已经有公私钥\n# 否则应该要根据 Help 上的提示进行生成\n$ ls - al ~/.ssh\n# 查看 id_rsa.pub 的内容\n$ cd ~/.ssh\n$ cat id_rsa.pub\n# 复制里面的内容，前往 github 账户设置里面添加 SSH keys","category":"page"},{"location":"git/git_notebook/#**把本地仓库同步到-Github**","page":"Git 学习笔记","title":"把本地仓库同步到 Github","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 添加一个新的 remote\n$ git remote add <名称> <SSH>\n# 查看已有的 remote\n$ git remote -v\n\n# 把所有内容 push\n$ git push <name> --all\n# 如果远端有某些文件是本地未包含的，这个分支会被拒绝 push\n# 需要把远端的先「拉」下来\n$ git fetch <name> master\n# 切换到 master 分支\n$ git checkout master\n# 与远端的 .../master 的分支合并\n$ git merge <远端分支>\n# 但注意如果两个分支都是独立的，没有共同的历史，那么会拒绝合并\n# 查看 merge 帮助\n$ git merge -h\n$ git merge --allow-unrelated-histories <远端分支>\n# 现在进行 push 就不会报错了\n$ git push <name> master","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"注：在之后为了方便学习，将一些命令与视频里面的进行同步，<name> 会用 github 来代替，因为我们把远端的仓库 fetch 下来并且命名为 gitHub 了","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"个人笔记总结git remote -v 查看远程版本库信息\ngit remote add <name> <url>添加 githup 远程版本库\ngit fetch <name> 拉取远程版本库\ngit merge -h 查看合并帮助信息\ngit merge --allow-unrelated-histories githup/master 合并 <name> 上的 master 分支（两分支不是父子关系，所以合并需要添加 –allow-unrelated-histories）\ngit push <name> 推送同步到 <name> 仓库—— by DriveMan_邱佳源","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"fast forward 到底是什么？举个例子，本地分支往远端分支做 push，如果远端分支不是本地分支的祖先，那它俩就不是 fast forward 了。反之，它俩就是 fast forward 的关系。","category":"page"},{"location":"git/git_notebook/#第四章-Git-多人单分支集成协作","page":"Git 学习笔记","title":"第四章 Git 多人单分支集成协作","text":"","category":"section"},{"location":"git/git_notebook/#.-多个人对文件修改","page":"Git 学习笔记","title":"1. 多个人对文件修改","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.1. 不同人修改了不同文件","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# \n# 会出现 Non fast_forwards 的报错，远端仓库会拒绝这个 push\n# 先把远端的 fetch 下来\n$ git fetch <name>(github)\n# 然后查看 branch 会发现有 [ahead 1, behind 1] 这样的信息，\n# 代表远端有的这里没有和这里有的远端没有\n$ git branch -av\nfeature/add_git_commands     07c85df [ahead 1, behind 1] ......\n\n# 有时候会考虑合并\n$ git merge (github/feature/add_git_commands)","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"老师你好，我有个问题哈，clone 命令 git clone git@github.com:git2019/gitlearning.git 既然已经把远程仓库所有内容都克隆到本地了，为什么还需要 git checkout -b feature/addgitcommands origin/feature/addgit_command 命令基于远程分支在本地建立分支，不是从远程clone下来了嘛，为什么还要新建，难道 clone 命令不能克隆分支吗？作者回复：我们在本地无法直接在 clone 下来的远程分支上做变更的，只能基于远程分支建本地分支后，才能创建 commit。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.2. 不同人修改同一文件的不同区域","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# pull 会把远端的内容拉下来，并且本地的也会进行更新\n# 简介的方法就是直接 pull，还有一种是 fetch + merge\n# 多查看 branch ，看看 ahead 和 behind 的数目\n$ git branch -av\n\n# 当只有 ahead 没有 behind 的时候，肯定是 fast-forward 可以提交","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"fast-forword 看了英语翻译为快进，结合 git branch -av 中的 ahead 和 behind，ahead 是本地仓库比远端仓库多 commit，behind 是本地仓库比远端仓库少 commit。对正常的备份系统来说，我本地只能比备份多，备份不可能比我本地多才是。然而，git 由于多用户提交原因出现备份比本地多了，本地滞后了，所以需要 pull 一下，让本地比备份相等或多，这种情况就是 fast forward ，也就是我本地要比备份快进。不知理解对否？作者回复：其实就是两个分支的关系为 0|n 或者 n|0 ，如果两个分支直接为 n|m 的关系就不是 fast forward 。A 分支比 B 分支多 5 个 commit，B 比 A 分支多 3 个 commit。A 和 B 就不是 fast forward。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.3. 不同人修改同一文件的同一区域","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 如果 push 不上去，使用 merge 又提示已经更新到最新了\n# 就说明远端变了，要及时更新\n$ git pull\nAuto-merging (index.html)\nCONFLICT(content): Merge conflict in (index.html)\n# 提示 CONFLICT(content) 说明文件有冲突，不能自动合并 index.html\n# 打开这个文件，会提示哪里出现冲突\n$ vi index.html\n# 编辑完成后查看状态\n$ git status\n\n# 如果这个分支有问题了，可以用 --abort 退出合并\n$ git merge --abort\n$ git commit -am'(commit text)'","category":"page"},{"location":"git/git_notebook/#.-更改了文件名","page":"Git 学习笔记","title":"2. 更改了文件名","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"2.1. 同时变更了文件名和内容","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 其中有一个人变更了文件名\n# 另一个人只变更了文件内容\n# pull 的话会智能识别问题\n$ git pull","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"2.2. 同一文件改成不同的文件名","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 依旧是报冲突\n$ git pull\n# 查看工作目录，会出现未命名的文件，和两个重命名的文件\n# 如果使用 diff 查看两个文件的差异，不会显示差异\n$ diff <file-1> <file-2>\n# 使用 status，会提示：\nboth deleted:  <oldfilename>\nadded by us:   <filename-1>\nadded by them: <filename-2>\n# 可以先移除不要的文件，再加上想要保存的文件名\n$ git rm <filename-2>\n$ git add <filename-1>\n$ git commit -am'(commit text)'","category":"page"},{"location":"git/git_notebook/#第五章-集成使用禁忌","page":"Git 学习笔记","title":"第五章 集成使用禁忌","text":"","category":"section"},{"location":"git/git_notebook/#.-禁止向集成分支执行-push-f","page":"Git 学习笔记","title":"1. 禁止向集成分支执行 push -f","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"-f, --force 是强制更新，即使不是 fast-forward 也可以 push。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 把历史 reset 到某个 log\n$ git reset --hard (b3f390c)\n# 强制 push，就会把在 b3f390c 后面做的改变都删除\n$ git push -f (origin) (feature/add_git_commands)","category":"page"},{"location":"git/git_notebook/#.-禁止向集成分支执行变更历史的操作","page":"Git 学习笔记","title":"2. 禁止向集成分支执行变更历史的操作","text":"","category":"section"},{"location":"git/git_notebook/#第六章-GitHub","page":"Git 学习笔记","title":"第六章 GitHub","text":"","category":"section"},{"location":"git/git_notebook/#.-核心功能","page":"Git 学习笔记","title":"1. 核心功能","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"代码预览、项目管理、集成、团队管理、社交编码（开源）、文档、存放代码。","category":"page"},{"location":"git/git_notebook/#.-寻找开源项目","page":"Git 学习笔记","title":"2. 寻找开源项目","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"使用高级搜索：光标放在搜索框里，按回车就会出现 advanced search 了。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"可以在 Help 上查看有哪些高级搜索的语法。  ","category":"page"},{"location":"git/git_notebook/#.-搭建个人博客","page":"Git 学习笔记","title":"3. 搭建个人博客","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"通过高级搜索在搜索框中输入 blog easily start in:readme stars:>5000 找到 jekyll-now 仓库。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"第一步就是 fork 一个到自己的账号里去。fork 完后修改工程名称：<username>.github.io","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"第二步修改 _config.yml。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"在 _posts 里面新增文件格式为：2018-12-24-<title>.md","category":"page"},{"location":"git/git_notebook/#第七章-团队协作","page":"Git 学习笔记","title":"第七章 团队协作","text":"","category":"section"},{"location":"git/git_notebook/#.-创建团队项目","page":"Git 学习笔记","title":"1. 创建团队项目","text":"","category":"section"},{"location":"git/git_notebook/#.-挑选合适的分支集成策略","page":"Git 学习笔记","title":"3. 挑选合适的分支集成策略","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"在仓库的 Insights => Network 里可以看到特性分支演变历史。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"在 Options 的 Merge button 可以设置允许哪种合并。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"merge 把分支最后合并到 master 上去；\nsquash merging 把分支的所有 commits 变成一个，再放到主线上去。（在当前主线后面加上）\nrebase merging 把分支的所有 commits 添加到主线后面去。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"后面两种适合于线性开发的情况。","category":"page"},{"location":"git/git_notebook/#.-issue-跟踪","page":"Git 学习笔记","title":"4. issue 跟踪","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"issue 上有标签管理，对不同的问题进行分类。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"还可以对 issue 进行模型管理，自定义一些问题报告的模板。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"有 Bug report、Feature request 等。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"使用 Projects 的看板来管理 issue","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"点击 Pojects 进行看板（Board）的设置。","category":"page"},{"location":"git/git_notebook/#.-Code-review","page":"Git 学习笔记","title":"5. Code review","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"在 Settings 的 Branches 上可以设置特殊分支的保护规则。比如对于 master 分支进行 push 保护，每次 push 都要有特定人数去检查才能通过。","category":"page"},{"location":"git/git_notebook/#.-多分支的集成","page":"Git 学习笔记","title":"6. 多分支的集成","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"特性分支往主干合，要发 Pull requests。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"git rerere 是一个隐藏功能，允许你让 Git 记住解决一个块冲突的方法，在下一次看到相同冲突时，自动解决。","category":"page"},{"location":"git/git_notebook/#第八章-GitLab","page":"Git 学习笔记","title":"第八章 GitLab","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"这两章先看视频过一遍，等到应用的时候可以复习。目前不知道具体的使用需求，先大概看个印象。","category":"page"},{"location":"git/git_notebook/#其它问题","page":"Git 学习笔记","title":"其它问题","text":"","category":"section"},{"location":"git/git_notebook/#.-在-Windows-上如何支持中文","page":"Git 学习笔记","title":"1. 在 Windows 上如何支持中文","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"参考解决 Git 在 windows 下中文乱码的问题.md。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"有一个注意的点：目前无法解决输入中文字符会显示乱码的问题解决方案：git commit 时，不用 -m 参数，直接回车让 vim 来处理\n进 vim 后按 i 进入编辑模式，完成后再保存退出","category":"page"},{"location":"AI/RNN/#循环神经网络","page":"循环神经网络","title":"循环神经网络","text":"","category":"section"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"记录网络的输入序列为 x_1x_2cdotsx_n，一个循环神经网络（RNN）展开后可以看做一个 n 层的前馈神经网络，第 t 层对应着 t 时刻的状态（t=12cdotsn），记第 t 层（时刻）的输入状态、隐藏状态、输出状态分别为 x_t h_t o_t，训练时的目标输出值为 y_t，则有：隐藏状态 h_t 由当前时刻的输入状态 x_t 和上一时刻的隐藏状态 h_t-1 共同确定，即","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"h_t=sigma(Ux_t+Wh_t-1+b)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中，U 是输入层到隐藏层的权重矩阵，W 是不同时刻的隐藏层之间的连接权重，b 是偏置向量，sigma(cdot) 是激活函数（通常使用 textttTanh 函数）。循环神经网络最大的特点就是当前时刻的隐藏状态不仅与当前时刻的输入状态有关，还受上一时刻的隐藏状态影响。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"输出状态 o_t 的计算公式为：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"o_t=g(Vh_t+c)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中，V 是隐藏层到输出层的权重矩阵，c 是偏置向量，g(cdot) 是输出层的激活函数（对于分类任务可以采用 textttSoftmax 函数）。在训练时，网络在整个序列上的损失可以定义为不同时刻的损失之和：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"mathcalL=sum_tmathcalL_t=sum_t Loss(o_ty_t)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"上述的权重矩阵 U W V 是所有时刻共享参数的，这种机制不仅可以极大地减少网络需要学习的参数数量，而且使得网络可以处理长度不固定的输入序列。在 RNN 的训练过程中，由于不同时刻的状态是相互依赖的，因此需要存储各个时刻的状态信息，而且无法进行并行计算，这导致整个训练过程内存消耗大，并且速度较慢。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"RNN 之所以能够在序列数据的处理上获得出色的表现，是因为它拥有长期记忆功能，能够压缩并获得长期数据的表示。实际上，在 RNN 训练过程中，为了防止梯度爆炸（或弥散）的问题，通常采用带截断的反向传播算法，即仅反向传播 k 个时间步的梯度。理论上的无限记忆优势在实际中几乎不存在。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"实际上，在序列任务中，卷积神经网络（CNN）在空洞卷积的帮助下（例如 TextCNN），具有更好的并行化和可训练性。只不过长短期记忆网络（LSTM）和 Seq2Seq 网络依然是序列数据处理中最为通用的架构。还有人很多工作对卷积神经网络和循环神经网络进行组合使用，提升序列数据处理能力（如 TrellisNet）。","category":"page"},{"location":"AI/RNN/#长短期记忆网络（LSTM）","page":"循环神经网络","title":"长短期记忆网络（LSTM）","text":"","category":"section"},{"location":"AI/RNN/#长程依赖问题","page":"循环神经网络","title":"长程依赖问题","text":"","category":"section"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"LSTM 是如何实现长短期记忆功能的？","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（1）一般的 RNN 中，只有一个隐藏状态（hidden state）单元 h_t，不同时刻隐藏状态单元的参数是相同（共享）的。LSTM 在普通 RNN 的基础上增加了一个元胞状态（cell state）单元 c_t，其在不同时刻有着可变的连接权重，可解决普通循环神经网络中的梯度消失或爆炸问题。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（2）LSTM 引入了门控单元，是神经网络学习到的用于控制信号的存储、利用和舍弃的单元。对于每一个时刻 t，LSTM 有输入门 i_t、遗忘门 f_i 和输出门 o_t 共 3 个门控单元。每个门控单元的输入包括当前时刻的序列信息 x_t 和上一时刻的隐藏状态单元 h_t-1，具体公式：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"i_t=sigmaleft( W_i x_t + U_i h_t-1 + b_i right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"f_t=sigmaleft( W_f x_t + U_f h_t-1 + b_f right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"o_t=sigmaleft( W_o x_t + U_o h_t-1 + b_o right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"3 个门控单元都相当于一个全连接层，激活函数 sigma(cdot) 的取值范围是 0 1，常用 textttSigmoid 作为激活函数。当门控单元的状态为 0 时，信号会被全部丢弃；当状态为 1 时，信号会被全部保留。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（3）元胞状态单元从上一个时刻 c_t-1 到当前时刻 c_t 的转移是由输入门和遗忘门共同控制的。输入门决定了当前时刻输入信息 tildec_t 有多少被吸收，遗忘门决定了上一时刻元胞状态单元 c_t-1 有多少不被遗忘，最终的元胞状态单元 c_t 由两个门控处理后的信号取和产生。具体公式：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"tildec_t = textttTanhleft( W_c x_t + U_c h_t-1 + b_c right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"c_t = f_t odot c_t-1 + i_t odot tildec_t","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中 odot 为逐元素点乘操作。LSTM 的隐藏状态单元 h_t 则由输出门 c_t 决定：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"h_t = o_t odot textttTanh(c_t)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"不仅隐藏状态单元 h_t-1 和 h_t 之间有着较为复杂的循环连接，内部的元胞状态单元 c_t-1 和 c_t 之间还具有线性自循环关系，这个关系可以看作是在滑动处理不同时刻的信息。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（4）LSTM 中遗忘门和输出门的激活函数十分重要。删除遗忘门的激活函数会导致之前的元胞状态不能很好地被抑制；而删除输出门的激活函数则可能会出现非常大的输出状态。","category":"page"},{"location":"AI/RNN/#门控循环单元（GRU）","page":"循环神经网络","title":"门控循环单元（GRU）","text":"","category":"section"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（1）GRU 只有两个门控单元，分别为重置门 r_t 和 更新门 z_t，一个控制短期记忆，另一个控制长期记忆。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（2）GRU 中每个门控单元的输入包括当前时刻和序列信息 x_t 和上一时刻的隐藏状态单元 h_t-1，具体计算公式为：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"r_t = sigmaleft( W_r x_t + U_r h_t-1 right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"z_t = sigmaleft( W_z x_t + U_z h_t-1 right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中 sigma(cdot) 是激活函数，一般用 textttSigmoid。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（3）GRU 中重置门决定先前的隐藏状态单元是否被忽略，而更新门则控制当前隐藏状态单元是否需要被新的隐藏状态单元更新，具体公式：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"tildeh_t = textttTanh left( W_h x_t + U_h (r_t odot h_t-1) right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"h_t = (1 - z_t) h_t - 1 + z_t tildeh_t","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中，(1 - z_t)h_t-1 表示上一时刻保留下来（没被遗忘）的信息，z_t tildeh_t 是当前时刻记忆下来的信息。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"用 1 - z_t z_t  作为系数，表明对上一时刻遗忘多少权重的信息，就会在这一时刻记忆多少权重的信息以作为弥补。GRU 就是用这样的一种方式用一个更新门 z_t 实现遗忘和记忆两个功能。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（4）GRU 只有一个隐藏状态单元 h_t，而 LSTM 有隐藏状态单元 h_t 和元胞状态单元 c_t。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（5）GRU 具有更少的参数，更易于计算和实现。在不同数据集、不同超参配置下，可以取得与 LSTM 相当甚至更好的性能，并且具有更快地收敛速度。","category":"page"},{"location":"AI/RNN/#序列到序列（Seq2Seq）","page":"循环神经网络","title":"序列到序列（Seq2Seq）","text":"","category":"section"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（1）Seq2Seq 的映射架构能够将一个可变长序列映射到另一个可变长序列。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（2）Seq2Seq 框架由于输入序列和输出序列是不等长的因此整个处理过程需要拆分为对序列的理解和翻译，也就是编码和解码。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（3）采用一个固定尺寸的状态向量 C 作为编码器与解码器之间的「桥梁」。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（4）假设输入序列为 X=(x_1x_2cdotsx_T)，编码器可以是一个简单的循环神经网络，其隐藏状态 h_t 的计算公式为：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"h_t = f(h_t-1 x_t)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中，f(cdot) 是非线性激活函数，可以是简单的 textttSigmoid 函数，也可以是复杂的门控函数（LSTM、GRU 等）。将上述循环神经网络（编码器）最后一个时刻的隐藏状态 h_T 作为状态向量，并输入到解码器。C 是一个尺寸固定的向量，并且包含了整个序列的所有信息。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（5）解码器需要根据固定尺寸的状态向量 C 来生成长度可变的解码序列 Y=(y_1 y_2 cdots y_T)。这里解码序列的长度 T^prime 和编码长度 T 可以是不同的。解码器也可以用一个简单的循环神经网络来实现，其隐藏状态 h_t 可以按照如下公式计算：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"h_t = f(h_t-1y_t-1C)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"解码器的输出由如下公式决定：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"P(y_tmid y_t-1y_t-2cdotsy_1C) = g(h_t y_t-1C)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中，g(cdot) 会产生一个概率分布（例如用 textttSoftmax 函数产生概率分布）。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（6）解码器的工作流程：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"首先在收到一个启动信号（如 y_0=text start gt）后开始工作，根据 h_t y_t-1 C 计算出 y_t  的概率分布；\n然后对 y_t 进行采样获得具体取值；\n循环上述操作，直到遇到结束信号（如 y_t=text eos gt；","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（7）解码器的实现还能够用一种更加简单的方式，仅在初始时刻需要状态向量 C，其他时刻仅接收隐藏状态和上一时刻的输出信息 P(y_t)=g(h_ty_t-1)。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（8）在训练时，需要让模型输出的序列尽可能正确，这可以通过最大化对数似然概率来实现：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"max_theta frac1Nsum_n=1^N log p_theta(Y_n mid X_n)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中 theta 为模型参数，X_n 是一个输入序列，Y_n 是对应的输出序列， (X_nY_n) 构成一个训练样本对。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（9）因为是序列到序列的转换，实际应用中可以通过贪心法求解 Seq2Seq，当度量标准、评估方式确定后，解码器每次根据当前的状态和已解码的序列选择一个最佳的解码结果，直至结束。","category":"page"},{"location":"AI/FE/#格莱姆角场（GAF）","page":"特征工程","title":"格莱姆角场（GAF）","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"利用 GAF 算法将一维时间序列数据转为二维图片数据的代码，在 GitHub 项目 Series2Image 上可以找到（记得把 star 点上呀！）","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"原文地址：Encoding Time Series as Images","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"虽然现在深度学习在计算机视觉和语音识别上发展得很好，但是碰到时间序列时，构建预测模型是很难的。原因包括循环神经网络较难训练、一些研究比较难以应用，而且没有现存与训练网络，1D-CNN 不方便。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"但是如果使用 Gramian Angular Field (GAF)，可以把时间序列转成图片，充分利用目前机器视觉上的优势。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"这篇文章会包括下面这些内容：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"数学先验知识；\nGram Matrix 为何可以为单变量的时间序列构建一个好的二维表示；\nGram Matrix 点积为何不能表示 CNN 的数据；\n为 CNN 准备好 Gram Matrix 结构的操作是什么；","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"还会包括 Python 代码：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"用于 GAF 计算的 NumPy 工具；","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"下面的动图展示了对数据进行极坐标编码，然后对生成的角度进行类似于 Gram 矩阵的操作：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(Image: )","category":"page"},{"location":"AI/FE/#.-数学先验知识","page":"特征工程","title":"1. 数学先验知识","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"GAF 的数学方法与内积与相应的 Gram 矩阵有很深的联系。","category":"page"},{"location":"AI/FE/#.1.-点积（Dot-product）","page":"特征工程","title":"1.1. 点积（Dot product）","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"内积是两个向量之间的运算，用来度量它们的「相似性」。它允许使用来自传统 Euclidian Geometry 的概念：长度、角度、第二维度和第三维度的正交性。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"在二维空间上，两个向量 u 和 v 之间的内积定义为：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"langle uvrangle=u_1 cdot v_1+u_2 cdot v_2","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"或者：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"langle uvrangle=ucdot vcdotcos(theta)","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"如果 u 和 v 的范数为 1，我们就得到：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"langle uvrangle=cos(theta)","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"因此，如果处理的是单位向量，他们的内积就只由角度 theta 决定了，这个角度可以用弧度来表示。计算出来的值会在 [-1,1] 内。记住这些定理，在本文的其他位置会用到。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"注意：在 Euclidian 集合（n 维）中，两个向量的内积的正式定义为：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"langle uvrangle=sum_i=1^nu_icdot v_i","category":"page"},{"location":"AI/FE/#.2.-Gram-矩阵（Gram-Matrix）","page":"特征工程","title":"1.2. Gram 矩阵（Gram Matrix）","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"在线性代数和几何中，Gram 矩阵是一个有用的工具，它经常用于计算一组向量的线性相关关系。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"定义：一组 n 个向量的 Gram 矩阵是由每一对向量的点积定义的矩阵。从数学上讲，这可以解释为：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"G=beginpmatrix \n langle u_1v_1rangle  langle u_1v_2rangle  cdots  langle u_1v_nrangle  \n langle u_2v_1rangle  langle u_2v_2rangle  cdots  langle u_2v_nrangle \nvdots  vdots  ddots  vdots \n langle u_nv_1rangle  langle u_nv_2rangle  cdots  langle u_nv_nrangle \nendpmatrix","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"再有，假设所有的二维向量都是单位向量，我们会得到：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"G=beginpmatrix \n cos(phi_11)  cos(phi_12)  cdots  cos(phi_1n)  \n cos(phi_21)  cos(phi_22)  cdots  cos(phi_2n)  \nvdots  vdots  ddots  vdots \n cos(phi_n1)  cos(phi_n2)  cdots  cos(phi_nn)  \nendpmatrix","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"其中 Phi(ij) 是两个向量的夹角。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"关键结论：为什么要用 Gram 矩阵？Gram 矩阵保留了时间依赖性。由于时间随着位置从左上角到右下角的移动而增加，所以时间维度被编码到矩阵的几何结构中。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(Image: )","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"注：单变量时间序列在某种程度上无法解释数据的共现和潜在状态；我们的目标应该是找到替代的和更丰富的表示。","category":"page"},{"location":"AI/FE/#.-实现方式","page":"特征工程","title":"2. 实现方式","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"假设有一时间序列 X=x_1cdotsx_n：","category":"page"},{"location":"AI/FE/#.1.-缩放","page":"特征工程","title":"2.1. 缩放","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"使用一个限定在 [-1,1] 的最小-最大定标器（Min-Max scaler）来把时间序列缩放到 [-1,1] 里，这样做的原因是为了使内积不偏向于值最大的观测。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(Image: )","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"在这个用例中，标准缩放器 不是合适的候选者，因为它的输出范围和产生的内部积都可能超过 [- 1,1]。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"然而，与最小-最大定标器结合，内积确实保留了输出范围：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"langlecdotcdotrangle-11times-11rightarrow-11","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(xy)rightarrow xcdot y","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"在 [-1,1] 中进行点积的选择并不是无害的。如非必要，把 [-1,1] 作为输入范围是非常可取的。","category":"page"},{"location":"AI/FE/#.2.-噪声图片","page":"特征工程","title":"2.2. 噪声图片","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"缩放完时间序列之后，我们计算每一对的点积并把它们放进 Gram 矩阵里：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"G=beginpmatrix \nx_1cdot x_1  x_1cdot x_2  cdots  x_1cdot x_n  \nx_2cdot x_1  x_2cdot x_2  cdots  x_2cdot x_n \nvdots  vdots  ddots  vdots \nx_ncdot x_1  x_ncdot x_2  cdots  x_ncdot x_n \nendpmatrix","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"我们查看一下 G 的值来看一下这个图片：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(Image: )","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"可以看到：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"输出似乎遵循以 0 为中心的高斯分布。\n得到的图片是有噪声的。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"前者解释了后者，因为数据的高斯分布越多，就越难将其与高斯噪声区分开来。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"这对我们的神经网络来说是个问题。此外，CNN 在处理稀疏数据方面表现更好（CNN work better with sparse data）已经得到证实。","category":"page"},{"location":"AI/FE/#.3.-非稀疏性","page":"特征工程","title":"2.3. 非稀疏性","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"高斯分布并不奇怪。当看三维内积值 z 的图像，对所有 (x y)R^² 的可能的组合,我们得到一个点积的三位表面：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(Image: )","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"假设时间序列的值服从均匀分布 [-1,1]，则矩阵的值服从高斯分布。下面是长度为 n 的不同时间序列的 Gram 矩阵值输出的直方图：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(Image: )","category":"page"},{"location":"AI/FE/#.-开始编码","page":"特征工程","title":"3. 开始编码","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"由于单变量时间序列是一维的，点积不能区分有价值的信息和高斯噪声，除了改变空间，没有其他利用「角」关系的方法。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"因此，在使用类似于 Gram 矩阵的构造之前，我们必须将时间序列编码为至少二维的空间。为此，我们将在一维时间序列和二维空间之间构造一个双射映射，这样就不会丢失任何信息。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"这种编码很大程度上是受到极坐标转换的启发，但是在这种情况下，半径坐标表示时间。","category":"page"},{"location":"AI/FE/#.1.-缩放序列","page":"特征工程","title":"3.1. 缩放序列","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"第一步：用 Min-Max scaler 把序列缩放到 [-1,1] 上","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"我们的过程与上面实现方式中类似。加上 Min-Max scaler，我们的极坐标编码将是双射的，使用 arccos函数双射（参见下一步)。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"第二步：将缩放后的时间序列转换到「极坐标」","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"需要考虑两个量，时间序列的值及其对应的时间戳。这两个变量分别用角度和半径表示。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(Image: )","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"假设我们的时间序列由 N 个时间戳 t 和对应的 x 组成，那么:","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"角度是用 arccos(x) 计算的，值在 0pi 之间。\n首先计算半径变量，我们把区间 [0,1] 分成 N 等份。因此，我们得到 N+1 个分隔点 0cdots1 。然后我们丢弃 0，并连续地将这些点与时间序列关联起来。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"数学定义为：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"begincases\nphi_i=arccos(x_i) \nr_i=fraciN\nendcases","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"这些编码有几个优点：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"整个编码是双射的（作为双射函数的组合）。\n它通过 r 坐标保持时间依赖性。这个优点很有用。","category":"page"},{"location":"AI/FE/#.-时间序列的内积","page":"特征工程","title":"4. 时间序列的内积","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"在二维空间中，接下来的问题是我们如何使用内积运算来处理稀疏性。","category":"page"},{"location":"AI/FE/#.1.-为什么不是极坐标编码值的内积呢？","page":"特征工程","title":"4.1. 为什么不是极坐标编码值的内积呢？","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"二维极坐标空间的内积有几个限制，因为每个向量的范数都根据时间依赖性进行了调整。更准确地说应该是：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"两个截然不同的观察结果之间的内积将偏向于最近的一个（因为范数随时间增加）；\n当计算观测值与自身的内积时，得到的范数也是有偏差的。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"因此，如果存在一个像这样的内积运算，它应该只依赖于角度。","category":"page"},{"location":"AI/FE/#.2.-使用角度","page":"特征工程","title":"4.2. 使用角度","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"由于任何类似于内积的操作都不可避免地将两个不同观测值的信息转换成一个值，所以我们不能同时保留两个角度给出的信息。我们必须做出一些让步。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"为了最好地从两个角度解释个体和连接信息，作者定义了内积的另一种操作：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"xoplus y=cos(theta_1+theta_2)","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"其中 theta 表示 x 和 y 的角度。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"注意：我选择了不同的符号而不是使用内积，因为这个操作不满足内积的要求（线性，正定）。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"这就产生了如下的类 Gram 矩阵: G=beginpmatrix   cos(phi_1+phi_1)  cos(phi_1+phi_2)  cdots  cos(phi_1+phi_n)    cos(phi_2+phi_1)  cos(phi_2+phi_2)  cdots  cos(phi_2+phi_n)   vdots  vdots  ddots  vdots \n cos(phi_n+phi_1)  cos(phi_n+phi_2)  cdots  cos(phi_n+phi_n)   endpmatrix","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"作者的选择这样做的动机是：相对于笛卡尔坐标，极坐标保留绝对的时间关系。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"优势","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"对角线由缩放后的时间序列的原始值构成（我们将根据深度神经网络学习到的高层特征来近似重构时间序列）；\n时间相关性是通过时间间隔 k 的方向叠加，用相对相关性来解释的。","category":"page"},{"location":"AI/FE/#.3.-稀疏表示","page":"特征工程","title":"4.3. 稀疏表示","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"现在我们来画出格拉姆角场（Gramian Angular Field）的值的密度分布：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(Image: )","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"从上图中我们可以看出，格拉姆角场要稀疏得多。为了解释这一点,让我们用 uoplus v 在笛卡尔坐标重新表示：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"beginmatrix \ncos(theta_1+theta_2)  =  cos(arccos(x)+arccos(y)) \n =  cos(acrccos(x))cdot cos(arccos(y))-sin(arccos(x))cdot sin(arccos(y)) \n =  xcdot y+sqrt1-x^2cdot sqrt1-y^2 \n =  langle xy rangle-sqrt1-x^2cdot sqrt1-y^2\nendmatrix","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"我们在上一项中注意到，新构造的运算对应于传统内积的惩罚版本：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"xoplus y=xcdot y-rangle-sqrt1-x^2cdot sqrt1-y^2","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"为了了解一下这种惩罚的作用。让我们先来看看整个操作的 3D 图：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"(Image: )","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"可以看到：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"惩罚将平均输出移向 -1；\nx\n和 y 越接近0，惩罚越大。主要的原因是，这些点点更接近高斯噪声；\n对于 x=y：会转换为 -1；\n输出很容易与高斯噪声区分开。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"缺点","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"主对角线,然而,生成的 GAM 很大是由于 nmapsto n^2，而原始时间序列的长度为 n。作者建议通过使用分段聚合近似（Piecewise Aggregation Approximation）减少大小。\n这个操作不是真正意义上的内积。","category":"page"},{"location":"AI/FE/#.-代码","page":"特征工程","title":"5. 代码","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"我自己利用 pyts 包写了一个可以使用的 demo，放在自己的 GitHub 上，详情可以参考另一篇博文 —— Python：使用 pyts 把一维时间序列转换成二维图片","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"总结：这篇博文的灵感主要来自王志光和 Tim 的一篇详细的论文，他们利用平铺的卷积神经网络将时间序列编码为图像进行视觉检查和分类。论文中还提到了另一种有趣的编码技术：马尔科夫转换域。","category":"page"},{"location":"AI/FE/#「类别型特征」的编码方式","page":"特征工程","title":"「类别型特征」的编码方式","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"序号编码（Ordinal Encoding）：通常用于处理类别间具有大小关系的数据，转换后依然保留相对的大小关系。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"独热编码（One-hot Encoding）：通常用于处理类别间不具有大小关系的特征。对于类别取值较多的情况下使用 One-hot Encoding 需要注意以下问题：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"使用稀疏向量来节省空间。\n配合特征选择来降低维度。高维度特征会带来几方面的问题：\n在 K 近邻算法中，高维度空间下两点之间的距离很难得到有效的衡量；\n在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合的问题；\n通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度；","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"二进制编码（Binary Encoding）：先用序号编码给每个类别赋予一个类别 ID，然后将类别 ID 对应的二进制编码作为结果。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"Helmert Contrast","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"Sum Contrast","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"Polynomial Contrast","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"Backward Difference Contrast","category":"page"},{"location":"AI/FE/#组合特征","page":"特征工程","title":"组合特征","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。在实际问题中，需要面对多种高维特征，简单地两两组合，依然容易存在参数过多、过拟合等问题。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"怎样有效地找到组合特征？可以利用决策树来寻找特征组合方式。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"例如，影视推荐问题有两个低阶特征「语言」和「类型」，其中有语言分为中文和英文，类型分为电影和电视剧，那么这两个特征的高阶组合特征有（中文，电影）、（英文，电视剧）、（英文，电影）、（中文，电视剧）四种。下表的数据，就可以变为新的数据：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"是否点击 语言 类型\n0 中文 电影\n1 英文 电影\n1 中文 电视剧\n0 英文 电视剧","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"是否点击 语言 = 中文，类型 = 电影 语言 = 英文，类型 = 电影 语言 = 中文，类型 = 电视剧 语言 = 英文，类型 = 电视剧\n0 1 0 0 0\n1 0 1 0 0\n1 0 0 0 1\n0 0 0 0 1","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"以逻辑回归为例，假设数据的特征向量为 X=(x_1x_2dotsx_k)，则有：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"Y=textsigmoid(sum_isum_jw_ijlangle x_ix_jrangle)","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"其中 langle x_ix_jrangle 表示 x_i 和 x_j 的组合特征，w_ij 的维度等于第 i 和第 j 个特征不同取值的个数。在上例中，「语言」这个特征有中文和英文两个选择，「类型」这个特征有电影和电视剧两个选择，那么 w_ij 的维度就为 2times 2=4. 当组合之前的两个特征不同取值的个数都不大时，用这种方式不会有太大的问题。但是对于某些问题，有用户 ID 和物品 ID，而用户和物品的数量动辄几千万，几千万乘几千万 mtimes n，这么大的参数量，无法进行学习。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"对于这种「高维组合特征」要如何处理？假设用户和物品的数量分别为 m 和 n，一种行之有效的方法是将两个特征分别用 k 维的低维向量表示（kll mkll n），这样原本 mtimes n 的学习参数就降低为 mtimes k + ntimes k，这其实等价于推荐算法中的矩阵分解。","category":"page"},{"location":"AI/FE/#文本表示模型","page":"特征工程","title":"文本表示模型","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"最基础的文本表示模型是词袋模型，就是将每篇文章看成一袋子词，并忽略每个词出现的顺序。每篇文章可以表示成一个长向量，向量中的每一维度代表一个单词，而该维对应的权重则反映了这个词在原文章中的重要程度。常用 TF-IDF（Term Frequency-Inverse Document Frequency）来计算权重：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"textTF-IDF(td)=textTF(td)times textIDF(t)","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"其中 textTF(td) 为单词 t 在文档 d 中出现的频率，textIDF(t) 是逆文档频率，用来衡量单词 t 对表达语义所起的重要性，表示为：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"textIDF(t) = logfractextNum of articlestextNum of articles containing word t","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"}+1}}$","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"直观解释为，如果一个单词在非常多的文章里面都出现，那么它可能是一个比较通用的词汇，对于区分谋篇文章特殊语义的贡献比较小，因此对权重做一定惩罚。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"有的时候，多个不同的单词组合起来会有特殊的含义，比如 natural language processing 组合起来就有「自然语言处理」的意思，但是把这三个单词拆开，就没有组合起来的特别。将类似这样的连续出现的 n 个词（nle N）组成的词组（N-gram）也作为一个单独的特征放到向量表示中去，构成 N-gram 模型。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"词干抽取（Word Stemming），同一个词可能有多种词性变化，却有相似的含义。在实际应用中，一般会对单词进行词干抽取，即将不同词性的单词统一成为同一词干的形式。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"主题模型用于从文本库中发现有代表性的主题（得到每个主题上面词的分布特性）。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"词嵌入是一类将词向量化的模型的统称，将每个词都映射成低维空间上的一个稠密向量（Dense Vector），通常维度 K=50sim 300。词嵌入将每个词映射成一个 K 维向量，如果一篇文章有 N 个词，就可以用一个 Ntimes K 的矩阵来表示这篇文章。但是这样的表示仅仅只是底层的表示，在实际应用中，如果仅仅把这个矩阵作为原文本的表示特征输入到机器学习模型当中，很难得到令人满意的结果。","category":"page"},{"location":"AI/RS/#大规模分段线性模型（LS-PLM）","page":"-","title":"大规模分段线性模型（LS-PLM）","text":"","category":"section"},{"location":"AI/RS/","page":"-","title":"-","text":"早在 2012 年，大规模分段线性模型（Large Scale Piece-wise Linear Model）就是阿里巴巴的主流推荐模型，又被称为混合逻辑回归（Mixed Logistics Regression），可以看作在逻辑回归的基础上采用分而治之的思路，先对样本进行分片，再在样本分片中应用逻辑回归进行 CTR（Click Through Rate，点击率）预估。","category":"page"},{"location":"AI/RS/#Embedding-技术","page":"-","title":"Embedding 技术","text":"","category":"section"},{"location":"AI/RS/","page":"-","title":"-","text":"Embedding，中文译为「嵌入」，常被翻译为「向量化」或者「向量映射」。形式上讲，Embedding 就是用一个低维稠密的向量「表示」一个对象，可以是词、商品、电影。","category":"page"},{"location":"AI/RS/","page":"-","title":"-","text":"","category":"page"},{"location":"AI/RS/","page":"-","title":"-","text":"参考","category":"page"},{"location":"AI/RS/","page":"-","title":"-","text":"[1] 王喆，《深度学习推荐系统》2020","category":"page"},{"location":"AI/ML/#验证方法","page":"机器学习","title":"验证方法","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"Holdout 检验","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"交叉检验","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"自助法（Bootstrap）：有放回地从 N 个样本中抽样 n 个样本。当样本规模比较小的时候，将样本集进行划分会让训练集进一步减小，这可能会影响模型训练效果。自助法是基于自助采样的检验方法。在 n 次采样过程中，有的样本会被重复采样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集，进行模型验证，这就是自助法的验证过程。","category":"page"},{"location":"AI/ML/#优化算法","page":"机器学习","title":"优化算法","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"随机梯度下降法本质上是采用迭代方式更新参数，每次迭代在当前位置的基础上，沿着某一方向迈一小步抵达下一位置，不断地重复这个步骤，它的更新公式为","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"theta_t+1=theta_t - eta g_t","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中 eta 是学习率。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"动量（Momentum）方法：类比中学物理知识，当前梯度就好比当前时刻受力产生的加速度，前一次步长 v_t-1 好比前一时刻的速度，当前步长 v_t 好比当前加速度共同作用的结果。这就好比小球有了惯性，而刻画惯性的物理量是动量。模型参数的迭代公式为：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"v_t = gamma v_t-1 + eta g_t","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"theta_t+1 = theta_t - v_t","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"在这里当前更新步长 v_t 直接依赖于前一次步长 v_t-1 和当前梯度 g_t，衰减系数 gamma 扮演了阻力的作用。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"AdaGrad 方法：在应用中，我们希望更新频率低的参数可以拥有较大的更新步幅，而更新频率高的参数的步幅可以减小，AdaGrad 方法采用「历史梯度平方和」来衡量不同参数的梯度的稀疏性，取值越小表明越稀疏。AdaGrad 借鉴了 mathscrl_2 正则化的思想，每次迭代时自适应地调整每个参数的学习率。这样的方式保证了不同的参数有具有自适应学习率。具体的更新公式表示为：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"在第 t 次迭代时，先计算每个参数梯度平方的累计值","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"G_t = sum_tau=1^t boldsymbolg_tau odot boldsymbolg_tau \n\n其中 odot","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"为按元素乘积，boldsymbolg_tauin mathbbR^theta 是第 tau 次迭代时的梯度。参数更新差值为","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"Deltatheta_t=-fracetasqrtG_t+epsilonodotboldsymbolg_t","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中 alpha 是初始学习率，epsilon 是为了保持数值稳定性而设定的非常小的常数，一般取值为 e^-7sim e^-10。分母中求和的形式实现了退火过程，意味着随着时间推移，学习速率 fracetasqrtG_t+epsilon 越来越小，保证算法的最终收敛。在 AdaGrad 算法中，如果某个参数的偏导数积累比较大，其学习率相对较小；相反如果其偏导数积累较小，其学习率相对较大，但整体是随着迭代次数的增加，学习率逐渐变小。","category":"page"},{"location":"AI/ML/#Adam-算法","page":"机器学习","title":"Adam 算法","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"Adam 算法的全称是自适应动量估计算法（Adaptive Moment Estimation Algorithm），它将惯性保持和自适应两个优点结合，可以看作是动量法和 RMSprop 算法（或者 AdaGrad 算法）的结合。它一方面记录梯度的一阶矩（First Moment）M_t，即过往梯度与当前梯度的平均，理解为「惯性」，是梯度 boldsymbolg_t 的指数加权平均；另一方面记录梯度的二阶矩（Second Moment）G_t，即过往梯度平方与当前梯度平方的平均，理解为「自适应部分」，是梯度 boldsymbolg_t^2 的指数加权平均。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"一阶矩可以理解为均值；二阶矩可以理解为未减去均值的方差","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"M_t = beta_1 M_t-1 + (1 - beta_1)boldsymbolg_t","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"G_t = beta_2 G_t-1 + (1 - beta_2)boldsymbolg_todotboldsymbolg_t","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中 beta_1 和 beta_2 分别为两个移动平均的衰减率，通常取值为 beta_1 = 09, beta_2 = 099. ","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"Adam 算法考虑了 M_t G_t 在零初始情况下的偏置矫正。假设 M_0=0 G_0=0，那么在迭代初期 M_t 和 G_t 的值会比真实的均值和方差要小，特别是当 beta_1 和 beta_2 都接近于 1 时，偏差会很大. 具体来说，Adam 算法的更新公式为：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"hatM_t = fracM_t1 - beta_1^t","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"hatG_t = fracG_t1 - beta_2^t","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"Deltatheta_t = -fracetasqrthatG_t + epsilon hatM_t","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中学习率 alpha 通常设为 0.001，并且也可以进行衰减，比如 alpha_t=alpha_0sqrtt. ","category":"page"},{"location":"AI/ML/#集成学习","page":"机器学习","title":"集成学习","text":"","category":"section"},{"location":"AI/ML/#Boosting-与-Bagging","page":"机器学习","title":"Boosting 与 Bagging","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"机器学习问题的两种策略：一种是研发人员尝试各种模型，选择其中表现最好的模型，做重点调参优化；另一种是将多个分类器的结果统一成一个最终的决策，其中每个单独的分类器称为基分类器，使用这类策略的机器学习方法统称为集成学习。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"集成学习分为 Boosting 和 Bagging 两种。Boosting 方法训练基分类器时采用串行方式，各个基分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。Bagging 与 Boosting 的串行训练方式不同，Bagging 方法在训练过程中，各基分类器之间无强依赖，可以进行并行训练。最著名的算法之一就是基于决策树基分类器的随机森林（Random Forest）。Bagging 方法更像是一个集体决策的过程，每个个体都进行单独学习，在最终做决策时，每个个体单独做出判断，再通过投票的方式做出最后的集体决策。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"基分类器，有时候又被称为弱分类器。基分类器的错误，是偏差和方差两种错误之和。偏差主要是由于分类器的表达能力有限导致的系统性错误，表现在训练误差不收敛，方差是由于分类器对于样本分布过于敏感，导致在训练样本数较少时，产生过拟合。而 Boosting 方法通过逐步聚焦于基分类器分错的样本，减小集成分类器的偏差。Bagging 方法则是采取分而治之的策略，通过对训练样本多次采样，并分别训练出多个不同模型，然后做综合，来减小集成分类器的方差。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"最常用的基分类器是决策树：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"决策树可以较为方便地将样本的权重整合到训练过程当中，而不需要使用过采样的方法来调整样本权重；\n决策树的表达能力和泛化能力，可以通过调节树的层数来做折中；\n数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性较大，这样的「不稳定学习期」更适合作为基分类器。（在这个点上，神经网络也因为不稳定性而适合作为基分类器，可以通过调节神经元数量、连接方式、网络层数、初始权值等方式引入随机性）；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"集成学习的基本步骤。集成学习一般可以分为以下 3 个步骤：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（1）找到误差互相独立的基分类器；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（2）训练基分类器；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（3）合并基分类器的结果；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"合并基分类器的方法有 voting 和 stacking 两种，前者对应 Bagging 方法，后者对应 Boosting 方法。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"以 Adaboost 为例，其基分类器的训练和合并的基本步骤如下：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（1）确定基分类器：可以选择 ID3 决策树作为基分类器。虽然任何分类模型都可以作为基分类器，但树形模型由于结构简单且较为容易产生随机性所以比较常用。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（2）训练基分类器：假设训练集为 x_iy_ii=1dotsN，其中 y_iin-11，并且有 T 个基分类器，则可以按照如下过程来训练基分类器：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"初始化采样分布 D_1(i)=1N；\n令 t=12dotsT 循环：\n从训练集中，按照 D_t 分布，采样出子集 S_i=x_iy_ii=1dotsN；\n用 S_i 训练出基分类器 h_t；\n计算基分类器 h_t 的错误率：\nvarepsilon_t=fracsum_i=1^N_tIh_t(x_i)neq y_iD_i(x_i)N_t\n其中 Icdot 为判别函数；\n计算基分类器 h_t 权重 a_t=logfrac(1-varepsilon_t)varepsilon_t，这里可以看到错误率 varepsilon_t 越大，基分类器的权重 a_t 就越小；\n设置下一次采样：\nD_t+1=begincasesD_t(i) text or  fracD_t(i)(1-varepsilon_t)varepsilon_t  h_t(x_i)neq y_i\nfracD_t(i)varepsilon_t(1-varepsilon_t)  h_t(x_i)= y_iendcases","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（3）合并基分类器：给定一个未知样本 z，输出分类结果为加权投票的结果 textSign(sum_t=1^Th_t(z)a_t).","category":"page"},{"location":"AI/ML/#梯度提升决策树（GBDT）","page":"机器学习","title":"梯度提升决策树（GBDT）","text":"","category":"section"},{"location":"AI/ML/#XGBoost","page":"机器学习","title":"XGBoost","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"XGBoost 是陈天奇等人开发的一个开源机器学习项目，高效地实现了 GBDT 算法并进行了算法和工程上的许多改进，被广泛应用在 Kaggle 竞赛以及其他许多机器学习竞赛中。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"XGBoost 本质上还是一个 GBDT（Gradient Boosting Decision Tree），只是把速度和效率发挥到极致，所以前面加上了 X（代表 Extreme）。原始的 GBDT 算法基于经验损失函数的负梯度来构造新的决策树，只是在决策树构建完成后再进行剪枝。XGBoost 再决策树构建阶段就加入了正则项，即","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"L_t=sum_i lleft(y_i F_t-1(x_i)+f_t(x_i)right)+Omega(f_t)","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中 F_t-1(x_i) 表示现有的 t-1 棵树最优解，树结构的正则项定义为","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"Omega(f_t)=gamma T+frac12lambdasum_j=1^Tw^2_j","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中 T 为叶子节点个数，w_j 表示第 j 个叶子节点的预测值。对该损失函数在 F_t-1 处进行二阶泰勒展开可以推导出","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"L_tapproxoversetsimL_t=sum_j=1^TleftG_jw_j+frac12(H_j+lambda)w^2_jright+gamma T","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"从所有的树结构中寻找最优的树结构是一个 NP-hard 问题，在实际中往往采用贪心法来构建出一个次优的树结构，基本思想是根据特定的准则选取最优的分裂。不同的决策树算法采用不同的准则，如 IC3 算法采用信息增益，C4.5 算法为了克服信息增益中容易偏向取值较多的特征而采用信息增益比，CART 算法使用基尼指数和平方误差，XGBoost 也有特定的准则来选取最优分裂。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"XGBoost 与 GBDT 的区别和联系：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（1）GBDT 是机器学习算法，XGBoost 是该算法的工程实现；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（2）在使用 CART 作为基分类器时，XGBoost 显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（3）GBDT 在模型训练时只使用了代价函数的一阶导数信息，XGBoost 对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（4）传统的 GBDT 采用 CART 作为基分类器，XGBoost 支持多种类型的基分类器，比如线性分类器；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（5）传统的 GBDT 在每轮迭代时使用全部的数据，XGBoost 则采用了与随机森林相似的策略，支持对数据进行采样；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（6）传统的 GBDT 没有设计对缺失值进行处理，XGBoost 能够自动学习出缺失值的处理策略；","category":"page"},{"location":"AI/ML/#XGBoost-的并行化","page":"机器学习","title":"XGBoost 的并行化","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"boosting 是一种串行结构，它的并行不是在 tree 粒度上的，而是在特征粒度上的并行。决策树学习最耗时的一个步骤就是对特征的值进行排序（为了确定最佳分割点）。XGBoost 训练之前，预先对数据进行排序，保存为 block 结构，后面的迭代中重复地使用这个结构，大大减小计算量。","category":"page"},{"location":"AI/ML/#XGBoost-的特点","page":"机器学习","title":"XGBoost 的特点","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"传统的 GBDT 以 CART 作为基函数，而 XGBoost 相当于有 L1/L2 正则化项的分类或者回归\n传统的 GBDT 在优化的时候只用到一阶导数，XGBoost 对代价函数进行了二阶泰勒展开，同时用到一阶和二阶导数。并且 XGBoost 工具支持自定义代价函数，只要函数可以一阶和二阶求导；\nXGBoost 在代价函数里加入了正则项，控制模型复杂度。正则项里包含了树的叶节点个数、每个叶子节点上输出 score 的 L2 模的平方和。从 Bias-variance tradeoff 角度来讲，正则项降低了模型 variance，使学习出来的模型更加简单，防止过拟合，这也是 XGBoost 优于传统 GBDT 的一个特性。 剪枝是都有的，叶子节点输出 L2 平滑是新增的；\nshrinkage 缩减和 column subsampling。shrinkage 缩减：类似于学习速率，在每一步 tree boosting 之后增加了一个参数 n（权重），通过这种方式来减小每棵树的影响力，给后面的树提供空间去优化模型。column subsampling：列（特征）抽样，随机森林那边学习来的，防止过拟合的效果比传统的行抽样还好（行抽样功能也有），并且有利于后面提到的并行化处理算法；\nsplit finding algorithms（划分点查找算法），树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法 greedy algorithm 枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以 XGBoost 还提出了一种可并行的近似直方图算法（Weighted Quantile Sketch），用于高效地生成候选的分割点；\n对缺失值的处理。对于特征的值有缺失的样本，XGBoost 可以自动学习出它的分裂方向。 稀疏感知算法 Sparsity-aware Split Finding；\n内置交叉验证（Built-in Cross-Validation），XGBoost 可以在 boosting 过程的每次迭代中运行交叉验证，因此很容易在一次运行中获得准确的最佳 boosting 迭代次数；\nXGBoost 支持并行，提高计算速度；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"参考","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"[1] GitHub 项目：ML-NLP；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"[2] XGBoost 特点、调参、讨论；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"[3] 诸葛越，葫芦娃，《百面机器学习》，中国工信出版集团，人民邮电出版社","category":"page"},{"location":"#Docs","page":"Home","title":"9Docs","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"参考：","category":"page"},{"location":"","page":"Home","title":"Home","text":"[1] 邱锡鹏《神经网络与深度学习》机械工业出版社（2020）","category":"page"},{"location":"","page":"Home","title":"Home","text":"[2] 诸葛越，葫芦娃《百面机器学习》中国工信出版集团，人民邮电出版社","category":"page"},{"location":"AI/GAN/#生成对抗网络","page":"生成对抗网络","title":"生成对抗网络","text":"","category":"section"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"2014 年，加拿大蒙特利尔大学的 Ian Goodfellow 和他的导师 Yoshua Bengio 提出生成对抗网络（Generative Adversarial Networks, GANs）。在 GANs 被提出来之后，发展迅速，出现了各种变种网络，包括 WGAN、InfoGAN、f-GANs、BiGAN、DCGAN、IRGAN 等。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"对于 GANs 的理解，可以想象成假币者与警察间展开的一场猫捉老鼠游戏，造假币者试图造出以假乱真的假币，警察试图发现这些假币，对抗使得二者的水平都得到提高。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"GANs 包括生成器（Generator）和判别器（Discriminator）两个部分。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（1）生成器的作用是合成「假」样本。它从先验分布中采样随机信号，通过神经网络得到模拟样本。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（2）判别器的作用是判断输入的样本是真实的还是合成的。它同时接收来自生成器的模拟样本和实际数据集的真实样本，并且判断当前接收的样本是「真」还是「假」。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"GANs 实际上是一个二分类问题，判别器 D 试图识别实际数据为真实样本，识别生成器生成的数据为模拟样本。它的损失函数写成负对数似然 （Negative Log-Likelihood），也称为 Categorical Cross-Entropy Loss，即：","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"mathcalL(D) = -int p(x) left p(data mid x) log D(x) + p(g mid x) log(1-D(x))  righttextdxqquad text(1)","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"其中 D(x) 表示判别器预测 x 为真实样本的概率，p(data mid x) 和 p(g mid x) 表示 x 分属真实数据集和生成器这两类的概率。即理解为，在给定样本 x 的条件下，该样本来自真实数据集 data 的概率和来自生成器的概率。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"样本 x 的来源应该各占实际数据集和生成器一半，即 p_textsrc(data)=p_textsrc(g)= 05。用 p_textdata(x)doteq p(xmid data) 表示从实际数据集得到 x 的概率，p_textg(x)doteq p(xmid g) 表示从生成器得到 x 的概率，有 x 的总概率：","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"p(x) = p_textsrc(data)p(xmid data) + p_textsrc(g)p(xmid g)","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"doteq和 approx 是等价的，都是表达约等于的意思。一般写完等号之后，发现不是等于，而是约等于，所以就懒得涂抹写成 approx，所以就添加一个点。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"将损失函数 (1) 式中的 p(x)p(datamid x) 替换为 p_textsrc(data)p_textdata(x)，以及将 p(x)p(gmid x) 替换为 p_textsrc(g)p_textg(x)，就可以得到最终的目标函数","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"mathcalD=-frac12left( mathbbE_xsim p_textdata(x)left log D(x) right + mathbbE_xsim p_textg(x)left log (1 - D(x)) rightright)","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"在此基础上可以得到值函数","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"V(GD) = mathbbE_xsim p_textdata(x)left log D(x) right + mathbbE_xsim p_textg(x)left log (1 - D(x)) right","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"在训练的时候，判别器 D 的目标就是最大化上述值函数，生成器 G 的目标就是最小化它，因此整个 MinMax 问题可以表示为 undersetGminundersetDmax V(GD)。","category":"page"},{"location":"AI/GAN/#GANs-的训练方式","page":"生成对抗网络","title":"GANs 的训练方式","text":"","category":"section"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"我们知道 GANs 的值函数为","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"V(GD) = mathbbE_xsim p_textdata(x)left log D(x) right + mathbbE_xsim p_textg(x)left log (1 - D(x)) right","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"在训练的时候，判别器 D 的目标就是最大化上述值函数，生成器 G 的目标就是最小化它，因此整个 MinMax 问题可以表示为 undersetGminundersetDmax V(GD)。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"GANs 在训练的时候是采用生成器和判别器交替优化的方式进行的。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"判别器 D 的训练：","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（1）先固定生成器 G(cdot)；","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（2）利用生成器随机模拟产生样本 G(z) 作为负样本（z 是一个随机向量），并从真实数据集中采样获得正样本 X；","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（3）将正负样本输入到判别器 D(cdot) 中，根据判别器的输出 D(X) 和 D(G(z)) 和样本标签来计算误差；","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（4）最后利用误差反向传播算法来更新判别器 D(cdot) 的参数；","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"判别器的训练是这样的一个问题：给定生成器 G，寻找当前情况下的最优判别器 D^*_G 。对于单个样本 x，最大化 undersetDmax p_textdata(x)log D(x) + p_textg(x)log(1-D(x)) 的解为 hatD(x)=p_textdata(x)p_textdata(x)+p_textg(x)，外面套上对 x 的积分就得到 undersetDmax V(GD)，解由单点变成一个函数解：","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"D^*_G=fracp_textdatap_textdata+p_textg","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"此时 undersetGminV(GD^*_G)=undersetGminleft-log 4 + 2cdot textJSD(p_textdata p_textg)right，其中 textJSD(cdot) 是 JS 距离。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"优化生成器 G 实际上是在最小化生成样本分布与真实样本分布的 JS 距离。最终达到的均衡点是 textJSD(p_textdata p_textg) 的最小值点，即 p_textg=p_textdata 时，textJSD(p_textdata p_textg) 取到零，最优解 G^*(z)=xsim p_textdata(x)，D^*(x)equiv frac12，值函数 V(G^*D^*)=-log 4。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"生成器 G 的训练：","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（1）先固定判别器 D(cdot)；","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（2）然后利用当前生成器 G(cdot) 随机模拟产生样本 G(z)，输入到判别器 G(cdot) 中；","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（3）根据判别器的输出 D(G(z)) 和样本标签来计算误差；","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"（4）最后利用误差反向传播算法来更新生成器 G(cdot) 的参数；","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"假设 G^prime 表示前一步的生成器，D 是 G^prime 下的最优判别器 D^*_G^prime。那么求解最优生成器 G 的过程为：","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"undersetGargminV(GD^*_G^prime)=undersetGargmintextKLleft( p_textg  fracp_textdata+p_textg^prime2 right) - textKL(P_textg P_textg^prime)","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"由此可以知道（1）优化 G 的过程是让 G 远离前一步的 G^prime，同时接近分布 (p_textdata+p_textg^prime)2；（2）达到均衡点时 p_textg^prime=p_textdata，有 undersetGargminV(GD^*_G^prime)=undersetGargmin()，如果用这时的判别器去训练一个全新的生成器 G_textnew，理论上可能啥也训练不出来。","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"参考：","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"[1] 诸葛越，葫芦娃，《百面机器学习》，中国工信出版集团，人民邮电出版社","category":"page"},{"location":"AI/GAN/","page":"生成对抗网络","title":"生成对抗网络","text":"[2] Goodfellow I. J., Pouget-Abadie J., Mirza M., et al. Generative adversarial networks[J]. Advances in Neural Information Processing Systems, 2014, 3: 2672-2680. ","category":"page"}]
}
