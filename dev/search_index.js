var documenterSearchIndex = {"docs":
[{"location":"AI/RNN/#循环神经网络","page":"循环神经网络","title":"循环神经网络","text":"","category":"section"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"记录网络的输入序列为 x_1x_2cdotsx_n，一个循环神经网络（RNN）展开后可以看做一个 n 层的前馈神经网络，第 t 层对应着 t 时刻的状态（t=12cdotsn），记第 t 层（时刻）的输入状态、隐藏状态、输出状态分别为 x_t h_t o_t，训练时的目标输出值为 y_t，则有：隐藏状态 h_t 由当前时刻的输入状态 x_t 和上一时刻的隐藏状态 h_t-1 共同确定，即","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"h_t=sigma(Ux_t+Wh_t-1+b)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中，U 是输入层到隐藏层的权重矩阵，W 是不同时刻的隐藏层之间的连接权重，b 是偏置向量，sigma(cdot) 是激活函数（通常使用 textttTanh 函数）。循环神经网络最大的特点就是当前时刻的隐藏状态不仅与当前时刻的输入状态有关，还受上一时刻的隐藏状态影响。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"输出状态 o_t 的计算公式为：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"o_t=g(Vh_t+c)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中，V 是隐藏层到输出层的权重矩阵，c 是偏置向量，g(cdot) 是输出层的激活函数（对于分类任务可以采用 textttSoftmax 函数）。在训练时，网络在整个序列上的损失可以定义为不同时刻的损失之和：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"mathcalL=sum_tmathcalL_t=sum_t Loss(o_ty_t)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"上述的权重矩阵 U W V 是所有时刻共享参数的，这种机制不仅可以极大地减少网络需要学习的参数数量，而且使得网络可以处理长度不固定的输入序列。在 RNN 的训练过程中，由于不同时刻的状态是相互依赖的，因此需要存储各个时刻的状态信息，而且无法进行并行计算，这导致整个训练过程内存消耗大，并且速度较慢。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"RNN 之所以能够在序列数据的处理上获得出色的表现，是因为它拥有长期记忆功能，能够压缩并获得长期数据的表示。实际上，在 RNN 训练过程中，为了防止梯度爆炸（或弥散）的问题，通常采用带截断的反向传播算法，即仅反向传播 k 个时间步的梯度。理论上的无限记忆优势在实际中几乎不存在。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"实际上，在序列任务中，卷积神经网络（CNN）在空洞卷积的帮助下（例如 TextCNN），具有更好的并行化和可训练性。只不过长短期记忆网络（LSTM）和 Seq2Seq 网络依然是序列数据处理中最为通用的架构。还有人很多工作对卷积神经网络和循环神经网络进行组合使用，提升序列数据处理能力（如 TrellisNet）。","category":"page"},{"location":"AI/RNN/#长短期记忆网络（LSTM）","page":"循环神经网络","title":"长短期记忆网络（LSTM）","text":"","category":"section"},{"location":"AI/RNN/#长程依赖问题","page":"循环神经网络","title":"长程依赖问题","text":"","category":"section"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"LSTM 是如何实现长短期记忆功能的？","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（1）一般的 RNN 中，只有一个隐藏状态（hidden state）单元 h_t，不同时刻隐藏状态单元的参数是相同（共享）的。LSTM 在普通 RNN 的基础上增加了一个元胞状态（cell state）单元 c_t，其在不同时刻有着可变的连接权重，可解决普通循环神经网络中的梯度消失或爆炸问题。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（2）LSTM 引入了门控单元，是神经网络学习到的用于控制信号的存储、利用和舍弃的单元。对于每一个时刻 t，LSTM 有输入门 i_t、遗忘门 f_i 和输出门 o_t 共 3 个门控单元。每个门控单元的输入包括当前时刻的序列信息 x_t 和上一时刻的隐藏状态单元 h_t-1，具体公式：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"i_t=sigmaleft( W_i x_t + U_i h_t-1 + b_i right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"f_t=sigmaleft( W_f x_t + U_f h_t-1 + b_f right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"o_t=sigmaleft( W_o x_t + U_o h_t-1 + b_o right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"3 个门控单元都相当于一个全连接层，激活函数 sigma(cdot) 的取值范围是 0 1，常用 textttSigmoid 作为激活函数。当门控单元的状态为 0 时，信号会被全部丢弃；当状态为 1 时，信号会被全部保留。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（3）元胞状态单元从上一个时刻 c_t-1 到当前时刻 c_t 的转移是由输入门和遗忘门共同控制的。输入门决定了当前时刻输入信息 tildec_t 有多少被吸收，遗忘门决定了上一时刻元胞状态单元 c_t-1 有多少不被遗忘，最终的元胞状态单元 c_t 由两个门控处理后的信号取和产生。具体公式：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"tildec_t = textttTanhleft( W_c x_t + U_c h_t-1 + b_c right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"c_t = f_t odot c_t-1 + i_t odot tildec_t","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中 odot 为逐元素点乘操作。LSTM 的隐藏状态单元 h_t 则由输出门 c_t 决定：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"h_t = o_t odot textttTanh(c_t)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"不仅隐藏状态单元 h_t-1 和 h_t 之间有着较为复杂的循环连接，内部的元胞状态单元 c_t-1 和 c_t 之间还具有线性自循环关系，这个关系可以看作是在滑动处理不同时刻的信息。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（4）LSTM 中遗忘门和输出门的激活函数十分重要。删除遗忘门的激活函数会导致之前的元胞状态不能很好地被抑制；而删除输出门的激活函数则可能会出现非常大的输出状态。","category":"page"},{"location":"AI/RNN/#门控循环单元（GRU）","page":"循环神经网络","title":"门控循环单元（GRU）","text":"","category":"section"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（1）GRU 只有两个门控单元，分别为重置门 r_t 和 更新门 z_t，一个控制短期记忆，另一个控制长期记忆。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（2）GRU 中每个门控单元的输入包括当前时刻和序列信息 x_t 和上一时刻的隐藏状态单元 h_t-1，具体计算公式为：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"r_t = sigmaleft( W_r x_t + U_r h_t-1 right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"z_t = sigmaleft( W_z x_t + U_z h_t-1 right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中 sigma(cdot) 是激活函数，一般用 textttSigmoid。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（3）GRU 中重置门决定先前的隐藏状态单元是否被忽略，而更新门则控制当前隐藏状态单元是否需要被新的隐藏状态单元更新，具体公式：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"tildeh_t = textttTanh left( W_h x_t + U_h (r_t odot h_t-1) right)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"h_t = (1 - z_t) h_t - 1 + z_t tildeh_t","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中，(1 - z_t)h_t-1 表示上一时刻保留下来（没被遗忘）的信息，z_t tildeh_t 是当前时刻记忆下来的信息。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"用 1 - z_t z_t  作为系数，表明对上一时刻遗忘多少权重的信息，就会在这一时刻记忆多少权重的信息以作为弥补。GRU 就是用这样的一种方式用一个更新门 z_t 实现遗忘和记忆两个功能。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（4）GRU 只有一个隐藏状态单元 h_t，而 LSTM 有隐藏状态单元 h_t 和元胞状态单元 c_t。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（5）GRU 具有更少的参数，更易于计算和实现。在不同数据集、不同超参配置下，可以取得与 LSTM 相当甚至更好的性能，并且具有更快地收敛速度。","category":"page"},{"location":"AI/RNN/#序列到序列（Seq2Seq）","page":"循环神经网络","title":"序列到序列（Seq2Seq）","text":"","category":"section"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（1）Seq2Seq 的映射架构能够将一个可变长序列映射到另一个可变长序列。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（2）Seq2Seq 框架由于输入序列和输出序列是不等长的因此整个处理过程需要拆分为对序列的理解和翻译，也就是编码和解码。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（3）采用一个固定尺寸的状态向量 C 作为编码器与解码器之间的「桥梁」。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（4）假设输入序列为 X=(x_1x_2cdotsx_T)，编码器可以是一个简单的循环神经网络，其隐藏状态 h_t 的计算公式为：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"h_t = f(h_t-1 x_t)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中，f(cdot) 是非线性激活函数，可以是简单的 textttSigmoid 函数，也可以是复杂的门控函数（LSTM、GRU 等）。将上述循环神经网络（编码器）最后一个时刻的隐藏状态 h_T 作为状态向量，并输入到解码器。C 是一个尺寸固定的向量，并且包含了整个序列的所有信息。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（5）解码器需要根据固定尺寸的状态向量 C 来生成长度可变的解码序列 Y=(y_1 y_2 cdots y_T)。这里解码序列的长度 T^prime 和编码长度 T 可以是不同的。解码器也可以用一个简单的循环神经网络来实现，其隐藏状态 h_t 可以按照如下公式计算：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"h_t = f(h_t-1y_t-1C)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"解码器的输出由如下公式决定：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"P(y_tmid y_t-1y_t-2cdotsy_1C) = g(h_t y_t-1C)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中，g(cdot) 会产生一个概率分布（例如用 textttSoftmax 函数产生概率分布）。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（6）解码器的工作流程：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"首先在收到一个启动信号（如 y_0=text start gt）后开始工作，根据 h_t y_t-1 C 计算出 y_t  的概率分布；\n然后对 y_t 进行采样获得具体取值；\n循环上述操作，直到遇到结束信号（如 y_t=text eos gt；","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（7）解码器的实现还能够用一种更加简单的方式，仅在初始时刻需要状态向量 C，其他时刻仅接收隐藏状态和上一时刻的输出信息 P(y_t)=g(h_ty_t-1)。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（8）在训练时，需要让模型输出的序列尽可能正确，这可以通过最大化对数似然概率来实现：","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"max_theta frac1Nsum_n=1^N log p_theta(Y_n mid X_n)","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"其中 theta 为模型参数，X_n 是一个输入序列，Y_n 是对应的输出序列， (X_nY_n) 构成一个训练样本对。","category":"page"},{"location":"AI/RNN/","page":"循环神经网络","title":"循环神经网络","text":"（9）因为是序列到序列的转换，实际应用中可以通过贪心法求解 Seq2Seq，当度量标准、评估方式确定后，解码器每次根据当前的状态和已解码的序列选择一个最佳的解码结果，直至结束。","category":"page"},{"location":"AI/FE/#「类别型特征」的编码方式","page":"特征工程","title":"「类别型特征」的编码方式","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"序号编码（Ordinal Encoding）：通常用于处理类别间具有大小关系的数据，转换后依然保留相对的大小关系。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"独热编码（One-hot Encoding）：通常用于处理类别间不具有大小关系的特征。对于类别取值较多的情况下使用 One-hot Encoding 需要注意以下问题：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"使用稀疏向量来节省空间。\n配合特征选择来降低维度。高维度特征会带来几方面的问题：\n在 K 近邻算法中，高维度空间下两点之间的距离很难得到有效的衡量；\n在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合的问题；\n通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度；","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"二进制编码（Binary Encoding）：先用序号编码给每个类别赋予一个类别 ID，然后将类别 ID 对应的二进制编码作为结果。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"Helmert Contrast","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"Sum Contrast","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"Polynomial Contrast","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"Backward Difference Contrast","category":"page"},{"location":"AI/FE/#组合特征","page":"特征工程","title":"组合特征","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。在实际问题中，需要面对多种高维特征，简单地两两组合，依然容易存在参数过多、过拟合等问题。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"怎样有效地找到组合特征？可以利用决策树来寻找特征组合方式。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"例如，影视推荐问题有两个低阶特征「语言」和「类型」，其中有语言分为中文和英文，类型分为电影和电视剧，那么这两个特征的高阶组合特征有（中文，电影）、（英文，电视剧）、（英文，电影）、（中文，电视剧）四种。下表的数据，就可以变为新的数据：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"是否点击 语言 类型\n0 中文 电影\n1 英文 电影\n1 中文 电视剧\n0 英文 电视剧","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"是否点击 语言 = 中文，类型 = 电影 语言 = 英文，类型 = 电影 语言 = 中文，类型 = 电视剧 语言 = 英文，类型 = 电视剧\n0 1 0 0 0\n1 0 1 0 0\n1 0 0 0 1\n0 0 0 0 1","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"以逻辑回归为例，假设数据的特征向量为 X=(x_1x_2dotsx_k)，则有：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"Y=textsigmoid(sum_isum_jw_ijlangle x_ix_jrangle)","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"其中 langle x_ix_jrangle 表示 x_i 和 x_j 的组合特征，w_ij 的维度等于第 i 和第 j 个特征不同取值的个数。在上例中，「语言」这个特征有中文和英文两个选择，「类型」这个特征有电影和电视剧两个选择，那么 w_ij 的维度就为 2times 2=4. 当组合之前的两个特征不同取值的个数都不大时，用这种方式不会有太大的问题。但是对于某些问题，有用户 ID 和物品 ID，而用户和物品的数量动辄几千万，几千万乘几千万 mtimes n，这么大的参数量，无法进行学习。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"对于这种「高维组合特征」要如何处理？假设用户和物品的数量分别为 m 和 n，一种行之有效的方法是将两个特征分别用 k 维的低维向量表示（kll mkll n），这样原本 mtimes n 的学习参数就降低为 mtimes k + ntimes k，这其实等价于推荐算法中的矩阵分解。","category":"page"},{"location":"AI/FE/#文本表示模型","page":"特征工程","title":"文本表示模型","text":"","category":"section"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"最基础的文本表示模型是词袋模型，就是将每篇文章看成一袋子词，并忽略每个词出现的顺序。每篇文章可以表示成一个长向量，向量中的每一维度代表一个单词，而该维对应的权重则反映了这个词在原文章中的重要程度。常用 TF-IDF（Term Frequency-Inverse Document Frequency）来计算权重：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"textTF-IDF(td)=textTF(td)times textIDF(t)","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"其中 textTF(td) 为单词 t 在文档 d 中出现的频率，textIDF(t) 是逆文档频率，用来衡量单词 t 对表达语义所起的重要性，表示为：","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"textIDF(t) = logfractextNum of articlestextNum of articles containing word t","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"}+1}}$","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"直观解释为，如果一个单词在非常多的文章里面都出现，那么它可能是一个比较通用的词汇，对于区分谋篇文章特殊语义的贡献比较小，因此对权重做一定惩罚。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"有的时候，多个不同的单词组合起来会有特殊的含义，比如 natural language processing 组合起来就有「自然语言处理」的意思，但是把这三个单词拆开，就没有组合起来的特别。将类似这样的连续出现的 n 个词（nle N）组成的词组（N-gram）也作为一个单独的特征放到向量表示中去，构成 N-gram 模型。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"词干抽取（Word Stemming），同一个词可能有多种词性变化，却有相似的含义。在实际应用中，一般会对单词进行词干抽取，即将不同词性的单词统一成为同一词干的形式。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"主题模型用于从文本库中发现有代表性的主题（得到每个主题上面词的分布特性）。","category":"page"},{"location":"AI/FE/","page":"特征工程","title":"特征工程","text":"词嵌入是一类将词向量化的模型的统称，将每个词都映射成低维空间上的一个稠密向量（Dense Vector），通常维度 K=50sim 300。词嵌入将每个词映射成一个 K 维向量，如果一篇文章有 N 个词，就可以用一个 Ntimes K 的矩阵来表示这篇文章。但是这样的表示仅仅只是底层的表示，在实际应用中，如果仅仅把这个矩阵作为原文本的表示特征输入到机器学习模型当中，很难得到令人满意的结果。","category":"page"},{"location":"AI/ML/#验证方法","page":"机器学习","title":"验证方法","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"Holdout 检验","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"交叉检验","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"自助法（Bootstrap）：有放回地从 N 个样本中抽样 n 个样本。当样本规模比较小的时候，将样本集进行划分会让训练集进一步减小，这可能会影响模型训练效果。自助法是基于自助采样的检验方法。在 n 次采样过程中，有的样本会被重复采样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集，进行模型验证，这就是自助法的验证过程。","category":"page"},{"location":"AI/ML/#集成学习","page":"机器学习","title":"集成学习","text":"","category":"section"},{"location":"AI/ML/#Boosting-与-Bagging","page":"机器学习","title":"Boosting 与 Bagging","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"机器学习问题的两种策略：一种是研发人员尝试各种模型，选择其中表现最好的模型，做重点调参优化；另一种是将多个分类器的结果统一成一个最终的决策，其中每个单独的分类器称为基分类器，使用这类策略的机器学习方法统称为集成学习。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"集成学习分为 Boosting 和 Bagging 两种。Boosting 方法训练基分类器时采用串行方式，各个基分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。Bagging 与 Boosting 的串行训练方式不同，Bagging 方法在训练过程中，各基分类器之间无强依赖，可以进行并行训练。最著名的算法之一就是基于决策树基分类器的随机森林（Random Forest）。Bagging 方法更像是一个集体决策的过程，每个个体都进行单独学习，在最终做决策时，每个个体单独做出判断，再通过投票的方式做出最后的集体决策。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"基分类器，有时候又被称为弱分类器。基分类器的错误，是偏差和方差两种错误之和。偏差主要是由于分类器的表达能力有限导致的系统性错误，表现在训练误差不收敛，方差是由于分类器对于样本分布过于敏感，导致在训练样本数较少时，产生过拟合。而 Boosting 方法通过逐步聚焦于基分类器分错的样本，减小集成分类器的偏差。Bagging 方法则是采取分而治之的策略，通过对训练样本多次采样，并分别训练出多个不同模型，然后做综合，来减小集成分类器的方差。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"最常用的基分类器是决策树：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"决策树可以较为方便地将样本的权重整合到训练过程当中，而不需要使用过采样的方法来调整样本权重；\n决策树的表达能力和泛化能力，可以通过调节树的层数来做折中；\n数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性较大，这样的「不稳定学习期」更适合作为基分类器。（在这个点上，神经网络也因为不稳定性而适合作为基分类器，可以通过调节神经元数量、连接方式、网络层数、初始权值等方式引入随机性）；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"集成学习的基本步骤。集成学习一般可以分为以下 3 个步骤：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（1）找到误差互相独立的基分类器；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（2）训练基分类器；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（3）合并基分类器的结果；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"合并基分类器的方法有 voting 和 stacking 两种，前者对应 Bagging 方法，后者对应 Boosting 方法。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"以 Adaboost 为例，其基分类器的训练和合并的基本步骤如下：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（1）确定基分类器：可以选择 ID3 决策树作为基分类器。虽然任何分类模型都可以作为基分类器，但树形模型由于结构简单且较为容易产生随机性所以比较常用。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（2）训练基分类器：假设训练集为 x_iy_ii=1dotsN，其中 y_iin-11，并且有 T 个基分类器，则可以按照如下过程来训练基分类器：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"初始化采样分布 D_1(i)=1N；\n令 t=12dotsT 循环：\n从训练集中，按照 D_t 分布，采样出子集 S_i=x_iy_ii=1dotsN；\n用 S_i 训练出基分类器 h_t；\n计算基分类器 h_t 的错误率：\nvarepsilon_t=fracsum_i=1^N_tIh_t(x_i)neq y_iD_i(x_i)N_t\n其中 Icdot 为判别函数；\n计算基分类器 h_t 权重 a_t=logfrac(1-varepsilon_t)varepsilon_t，这里可以看到错误率 varepsilon_t 越大，基分类器的权重 a_t 就越小；\n设置下一次采样：\nD_t+1=begincasesD_t(i) text or  fracD_t(i)(1-varepsilon_t)varepsilon_t  h_t(x_i)neq y_i\nfracD_t(i)varepsilon_t(1-varepsilon_t)  h_t(x_i)= y_iendcases","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（3）合并基分类器：给定一个未知样本 z，输出分类结果为加权投票的结果 textSign(sum_t=1^Th_t(z)a_t).","category":"page"},{"location":"AI/ML/#梯度提升决策树（GBDT）","page":"机器学习","title":"梯度提升决策树（GBDT）","text":"","category":"section"},{"location":"AI/ML/#XGBoost","page":"机器学习","title":"XGBoost","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"XGBoost 是陈天奇等人开发的一个开源机器学习项目，高效地实现了 GBDT 算法并进行了算法和工程上的许多改进，被广泛应用在 Kaggle 竞赛以及其他许多机器学习竞赛中。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"XGBoost 本质上还是一个 GBDT（Gradient Boosting Decision Tree），只是把速度和效率发挥到极致，所以前面加上了 X（代表 Extreme）。原始的 GBDT 算法基于经验损失函数的负梯度来构造新的决策树，只是在决策树构建完成后再进行剪枝。XGBoost 再决策树构建阶段就加入了正则项，即","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"L_t=sum_i lleft(y_i F_t-1(x_i)+f_t(x_i)right)+Omega(f_t)","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中 F_t-1(x_i) 表示现有的 t-1 棵树最优解，树结构的正则项定义为","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"Omega(f_t)=gamma T+frac12lambdasum_j=1^Tw^2_j","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"其中 T 为叶子节点个数，w_j 表示第 j 个叶子节点的预测值。对该损失函数在 F_t-1 处进行二阶泰勒展开可以推导出","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"L_tapproxoversetsimL_t=sum_j=1^TleftG_jw_j+frac12(H_j+lambda)w^2_jright+gamma T","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"从所有的树结构中寻找最优的树结构是一个 NP-hard 问题，在实际中往往采用贪心法来构建出一个次优的树结构，基本思想是根据特定的准则选取最优的分裂。不同的决策树算法采用不同的准则，如 IC3 算法采用信息增益，C4.5 算法为了克服信息增益中容易偏向取值较多的特征而采用信息增益比，CART 算法使用基尼指数和平方误差，XGBoost 也有特定的准则来选取最优分裂。","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"XGBoost 与 GBDT 的区别和联系：","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（1）GBDT 是机器学习算法，XGBoost 是该算法的工程实现；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（2）在使用 CART 作为基分类器时，XGBoost 显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（3）GBDT 在模型训练时只使用了代价函数的一阶导数信息，XGBoost 对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（4）传统的 GBDT 采用 CART 作为基分类器，XGBoost 支持多种类型的基分类器，比如线性分类器；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（5）传统的 GBDT 在每轮迭代时使用全部的数据，XGBoost 则采用了与随机森林相似的策略，支持对数据进行采样；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"（6）传统的 GBDT 没有设计对缺失值进行处理，XGBoost 能够自动学习出缺失值的处理策略；","category":"page"},{"location":"AI/ML/#XGBoost-的并行化","page":"机器学习","title":"XGBoost 的并行化","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"boosting 是一种串行结构，它的并行不是在 tree 粒度上的，而是在特征粒度上的并行。决策树学习最耗时的一个步骤就是对特征的值进行排序（为了确定最佳分割点）。XGBoost 训练之前，预先对数据进行排序，保存为 block 结构，后面的迭代中重复地使用这个结构，大大减小计算量。","category":"page"},{"location":"AI/ML/#XGBoost-的特点","page":"机器学习","title":"XGBoost 的特点","text":"","category":"section"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"传统的 GBDT 以 CART 作为基函数，而 XGBoost 相当于有 L1/L2 正则化项的分类或者回归\n传统的 GBDT 在优化的时候只用到一阶导数，XGBoost 对代价函数进行了二阶泰勒展开，同时用到一阶和二阶导数。并且 XGBoost 工具支持自定义代价函数，只要函数可以一阶和二阶求导；\nXGBoost 在代价函数里加入了正则项，控制模型复杂度。正则项里包含了树的叶节点个数、每个叶子节点上输出 score 的 L2 模的平方和。从 Bias-variance tradeoff 角度来讲，正则项降低了模型 variance，使学习出来的模型更加简单，防止过拟合，这也是 XGBoost 优于传统 GBDT 的一个特性。 剪枝是都有的，叶子节点输出 L2 平滑是新增的；\nshrinkage 缩减和 column subsampling。shrinkage 缩减：类似于学习速率，在每一步 tree boosting 之后增加了一个参数 n（权重），通过这种方式来减小每棵树的影响力，给后面的树提供空间去优化模型。column subsampling：列（特征）抽样，随机森林那边学习来的，防止过拟合的效果比传统的行抽样还好（行抽样功能也有），并且有利于后面提到的并行化处理算法；\nsplit finding algorithms（划分点查找算法），树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法 greedy algorithm 枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以 XGBoost 还提出了一种可并行的近似直方图算法（Weighted Quantile Sketch），用于高效地生成候选的分割点；\n对缺失值的处理。对于特征的值有缺失的样本，XGBoost 可以自动学习出它的分裂方向。 稀疏感知算法 Sparsity-aware Split Finding；\n内置交叉验证（Built-in Cross-Validation），XGBoost 可以在 boosting 过程的每次迭代中运行交叉验证，因此很容易在一次运行中获得准确的最佳 boosting 迭代次数；\nXGBoost 支持并行，提高计算速度；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"参考","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"[1] GitHub 项目：ML-NLP；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"[2] XGBoost 特点、调参、讨论；","category":"page"},{"location":"AI/ML/","page":"机器学习","title":"机器学习","text":"[3] 诸葛越，葫芦娃，《百面机器学习》，中国工信出版集团，人民邮电出版社","category":"page"},{"location":"AI/RS/#大规模分段线性模型（LS-PLM）","page":"-","title":"大规模分段线性模型（LS-PLM）","text":"","category":"section"},{"location":"AI/RS/","page":"-","title":"-","text":"早在 2012 年，大规模分段线性模型（Large Scale Piece-wise Linear Model）就是阿里巴巴的主流推荐模型，又被称为混合逻辑回归（Mixed Logistics Regression），可以看作在逻辑回归的基础上采用分而治之的思路，先对样本进行分片，再在样本分片中应用逻辑回归进行 CTR（Click Through Rate，点击率）预估。","category":"page"},{"location":"AI/RS/#Embedding-技术","page":"-","title":"Embedding 技术","text":"","category":"section"},{"location":"AI/RS/","page":"-","title":"-","text":"Embedding，中文译为「嵌入」，常被翻译为「向量化」或者「向量映射」。形式上讲，Embedding 就是用一个低维稠密的向量「表示」一个对象，可以是词、商品、电影。","category":"page"},{"location":"AI/RS/","page":"-","title":"-","text":"","category":"page"},{"location":"AI/RS/","page":"-","title":"-","text":"参考","category":"page"},{"location":"AI/RS/","page":"-","title":"-","text":"[1] 王喆，《深度学习推荐系统》2020","category":"page"},{"location":"AI/GNN/#通用框架","page":"图神经网络","title":"通用框架","text":"","category":"section"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"除了图神经网络的不同变体，人们还提出了一些通用框架，旨在将不同的模型集成到单一的框架中。^1","category":"page"},{"location":"AI/GNN/#消息传递神经网络","page":"图神经网络","title":"消息传递神经网络","text":"","category":"section"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"消息传递神经网络^2（MPNN, Message Passing Neural Network）包含两个阶段：消息传递阶段和读出阶段。\t","category":"page"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"","category":"page"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"参考：","category":"page"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"[1] 刘知远，周界，《图神经网络导论》","category":"page"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"[2]  J. Gilmmer, S. S. Schoenholz, P. F. Riley, et al. Neural message passing for quantum chemistry. In Proc. of ICML, 2018: 1263-1272. ","category":"page"},{"location":"AI/GNN/","page":"图神经网络","title":"图神经网络","text":"[3] 刘忠雨，李彦霖，周洋，《深入浅出图神经网络》","category":"page"},{"location":"git/git_notebook/#第一章-Git-基础","page":"Git 学习笔记","title":"第一章 Git 基础","text":"","category":"section"},{"location":"git/git_notebook/#.-常用命令","page":"Git 学习笔记","title":"1. 常用命令","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.1. 配置 user 信息","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"配置自己的用户名为 strongnine，邮箱为 strongnine@163.com，实际用的时候请将此换成自己的用户名和邮箱。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"$ git config --global user.name 'strongnine'\n$ git config --global user.email 'strongnine@163.com'","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.2. config 的三个作用域","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"$ git config --global\n$ git config --local","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"可以显示自己目前的局部（local）或者全局（global）的配置。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 显示 config 设置\n$ git config --list --local\n$ git config --list --global","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.3. git 命令","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"创建仓库可以在 GitHub 上创建仓库，然后再通过 git clone 克隆到自己的本地，也可以现在本地新建的文件夹里用 git init 初始化创建仓库。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# git 仓库的初始化\n$ git init","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"查看当前仓库状态：git status 可以查看当前仓库的状态。能够看到文件的修改、删除、添加、以及重命名（重命名的逻辑就是删除一个文件并且添加一个文件），并且还能够看到当前存在的冲突啥的。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"添加文件：git add 可以将某个文件的更新添加到暂存区区里；","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"git add -u：将文件的修改、文件的删除，添加到暂存区。git add .：将文件的修改，文件的新建，添加到暂存区。git add -A：将文件的修改，文件的删除，文件的新建，添加到暂存区。git add -A 相对于 git add -u 命令的优点 ： git add -A 可以提交所有被删除、被替换、被修改和新增的文件到数据暂存区，而 git add -u 只能操作跟踪过的文件。git add -A 等同于 git add -all. ","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"撤销添加（版本回退）：有的时候我们 add 了一个文件，想要撤销，可以用 git reset","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"撤销添加：git reset HEAD 将绿字变成红字；","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"提交修改：git commit 将当前暂存区里的更新提交，会用默认编辑器跳出信息，可以在第一行添加提交的备注信息，例如 \"add README.md\". ","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"git commit -m\"add README.md\" 可以直接将备注信息一起提交。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"删除文件：git mv <文件名> 是正确删除文件的方法。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"重命名的文件：git mv oldname newname","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"当你重命名了一个文件之后，用 git status 会提示有一个文件被删除，有一个文件是新的 Untracked 文件。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"重置文件：git reset --hard 用来对暂存区的文件进行重置。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"warning: Warning\n注意：git reset 是一条相对危险的命令。","category":"page"},{"location":"git/git_notebook/#.-版本管理","page":"Git 学习笔记","title":"2. 版本管理","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"2.1. 分支管理","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"查看历史：git log 可以查看当前分支的提交历史记录日志，命令 gitk 可以调出图形界面查看历史版本。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"git log --graph 可以有比较图形化的界面；git log --oneline 只显示每次提交的备至；git log -n4 --online 指定查看最近 4 个 commit；git log --all 查看全部分支的日志；git log --all --graph 用图形化的方式显示所有分支的日志；","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"查看分支：git branch -v 可以查看本地有多少分支。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"git branch -av：查看所有分支；git branch -d 分支名：删除分支；git branch -D 分支名：强制删除分支；","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"warning: Warning\n如果分支还未被 merged 的时候要用强制删除，请确保该分支无用。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"创建分支：git checkout ","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"git checkout -b 可以创建新分支并且切换到该新的分支；","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"有的时候需要加上 --decorate 参数才可以显示（master）（temp）等分支信息。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"2.2. .git 目录的内容","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"cat 命令主要用来查看文件内容，创建文件，文件合并，追加文件内容等功能。 cat HEAD 查看 HEAD 文件的内容 git cat-file 命令 显示版本库对象的内容、类型及大小信息。 git cat-file -t b44dd71d62a5a8ed3 显示版本库对象的类型 git cat-file -s b44dd71d62a5a8ed3 显示版本库对象的大小 git cat-file -p b44dd71d62a5a8ed3 显示版本库对象的内容","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"HEAD：指向当前的工作路径 config：存放本地仓库（local）相关的配置信息。 refs/heads：存放分支 refs/tags：存放tag，又叫里程牌 （当这次 commit 是具有里程碑意义的，比如项目 1.0 的时候 就可以打 tag） objects：存放对象 .git/objects/ 文件夹中的子文件夹都是以哈希值的前两位字符命名 每个 object 由 40 位字符组成，前两位字符用来当文件夹，后 38 位做文件。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"commit、tree、blob 的关系","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"(Image: relations)","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"一个 commit 对应一颗 tree，tree 相当于文件夹，blob 相当于具体的文件（数据）。git 里面，文件内容相同， 就是视为同一个文件。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"当创建了新的空文件夹时，使用 status 不会检测到这个空的文件夹。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"2.3. 分离头指针","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"变更没有基于某个 branch，在分离头指针进行的 commit，如果没有及时合并到某个 branch，可能会被 git 当作垃圾清掉。如果这种变更是重要的，就要将其与某个 branch 绑在一起。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"git checkout -b 可以创建新分支并且切换到该新的分支。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"HEAD 指针可以指向某个分支的最后一次提交，也可以不和某个分支挂钩，当处于分离头指针时，可以直接指向某个 commit。它只能够定位到某个 commit。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"对比提交之间的差异：git diff [commit1] [commit2] 可以比较两个具体的 commit 的差异。git diff HEAD HEAD^1 将当前结点与其父亲结点进行对比。HEAD^1, HEAD~1, HEAD~, HEAD^ 都一样。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"一个节点，可以包含多个子节点（checkout 出多个分支）\n一个节点可以有多个父节点（多个分支合并）\n^ 和 ~ 都是父节点，区别是跟随数字时候，^2 是第二个父节点，而 ~2 是父节点的父节点\n^ 和 ~ 可以组合使用,例如 HEAD~2^2","category":"page"},{"location":"git/git_notebook/#第二章-独自使用-Git","page":"Git 学习笔记","title":"第二章 独自使用 Git","text":"","category":"section"},{"location":"git/git_notebook/#.-commit-的操作","page":"Git 学习笔记","title":"1. commit 的操作","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.1. 修改 commit 的 message","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 修改最新 commit 的信息\n$ git commit --amend\n# 想要修改旧 commit 的信息，需要先选择其父节点\n# 运行后会弹出一个交互界面，在里面修改、保存之后\n# 还会继续弹出一个交互界面，提示要把 message 如何修改\n$ git rebase -i 父节点","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"warning: Warning\n这种操作只适用于还未合并到「主线」 的分支上，否则会影响到合作者的工作。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.2. 整理多个 commit ","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 和上一个操作相似\n# 在弹出的交互界面进行不同的修改就行（会有提示）\n$ git rebase -i 父节点\n\n# 上面的是把「连续的」commit 合并，还有一种是把「间隔的」合并\n$ git rebase -i 父节点","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.3. 对比差异","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 对比暂存区和 HEAD 里面内容的差异（看看做了哪些改动）\n$ git diff --cached\n\n# 对比工作区和暂存区的不同\n$ git diff\n\n# 只比较某个文件\n$ git diff -- <文件名>\n\n# 查看不同提交的指定文件的差异\n$ git diff <指针 1> <指针 2> -- <文件名>","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.4. 恢复变更","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 把暂存区里面的文件全部恢复成和 HEAD 一样的\n$ git reset HEAD\n\n# 让工作区的文件恢复为暂存区一样（变更工作区）\n$ git checkout -- index.html\n\n# 取消暂存区部分文件的更改\n$ git reset HEAD -- <文件名>...","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.5. 消除最近几次提交","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 将头指针指向特定的某次提交，并且删除这之前的提交\n# <危险指令> 慎用！！！\n$ git reset --hard <指针>","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.6. 删除文件","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 正确删除文件的方法\n$ git rm <文件名>","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.7. 临时加塞的紧急任务 —— stash 的使用","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 把当前状态存放\n$ git stash\n# 列出状态区\n$ git stash list\n# 恢复暂存区（弹出之前放进 stash 顶的），但是 stash 堆栈里的信息还会在\n$ git stash apply\n# 恢复的基础上还会丢掉 stash 里面的信息\n$ git stash pop","category":"page"},{"location":"git/git_notebook/#.-Git-管理","page":"Git 学习笔记","title":"2. Git 管理","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"2.1. 指定不需要 Git 管理的文件","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":".gitignore 文件上的内容就是表示指定类型的文件不给 Git 进行管理。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"2.2. Git 的备份","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"(Image: git_learning_fig2)","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"哑协议传输进度不可看见，智能协议可见。智能协议比哑协议快。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# --bare 代表不带工作区的裸仓库\n# 哑协议\n$ git clone --bare /path/to/repo.git <拷贝路径.git>\n# 智能协议\n$ git clone --bare / file:///path/to/repo.git <拷贝路径.git>\n\n# 把本地的变更同步到远端\n$ git remote -v\n$ git remote add <名称> <协议地址>\n# 查看分支\n$ git branch -av\n$ git push <名称>\n$ git push --set-upstream <  > <  >","category":"page"},{"location":"git/git_notebook/#第三章-Github-同步","page":"Git 学习笔记","title":"第三章 Github 同步","text":"","category":"section"},{"location":"git/git_notebook/#配置公私钥","page":"Git 学习笔记","title":"配置公私钥","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"在 Github 首页上，寻找 help，在上面有关于如何 connecting to github with SSH 的做法。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 打开 git bash 在里面输入下面命令\n# 若干存在 id_rsa 和 id_rsa.pub 文件则代表已经有公私钥\n# 否则应该要根据 Help 上的提示进行生成\n$ ls - al ~/.ssh\n# 查看 id_rsa.pub 的内容\n$ cd ~/.ssh\n$ cat id_rsa.pub\n# 复制里面的内容，前往 github 账户设置里面添加 SSH keys","category":"page"},{"location":"git/git_notebook/#**把本地仓库同步到-Github**","page":"Git 学习笔记","title":"把本地仓库同步到 Github","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 添加一个新的 remote\n$ git remote add <名称> <SSH>\n# 查看已有的 remote\n$ git remote -v\n\n# 把所有内容 push\n$ git push <name> --all\n# 如果远端有某些文件是本地未包含的，这个分支会被拒绝 push\n# 需要把远端的先「拉」下来\n$ git fetch <name> master\n# 切换到 master 分支\n$ git checkout master\n# 与远端的 .../master 的分支合并\n$ git merge <远端分支>\n# 但注意如果两个分支都是独立的，没有共同的历史，那么会拒绝合并\n# 查看 merge 帮助\n$ git merge -h\n$ git merge --allow-unrelated-histories <远端分支>\n# 现在进行 push 就不会报错了\n$ git push <name> master","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"注：在之后为了方便学习，将一些命令与视频里面的进行同步，<name> 会用 github 来代替，因为我们把远端的仓库 fetch 下来并且命名为 gitHub 了","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"个人笔记总结git remote -v 查看远程版本库信息\ngit remote add <name> <url>添加 githup 远程版本库\ngit fetch <name> 拉取远程版本库\ngit merge -h 查看合并帮助信息\ngit merge --allow-unrelated-histories githup/master 合并 <name> 上的 master 分支（两分支不是父子关系，所以合并需要添加 –allow-unrelated-histories）\ngit push <name> 推送同步到 <name> 仓库—— by DriveMan_邱佳源","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"fast forward 到底是什么？举个例子，本地分支往远端分支做 push，如果远端分支不是本地分支的祖先，那它俩就不是 fast forward 了。反之，它俩就是 fast forward 的关系。","category":"page"},{"location":"git/git_notebook/#第四章-Git-多人单分支集成协作","page":"Git 学习笔记","title":"第四章 Git 多人单分支集成协作","text":"","category":"section"},{"location":"git/git_notebook/#.-多个人对文件修改","page":"Git 学习笔记","title":"1. 多个人对文件修改","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.1. 不同人修改了不同文件","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# \n# 会出现 Non fast_forwards 的报错，远端仓库会拒绝这个 push\n# 先把远端的 fetch 下来\n$ git fetch <name>(github)\n# 然后查看 branch 会发现有 [ahead 1, behind 1] 这样的信息，\n# 代表远端有的这里没有和这里有的远端没有\n$ git branch -av\nfeature/add_git_commands     07c85df [ahead 1, behind 1] ......\n\n# 有时候会考虑合并\n$ git merge (github/feature/add_git_commands)","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"老师你好，我有个问题哈，clone 命令 git clone git@github.com:git2019/gitlearning.git 既然已经把远程仓库所有内容都克隆到本地了，为什么还需要 git checkout -b feature/addgitcommands origin/feature/addgit_command 命令基于远程分支在本地建立分支，不是从远程clone下来了嘛，为什么还要新建，难道 clone 命令不能克隆分支吗？作者回复：我们在本地无法直接在 clone 下来的远程分支上做变更的，只能基于远程分支建本地分支后，才能创建 commit。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.2. 不同人修改同一文件的不同区域","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# pull 会把远端的内容拉下来，并且本地的也会进行更新\n# 简介的方法就是直接 pull，还有一种是 fetch + merge\n# 多查看 branch ，看看 ahead 和 behind 的数目\n$ git branch -av\n\n# 当只有 ahead 没有 behind 的时候，肯定是 fast-forward 可以提交","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"fast-forword 看了英语翻译为快进，结合 git branch -av 中的 ahead 和 behind，ahead 是本地仓库比远端仓库多 commit，behind 是本地仓库比远端仓库少 commit。对正常的备份系统来说，我本地只能比备份多，备份不可能比我本地多才是。然而，git 由于多用户提交原因出现备份比本地多了，本地滞后了，所以需要 pull 一下，让本地比备份相等或多，这种情况就是 fast forward ，也就是我本地要比备份快进。不知理解对否？作者回复：其实就是两个分支的关系为 0|n 或者 n|0 ，如果两个分支直接为 n|m 的关系就不是 fast forward 。A 分支比 B 分支多 5 个 commit，B 比 A 分支多 3 个 commit。A 和 B 就不是 fast forward。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"1.3. 不同人修改同一文件的同一区域","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 如果 push 不上去，使用 merge 又提示已经更新到最新了\n# 就说明远端变了，要及时更新\n$ git pull\nAuto-merging (index.html)\nCONFLICT(content): Merge conflict in (index.html)\n# 提示 CONFLICT(content) 说明文件有冲突，不能自动合并 index.html\n# 打开这个文件，会提示哪里出现冲突\n$ vi index.html\n# 编辑完成后查看状态\n$ git status\n\n# 如果这个分支有问题了，可以用 --abort 退出合并\n$ git merge --abort\n$ git commit -am'(commit text)'","category":"page"},{"location":"git/git_notebook/#.-更改了文件名","page":"Git 学习笔记","title":"2. 更改了文件名","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"2.1. 同时变更了文件名和内容","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 其中有一个人变更了文件名\n# 另一个人只变更了文件内容\n# pull 的话会智能识别问题\n$ git pull","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"2.2. 同一文件改成不同的文件名","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 依旧是报冲突\n$ git pull\n# 查看工作目录，会出现未命名的文件，和两个重命名的文件\n# 如果使用 diff 查看两个文件的差异，不会显示差异\n$ diff <file-1> <file-2>\n# 使用 status，会提示：\nboth deleted:  <oldfilename>\nadded by us:   <filename-1>\nadded by them: <filename-2>\n# 可以先移除不要的文件，再加上想要保存的文件名\n$ git rm <filename-2>\n$ git add <filename-1>\n$ git commit -am'(commit text)'","category":"page"},{"location":"git/git_notebook/#第五章-集成使用禁忌","page":"Git 学习笔记","title":"第五章 集成使用禁忌","text":"","category":"section"},{"location":"git/git_notebook/#.-禁止向集成分支执行-push-f","page":"Git 学习笔记","title":"1. 禁止向集成分支执行 push -f","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"-f, --force 是强制更新，即使不是 fast-forward 也可以 push。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"# 把历史 reset 到某个 log\n$ git reset --hard (b3f390c)\n# 强制 push，就会把在 b3f390c 后面做的改变都删除\n$ git push -f (origin) (feature/add_git_commands)","category":"page"},{"location":"git/git_notebook/#.-禁止向集成分支执行变更历史的操作","page":"Git 学习笔记","title":"2. 禁止向集成分支执行变更历史的操作","text":"","category":"section"},{"location":"git/git_notebook/#第六章-GitHub","page":"Git 学习笔记","title":"第六章 GitHub","text":"","category":"section"},{"location":"git/git_notebook/#.-核心功能","page":"Git 学习笔记","title":"1. 核心功能","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"代码预览、项目管理、集成、团队管理、社交编码（开源）、文档、存放代码。","category":"page"},{"location":"git/git_notebook/#.-寻找开源项目","page":"Git 学习笔记","title":"2. 寻找开源项目","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"使用高级搜索：光标放在搜索框里，按回车就会出现 advanced search 了。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"可以在 Help 上查看有哪些高级搜索的语法。  ","category":"page"},{"location":"git/git_notebook/#.-搭建个人博客","page":"Git 学习笔记","title":"3. 搭建个人博客","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"通过高级搜索在搜索框中输入 blog easily start in:readme stars:>5000 找到 jekyll-now 仓库。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"第一步就是 fork 一个到自己的账号里去。fork 完后修改工程名称：<username>.github.io","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"第二步修改 _config.yml。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"在 _posts 里面新增文件格式为：2018-12-24-<title>.md","category":"page"},{"location":"git/git_notebook/#第七章-团队协作","page":"Git 学习笔记","title":"第七章 团队协作","text":"","category":"section"},{"location":"git/git_notebook/#.-创建团队项目","page":"Git 学习笔记","title":"1. 创建团队项目","text":"","category":"section"},{"location":"git/git_notebook/#.-挑选合适的分支集成策略","page":"Git 学习笔记","title":"3. 挑选合适的分支集成策略","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"在仓库的 Insights => Network 里可以看到特性分支演变历史。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"在 Options 的 Merge button 可以设置允许哪种合并。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"merge 把分支最后合并到 master 上去；\nsquash merging 把分支的所有 commits 变成一个，再放到主线上去。（在当前主线后面加上）\nrebase merging 把分支的所有 commits 添加到主线后面去。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"后面两种适合于线性开发的情况。","category":"page"},{"location":"git/git_notebook/#.-issue-跟踪","page":"Git 学习笔记","title":"4. issue 跟踪","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"issue 上有标签管理，对不同的问题进行分类。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"还可以对 issue 进行模型管理，自定义一些问题报告的模板。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"有 Bug report、Feature request 等。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"使用 Projects 的看板来管理 issue","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"点击 Pojects 进行看板（Board）的设置。","category":"page"},{"location":"git/git_notebook/#.-Code-review","page":"Git 学习笔记","title":"5. Code review","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"在 Settings 的 Branches 上可以设置特殊分支的保护规则。比如对于 master 分支进行 push 保护，每次 push 都要有特定人数去检查才能通过。","category":"page"},{"location":"git/git_notebook/#.-多分支的集成","page":"Git 学习笔记","title":"6. 多分支的集成","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"特性分支往主干合，要发 Pull requests。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"git rerere 是一个隐藏功能，允许你让 Git 记住解决一个块冲突的方法，在下一次看到相同冲突时，自动解决。","category":"page"},{"location":"git/git_notebook/#第八章-GitLab","page":"Git 学习笔记","title":"第八章 GitLab","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"这两章先看视频过一遍，等到应用的时候可以复习。目前不知道具体的使用需求，先大概看个印象。","category":"page"},{"location":"git/git_notebook/#其它问题","page":"Git 学习笔记","title":"其它问题","text":"","category":"section"},{"location":"git/git_notebook/#.-在-Windows-上如何支持中文","page":"Git 学习笔记","title":"1. 在 Windows 上如何支持中文","text":"","category":"section"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"参考解决 Git 在 windows 下中文乱码的问题.md。","category":"page"},{"location":"git/git_notebook/","page":"Git 学习笔记","title":"Git 学习笔记","text":"有一个注意的点：目前无法解决输入中文字符会显示乱码的问题解决方案：git commit 时，不用 -m 参数，直接回车让 vim 来处理\n进 vim 后按 i 进入编辑模式，完成后再保存退出","category":"page"},{"location":"#Docs","page":"Home","title":"9Docs","text":"","category":"section"},{"location":"AI/CNN/#卷积神经网络","page":"卷积神经网络","title":"卷积神经网络","text":"","category":"section"},{"location":"AI/CNN/#深度卷积神经网络","page":"卷积神经网络","title":"深度卷积神经网络","text":"","category":"section"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"感受野（Receptive Field），指的是神经网络中神经元「看到的」输入区域，在卷积神经网络中，feature map 上某个元素的计算受输入图像上某个区域的影响，这个区域即该元素的感受野。感受野是个相对概念，某层 feature map 上的元素看到前面不同层上的区域范围是不同的，通常在不特殊指定的情况下，感受野指的是看到输入图像上的区域。","category":"page"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"例如两个级联的卷积核大小为 3times 3，stride = 2 的卷积层的感受野为 7times 7，如图所示","category":"page"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"(Image: 感受野)","category":"page"},{"location":"AI/CNN/#","page":"卷积神经网络","title":"","text":"","category":"section"},{"location":"AI/CNN/#卷积详解","page":"卷积神经网络","title":"卷积详解","text":"","category":"section"},{"location":"AI/CNN/#卷积","page":"卷积神经网络","title":"卷积","text":"","category":"section"},{"location":"AI/CNN/#因果卷积（Causal-Convolution）","page":"卷积神经网络","title":"因果卷积（Causal Convolution）","text":"","category":"section"},{"location":"AI/CNN/#空洞卷积","page":"卷积神经网络","title":"空洞卷积","text":"","category":"section"},{"location":"AI/CNN/","page":"卷积神经网络","title":"卷积神经网络","text":"空洞卷积具有更大的感受野，有助于构建长期记忆功能。","category":"page"}]
}
