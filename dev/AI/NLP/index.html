<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>- · 9Docs</title><meta name="title" content="- · 9Docs"/><meta property="og:title" content="- · 9Docs"/><meta property="twitter:title" content="- · 9Docs"/><meta name="description" content="Documentation for 9Docs."/><meta property="og:description" content="Documentation for 9Docs."/><meta property="twitter:description" content="Documentation for 9Docs."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">9Docs</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">机器视觉</span><ul><li><a class="tocitem" href="../../CV/contrast/">对比度增强</a></li><li><a class="tocitem" href="../../CV/dark_channel_prior/">暗通道先验去雾</a></li></ul></li><li><span class="tocitem">深度学习</span><ul><li><a class="tocitem" href="../../DL/CNN/">卷积神经网络</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>-</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>-</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/strongnine/9Docs" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/strongnine/9Docs/blob/main/docs/src/AI/NLP.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><p>多模态数据：文本、图像、音频、视频、结构化数据</p><p>自然语言处理发展的三个阶段：</p><ul><li>1950 ～ 1970 年：基于经验、规则的阶段；</li><li>1970 ～ 2008 年：基于统计方法的阶段；</li><li>2008 年至今：基于深度学习技术的阶段；</li></ul><p><strong>贝叶斯模型</strong></p><h3 id="词袋模型"><a class="docs-heading-anchor" href="#词袋模型">词袋模型</a><a id="词袋模型-1"></a><a class="docs-heading-anchor-permalink" href="#词袋模型" title="Permalink"></a></h3><p><strong>词袋模型（Bag-of-words, BOW）</strong>：假设词与词之间是上下文独立的，即不考虑词之间的上下文关系；</p><ul><li>优点：<ul><li>简单易用速度快；</li><li>在丢失一定预测精度的前提下，很好地通过词出现的频率来表征整个语句的信息；</li></ul></li><li>缺点：仅考虑词在一个句子中是否出现，而不考虑词本身在句子中的重要性（使用 TF-IDF 可以考虑重要性）；</li></ul><p>对于两个语句：</p><pre><code class="language-python hljs">&quot;We have noticed a new sign in to your Zoho account.&quot;
&quot;We have sent back permission.&quot;</code></pre><p>构造语料字典：</p><pre><code class="language-python hljs">{
    &#39;We&#39;: 2, &#39;have&#39;: 2, &#39;noticed&#39;: 1, &#39;a&#39;: 1,
    &#39;new&#39;: 1, &#39;sign&#39;: 1, &#39;in&#39;: 1, &#39;to&#39;: 1,
    &#39;your&#39;: 1, &#39;Zoho&#39;: 1, &#39;account&#39;: 1, &#39;sent&#39;: 1,
    &#39;back&#39;: 1, &#39;permission.&#39;: 1
}
# 上面两个语句生成的 BOW 特征分别为：
[1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1]
[0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0]</code></pre><p><strong>TF-IDF（Term Frequency-Inverse Document Frequency）</strong>：使用 <span>$\text{TF}\times \text{IDF}$</span> 对每一个出现的词进行加权：</p><p class="math-container">\[\text{TF-IDF}(t,d)=\text{TF}(t,d)\times \text{IDF}(t)\]</p><p>其中 <span>$\text{TF}(t,d)$</span> 为单词 <span>$t$</span> 在文档 <span>$d$</span> 中出现的频率，<span>$\text{IDF}(t)$</span> 是逆文档频率，用来衡量单词 <span>$t$</span> 对表达语义所起的重要性，表示为：</p><p class="math-container">\[\text{IDF}(t) = \log{\frac{\text{Num. of articles}}{\text{Num. of articles containing word }t+1}}\]</p><p>直观解释为，如果一个单词在非常多的文章里面都出现，那么它可能是一个比较通用的词汇，对于区分谋篇文章特殊语义的贡献比较小，因此对权重做一定惩罚。</p><ul><li>优点：简单易用速度快；</li><li>缺点：文本语料稀少、字典大小大于文本语料大小时，容易发生过拟合；</li></ul><p><strong>N-Gram 语言模型</strong>：假设有一个句子 <span>$S(w_1,w_2,w_3,\cdots,w_n)$</span>，其中 <span>$w_i$</span> 代表句子中的词，那么这个句子的出现概率就是所以单词出现概率的乘积 <span>$p(S)=p(w_1)\times p(w_2)\times p(w_3)\times\cdots\times p(w_n)$</span>. 在此基础上加上马尔科夫假设，即当前词的出现之和前 <span>$n$</span> 个词有关，则有：</p><p class="math-container">\[p(S) = p(w_1)\times p(w_2\mid w_1)\times\cdots \times p(w_n\mid w_{n-1}).\]</p><p>N-Gram 模型可以与 BOW、TF-IDF 模型相结合，构建 Bi-Gram、Tri-Gram 等生成额外的稀疏特征向量，构建出来的特征比使用 Uni-Gram 的 BOW、TF-IDF 特征更具有表征能力。</p><p>词袋模型的问题：如果近义词出现在不同文本中，那么在计算这一类文本的相似度或者进行预测时，如果训练数据不含大量标注，就会出现无法识别拥有相似上下文语义词的情况；</p><h3 id="词嵌入模型"><a class="docs-heading-anchor" href="#词嵌入模型">词嵌入模型</a><a id="词嵌入模型-1"></a><a class="docs-heading-anchor-permalink" href="#词嵌入模型" title="Permalink"></a></h3><p><strong>Word2Vec</strong>：常用的模型训练方式为 CBOW 和 Skip-Gram 两种算法。</p><p><strong>glove</strong>：</p><p><strong>fastText</strong>：</p><blockquote><p>针对中文词向量的预训练，有腾讯公开的 AI Lab 词向量。</p></blockquote><h3 id="深度学习"><a class="docs-heading-anchor" href="#深度学习">深度学习</a><a id="深度学习-1"></a><a class="docs-heading-anchor-permalink" href="#深度学习" title="Permalink"></a></h3><p><strong>TextCNN</strong>：模型结构简单，训练和预测速度快，同时拥有比传统模型更高的精度。采用多尺度卷积来模拟 N-Gram 模型在文本上的操作，最终合并之后进行呢预测。适合短文本以及有明显端与结构的语料。</p><p><strong>DPCNN</strong>：从 ResNet 结构中借鉴了残差块（residual block）的理念，模拟 CV 任务中对于图像特征进行逐层提取的操作。相比较于 TextCNN 能够在文本结构复杂、语义丰富或者上下文依赖性强的文本上有更好的表现。</p><p><strong>LSTM 类模型</strong>：包括典型双向循环神经网络结构 LSTM、Bi-LSTM、Bi-GRU + Attention，LSTM 和 GRU 层具有非常好的时序拟合能力。Attention 机制对不同时间的状态值进行加权，能够进一步提升模型的预测能力，适合具有复杂语义上下文的文本。</p><p><strong>Attention 机制</strong>：从原理上分析，是一种对词在句子中的不同状态进行加权的操作，从最原始的加权平均，逐步发展到 Self-Attention。通过使用词的相似度矩阵进行计算，调整词在句子中对应的权重，从而允许将词的加权求和作为输出或者下一层的输入。</p><p>现有的上下文相关的预训练模型包括：ELMo、GPT、BERT、BERT-wwm、ERNIE_1.0、XLNet、ERNIE_2.0、RoBERTa、ALBERT、ELECTRA</p><p><strong>ELMo</strong>：是一个采用自回归语言模型方式训练的语言模型。自回归语言模型的本质是通过输入的文本序列，对下一个词进行预测，通过不断优化预测的准确率，使模型逐步学到上下文的语义关系。ELMo 的结构包括正向 LSTM 层和反向 LSTM 层，通过分别优化正向下一次词和反向下一个词达到更好的预测效果。</p><p><strong>GPT</strong>：将 Multi-Head Attention 和 Transformer 结构应用到了语言模型的预训练上，采用正向 Transformer 结构，去除了其中的解码器，同 ELMo 模型一样，采用自回归语言模型的方式进行训练。</p><p><strong>BERT</strong>：使用自编码器模式进行训练，模型结构中包含正向和反向 Transformer 结构。为了减少由双向 Transformer 结构和自编码器造成的信息溢出影响，BERT 在训练中引入了 MLM，防止 BERT 模型因双向 Self-Attention 而导致的过拟合。</p><p><strong>MLM</strong>（Masked Language Model，遮蔽语言模型）：预训练中 15% 的词条（token）会被遮蔽，对于这 15% 的词条，有 80% 的概率会使用 [MASK] 替换，10% 的概率随机替换，10% 的概率保持原样，这个替换策略在模型训练中起到正则作用。</p><p><strong>RoBERTa</strong>：是 Facebook 提出的模型，在 BERT 的基础上移除了 NSP（Next Sentence Prediction）机制，并且修改了 MLM 机制，调整了其参数。</p><p><strong>ERNIE</strong>：是百度提出的模型，在 BERT 的基础上优化了对中文数据的预训练，增加了三个层次的预训练：Basic-Level Masking（第一层）、Phrase-Level Masking（第二层）、Entity-Level Masking（第三层），分别从字、短语、实体三个层次上加入先验知识，提高模型在中文语料上的预测能力。</p><p><strong>RoBERTa-wwm</strong>：由哈工大讯飞联合实验室发布，并不是一个严格意义上的新模型，wwm（whole word mask）是一种训练策略。BERT 所用的 MLM 具有一定的随机性，会将原始词的 word pieces 遮蔽掉，而 wwm 策略中，相同词所属的 word pieces 被遮蔽之后，其同属其他部分也会一同被遮蔽，在保证词语完整性的同时又不影响其独立性。</p><p>卷积神经网络</p><p>循环神经网络</p><p>自注意力机制（Self-Attention ）</p><p>Transformer</p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Saturday 13 September 2025 14:58">Saturday 13 September 2025</span>. Using Julia version 1.11.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
