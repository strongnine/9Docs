<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>计算机视觉 · 9Docs</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">9Docs</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">人工智能</span><ul><li><a class="tocitem" href="../FE/">特征工程</a></li><li><a class="tocitem" href="../ML/">机器学习</a></li><li><a class="tocitem" href="../NN/">神经网络</a></li><li><a class="tocitem" href="../CNN/">卷积神经网络</a></li><li><a class="tocitem" href="../RNN/">循环神经网络</a></li><li><a class="tocitem" href="../GNN/">图神经网络</a></li><li><a class="tocitem" href="../GAN/">生成对抗网络</a></li><li class="is-active"><a class="tocitem" href>计算机视觉</a><ul class="internal"><li><a class="tocitem" href="#目标检测"><span>目标检测</span></a></li><li><a class="tocitem" href="#人脸识别"><span>人脸识别</span></a></li></ul></li><li><a class="tocitem" href="../NLP/">自然语言处理</a></li></ul></li><li><a class="tocitem" href="../../git/git_notebook/">Git 学习笔记</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">人工智能</a></li><li class="is-active"><a href>计算机视觉</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>计算机视觉</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/strongnine/9Docs/blob/master/docs/src/AI/CV.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><p>计算机视觉的任务有：图像分类、物体检测、语义分割、文字识别、人脸识别等</p><h2 id="目标检测"><a class="docs-heading-anchor" href="#目标检测">目标检测</a><a id="目标检测-1"></a><a class="docs-heading-anchor-permalink" href="#目标检测" title="Permalink"></a></h2><p>目标检测（Object Detection）是计算机视觉中极为重要的基础问题，是实例分割（Instance Segmentation)、场景理解（Secne Understanding）、目标跟踪（Object Tracking）、图像标注（Image Captioning）等问题的基础。</p><p><strong>目标检测任务：</strong>给定一张图片，将图片中的每个物体识别出来并且提出一个置信度，用矩形方框（Bounding Box）或者不规则的区域标识出来。</p><p>目标检测模型分为单步（one-stage）模型和两步（two-stage）模型两大类。单步模型在计算效率上有优势，两步模型在检测精度上有优势。</p><p><strong>单步模型</strong>是指没有独立地、显式地提取候选区域（region proposal），直接由输入图像得到其中存在的物体的类别和位置信息的模型。例如 OverFeat、SSD（Single Shot multibox-Detector）、YOLO（You Only Look Once） 等模型。</p><p><strong>两步模型</strong>是指有独立的、显式的候选区域提取过程，即先在输入图像上筛选出一些可能存在物体的候选区域，然后针对每个候选区域，判断其是否存在物体，如果存在就给出物体的类别和位置修正信息。例如 R-CNN、SPPNet、Fast R-CNN、Faster R-CNN、R-FCN、Mask R-CNN 等模型。</p><h3 id="目标检测历史"><a class="docs-heading-anchor" href="#目标检测历史">目标检测历史</a><a id="目标检测历史-1"></a><a class="docs-heading-anchor-permalink" href="#目标检测历史" title="Permalink"></a></h3><p><img src="../../assets/目标检测模型发展历程.png" alt/></p><p>在一开始的 CNNs 上，把一张图片划分成为固定的区域，然后识别这些区域中是否有某个物体，有的话就把这个区域标识出来。但是在实际中，图片上的物体大小是不固定的，用这种固定大小的区域去识别物体显然是不合理的。人们想到，如果想要让框更加合适，可以增加框的数量，然后让每个区域都变得尽可能地小。但是这样框太多的时候，又会导致计算量的增加。</p><p><strong>基于区域的卷积神经网络（Region-based CNN，R-CNN）</strong>：是第一个将 CNN 用于目标检测的深度学习模型。它是是解决这种缺点的更好方法，它使用生成区域建议的方式来选择区域。R-CNN 的选框方式是根据选择性搜索（selective search）来进行的，选框也叫做区域（regions）。</p><p><strong>选择性搜索</strong>：一个物体会包括四种信息：不同的尺度、颜色、纹理和边界，选择性搜索目标就是识别这些模式，提出不同的区域。首先，先生成最初的分割得很细的子分割，然后再将这些很细的小区域按照颜色相似度、纹理相似度、大小相似度和形状相似兼容性来合并成更大的区域。这个最后合成的区域就是物体在图片中的位置，即<strong>感兴趣区域（Region of Interest，RoI）</strong>。</p><p>得到的 RoI 大小是不一样的，将这些 RoI reshape成 CNN 输入的大小，然后 CNN 提取每个区域的特征值，用<strong>支持向量机（Support Vector Machine，SVM）</strong>来对这些区域进行分类，最后<strong>边界框回归（Bounding Box Regression）</strong>就预测生成的框。</p><p>R-CNN 具有的不足就在于，每一张图片都会生成很多个 RoI，整个过程用了三个模型：特征提取的 CNN、物体分类的 SVM、预测边界框的回归模型。这些过程让 R-CNN 变得非常慢，预测一张图片要几十秒。</p><p><strong>Fast R-CNN</strong>：添加了一个 <strong>RoI 池化层（RoI Pooling Layer）</strong>来把所有的建议区域转换成适合的尺寸，输入到后面的<strong>全连接层（Fully Connection）</strong>。Fast R-CNN 将 R-CNN 的三个独立的模型集合到一个模型中，因为减少了很多的计算量，Fast R-CNN 在时间花费大大地减少了。</p><p>具体步骤为：图片通过 CNN 得到 RoI，然后 RoI 池化层将 RoI 改变成相同的尺寸，再将这些区域输入到全连接层上进行分类，同时使用 softmax 和<strong>线性回归层（Linear Regression Layers）</strong>来输出 Bounding Boxes。</p><p>Faster R-CNN：Fast R-CNN 依然在使用选择性搜索来作为寻找 RoI 的方法，虽然速度提高了，但是一张图片依旧需要花费 2 秒的时间。因此，Faster R-CNN 使用一个<strong>区域建议网络（Region Proposal Network，RPN）</strong>来获得更高的效率。RPN 图片特征 map 作为输入，生成一系列带目标分数的建议，也就是告诉网络给出的区域有物体的可能性有多大，分数越高代表包含了物体的可能性越高。</p><p>具体步骤：</p><ol><li>把图片作为输入放进卷积网络中去，返回的是一个特征映射（feature map）；</li><li>RPN 处理这些 map，返回带分数的物体建议；</li><li>接下来的 RoI pooling 把这些建议都 reshape 成相同的尺寸；</li><li>最后，放到含有 softmax 层和线性回归层的全连接层上，来分类和输出 bounding boxes。</li></ol><p>RPN 被集成在了网络里面，等于从区域建议到最后的分类回归都在同一个网络，实现了端到端。即我们给这个网络输入一张图片，网络就会输出 bounding boxes 和分数。</p><p><strong>区域建议网络（Region Proposal Network，RPN）</strong>：在 RPN 在这个 map 上使用一个滑动窗口（sliding window），在每个窗口中都会生成 k 个不同形状和大小的 anchor boxes。Anchor 就是在图片中有不同形状和大小的但具有固定尺寸的边界框。什么意思呢？就是每一个 anchor 都是固定的大小，比如有 <span>$3\times3$</span>、<span>$6\times 6$</span>、<span>$3\times 6$</span>、<span>$6\times 3$</span> 这些，他们和最后的 bounding boxes 不一样，anchor 的尺寸都是固定的。对于每一个 anchor，RPN 会做两件事情：</p><p>（1）首先预测 anchor 框出的部分属于物体的可能性；</p><p>（2）然后对 anchor 生成的 bounding box 做回归，去调节这个 anchor 能使它更加好的框出物体。</p><p>在 RPN 之后我们会得到不同形状大小的 bounding boxes，再输入到 RoI 池化层中。在这一步，虽然知道 boxes 里面是一个物体了，但其实是不知道它属于哪个类别的。就好像是，它知道这个东西是个物体，但是不知道是猫是狗还是人。</p><p><strong>RoI 池化层</strong>的作用就是提取每个 anchor 的固定大小的 feature map。这些 feature map 最后就被送到全连接层里去做 softmax 分类和线性回归。最后就会得到分类好的又有 bounding box 的物体了。</p><p>通过使用端到端的方式去进行，并且也不会考虑所有的 RoI，处理一张图片只需要 0.2 秒。</p><h3 id="RetinaNet-中的-Focal-Loss-——-解决正负样本不均衡问题"><a class="docs-heading-anchor" href="#RetinaNet-中的-Focal-Loss-——-解决正负样本不均衡问题">RetinaNet 中的 Focal Loss —— 解决正负样本不均衡问题</a><a id="RetinaNet-中的-Focal-Loss-——-解决正负样本不均衡问题-1"></a><a class="docs-heading-anchor-permalink" href="#RetinaNet-中的-Focal-Loss-——-解决正负样本不均衡问题" title="Permalink"></a></h3><p>在 RetinaNet 的论文 <em>Focal Loss for Dense Object Detection</em> 中提出了一个新的损失函数 —— Focal Loss，主要用于解决在单步目标检测场景上训练时前景（foreground）和背景（background）类别极端失衡（比如 1:1000）的问题。</p><p>一般来说，对于二分类问题，交叉熵损失为：</p><p class="math-container">\[\text{CE}(p,y)=\begin{cases}-\log(p),\qquad \text{if } y=1\\ -\log(1-p),\quad \text{otherwise}. \end{cases}\]</p><p>其中 <span>$y\in\{\pm 1\}$</span> 是类别标签，<span>$p\in[0, 1]$</span> 是模型对于样本类别属于 <span>$y=1$</span> 的预测概率，定义</p><p class="math-container">\[p_t\begin{cases}p, \qquad \text{if } y=1,\\ 1-p, \quad \text{otherwise}. \end{cases}\]</p><p>因此交叉熵损失可以重写为 <span>$\text{CE}(p,y)=\text{CE}(p_t)=-\log(p_t).$</span></p><h2 id="人脸识别"><a class="docs-heading-anchor" href="#人脸识别">人脸识别</a><a id="人脸识别-1"></a><a class="docs-heading-anchor-permalink" href="#人脸识别" title="Permalink"></a></h2><h3 id="人脸识别中的损失函数"><a class="docs-heading-anchor" href="#人脸识别中的损失函数">人脸识别中的损失函数</a><a id="人脸识别中的损失函数-1"></a><a class="docs-heading-anchor-permalink" href="#人脸识别中的损失函数" title="Permalink"></a></h3><p><strong>Softmax Loss</strong>：最常见的人脸识别函数，原理是去掉最后的分类层，作为解特征网络导出解特征向量用于人脸识别。</p><p class="math-container">\[\hat{\boldsymbol{y}}=\text{softmax}(\boldsymbol{W}^\top \boldsymbol{x})=\frac{\exp(\boldsymbol{W}^\top \boldsymbol{x}+\boldsymbol{b})}{\boldsymbol{1}^\top_C\exp(\boldsymbol{W}^\top \boldsymbol{x}+\boldsymbol{b})},\]</p><p>其中 <span>$\boldsymbol{W}=[\boldsymbol{w}_1\cdots,\boldsymbol{w}_C]$</span> 是由 C 个类的权重向量组成的矩阵，<span>$\boldsymbol{1}^\top_C$</span> 为 C 维的全 1 向量，<span>$\hat{\boldsymbol{y}}\in\mathbb{R}^C$</span> 为所有类别的预测条件概率组成的向量，第 c 维的值是第 c 类的预测条件概率。</p><p class="math-container">\[\mathcal{L}_{\text{softmax}}=-\frac{1}{N_b}\sum_{i=1}^{N_b}\log \frac{\exp({\boldsymbol{w}_{y_i}\boldsymbol{x}+b_{y_i}})}{\boldsymbol{1}^\top_C\exp(\boldsymbol{W}^\top \boldsymbol{x}+\boldsymbol{b})},\]</p><p>其中 <span>$\boldsymbol{w}_{y_i}, b_{y_i}$</span> 代表实际标签 <span>$y_i$</span> 对应的权重和偏置。softmax 在训练的时候收敛迅速，但是精确度一般达到 0.9 时就不会再上升。一方面作为分类网络，softmax 不能像 metric learning 一样显式地优化类间和类内距离，所以性能不会特别好；另外，人脸识别的关键在于得到泛化能力强的 feature，与分类能力并不是完全等价的。</p><p><strong>L-Softmax Loss</strong>：</p><p><strong>SphereFace（A-Softmax）</strong>：</p><p><strong>Focal Loss</strong>：</p><p><strong>Triplet Loss</strong>：</p><p><strong>Center Loss</strong>：</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../GAN/">« 生成对抗网络</a><a class="docs-footer-nextpage" href="../NLP/">自然语言处理 »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.18 on <span class="colophon-date" title="Tuesday 31 May 2022 03:58">Tuesday 31 May 2022</span>. Using Julia version 1.6.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
