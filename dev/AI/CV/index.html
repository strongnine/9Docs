<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>计算机视觉 · 9Docs</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">9Docs</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">人工智能</span><ul><li><a class="tocitem" href="../FE/">特征工程</a></li><li><a class="tocitem" href="../ML/">机器学习</a></li><li><a class="tocitem" href="../NN/">神经网络</a></li><li><a class="tocitem" href="../CNN/">卷积神经网络</a></li><li><a class="tocitem" href="../RNN/">循环神经网络</a></li><li><a class="tocitem" href="../GNN/">图神经网络</a></li><li><a class="tocitem" href="../GAN/">生成对抗网络</a></li><li class="is-active"><a class="tocitem" href>计算机视觉</a><ul class="internal"><li><a class="tocitem" href="#目标检测"><span>目标检测</span></a></li><li><a class="tocitem" href="#人脸识别"><span>人脸识别</span></a></li><li><a class="tocitem" href="#光学字符识别"><span>光学字符识别</span></a></li></ul></li><li><a class="tocitem" href="../NLP/">自然语言处理</a></li><li><a class="tocitem" href="../Transformer/">Transformer</a></li></ul></li><li><span class="tocitem">编程语言</span><ul><li><a class="tocitem" href="../../lang/Python/">Python</a></li><li><a class="tocitem" href="../../lang/Cpp/">C++</a></li><li><a class="tocitem" href="../../lang/Julia/">Julia</a></li></ul></li><li><a class="tocitem" href="../../interview/">面试笔试</a></li><li><a class="tocitem" href="../../git/">Git</a></li><li><a class="tocitem" href="../../docker/">Docker</a></li><li><a class="tocitem" href="../../Linux/">Linux</a></li><li><a class="tocitem" href="../../vim/">Vim</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">人工智能</a></li><li class="is-active"><a href>计算机视觉</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>计算机视觉</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/strongnine/9Docs/blob/main/docs/src/AI/CV.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><p>计算机视觉的任务有：图像分类、物体检测、语义分割、文字识别、人脸识别等</p><h2 id="目标检测"><a class="docs-heading-anchor" href="#目标检测">目标检测</a><a id="目标检测-1"></a><a class="docs-heading-anchor-permalink" href="#目标检测" title="Permalink"></a></h2><p>目标检测学习路径：</p><p>理论：要求能够复现经典论文的代码</p><ul><li>图像分类：VGG、Inception、ResNet、MobileNet、SENet</li><li>图像分割：UNet、DeepLab 系列、FCN、SegNet、BiSeNet</li><li>目标检测：YOLOv3、Faster R-CNN</li><li>GAN：GAN、DCGAN、Pix2Pix</li></ul><p>实践：</p><ul><li>数据增强技巧：MixUp、Label Smoothing</li><li>长尾分布（Long-Tail）、渐进式采样（PB-sampling, Progressively-balanced Sampling）</li><li>数据爬取与筛选：常规筛选方法（经典图像处理和分析方法）、高阶筛选方法（model-base，基于内容的筛选）</li><li>语义分割：<ul><li>自动驾驶语义分割：CamVid 数据集，训练 UNet、SegNet；deeplabv3+ 进行模型评估和推理</li><li>人像分割：Portrait 数据集；训练 BiseNet；Dice-Loss、CE Dice Loss、Focal Loss</li><li>数据增强工具：Albumentations</li></ul></li><li>目标检测：<ul><li>YOLOX：Neck、Head、正负样本分配方式</li><li>COCO 数据集：Mosaic、Mixup、Affine 变化等数据增强方法</li><li>轻量级目标检测器：NanoDetPlus</li><li>算法终端部署：OpenVINO</li></ul></li></ul><h3 id="基础概念"><a class="docs-heading-anchor" href="#基础概念">基础概念</a><a id="基础概念-1"></a><a class="docs-heading-anchor-permalink" href="#基础概念" title="Permalink"></a></h3><p><strong>计算特征图大小</strong>：计算经过卷积、池化等操作之后的特征图大小，这是一个十分常见的考题。假设特征图的输入尺寸为 <span>$l_i$</span>，Padding 大小为 <span>$p$</span>，卷积核或者池化核大小为 <span>$k$</span>，步长为 <span>$s$</span>，那么特征图的输出尺寸 <span>$l_o$</span> 计算公式为：</p><p class="math-container">\[l_o=\lfloor\frac{l_i+2p-k}{s}\rfloor+1,\]</p><p>其中 <span>$\lfloor\cdot\rfloor$</span> 代表向下取整。很多深度学习框架会采取向下取整的方式，放弃输入特征图的一部分边界数据。</p><p><strong>计算感受野</strong>：假设网络的原始输入特征图的尺寸为 <span>$L$</span>，第 i 层卷积核（池化核）尺寸 <span>$k_i$</span>，第 j 层的步长为 <span>$s_j$</span>，则第 i 层的感受野大小 <span>$R_i$</span> 计算如下</p><p class="math-container">\[R_i=\min\left( R_{i-1} + (k_i-1)\prod_{j=0}^{i-1}s_j, L \right)\]</p><p>其中对于原始输入层 <span>$R_0=1, s_0=1$</span>. </p><p><strong>目标检测（Object Detection）</strong>是计算机视觉中极为重要的基础问题，是实例分割（Instance Segmentation)、场景理解（Secne Understanding）、目标跟踪（Object Tracking）、图像标注（Image Captioning）等问题的基础。</p><p><strong>目标检测任务：</strong>给定一张图片，将图片中的每个物体识别出来并且提出一个置信度，用矩形方框（Bounding Box）或者不规则的区域标识出来。</p><p>目标检测模型分为单步（one-stage）模型和两步（two-stage）模型两大类。单步模型在计算效率上有优势，两步模型在检测精度上有优势。</p><p><strong>单步模型</strong>是指没有独立地、显式地提取候选区域（region proposal），直接由输入图像得到其中存在的物体的类别和位置信息的模型。例如 OverFeat、SSD（Single Shot multibox-Detector）、YOLO（You Only Look Once） 等模型。</p><p><strong>两步模型</strong>是指有独立的、显式的候选区域提取过程，即先在输入图像上筛选出一些可能存在物体的候选区域，然后针对每个候选区域，判断其是否存在物体，如果存在就给出物体的类别和位置修正信息。例如 R-CNN、SPPNet、Fast R-CNN、Faster R-CNN、R-FCN、Mask R-CNN 等模型。</p><p><strong>交并比（Intersection-over-Union，IoU）</strong>：即两个 Bounding Boxes 之间交集与并集的比值。对于预测 Bounding Box 与 Ground-truth Box 来说，比值越大代表预测的 Bounding Box 结果越好。</p><p>可以学习一下 IoU 的 Python 代码 <a href="https://github.com/humengdoudou/object_detection_mAP/blob/master/IoU_demo.py">IoU_demo.py</a>。</p><pre><code class="language-python hljs"># 这六行短短的代码可以囊括所有 pred bbox 和 gt bbox 之间的关系。包括相交、不相交、各种相交形式等等
ixmin = max(pred_bbox[0], gt_bbox[0])
iymin = max(pred_bbox[1], gt_bbox[1])
ixmax = min(pred_bbox[2], gt_bbox[2])
iymax = min(pred_bbox[3], gt_bbox[3])
iw = np.maximum(ixmax - ixmin + 1., 0.)
ih = np.maximum(iymax - iymin + 1., 0.)

inters = iw * ih  # 交集
uni = ((pred_bbox[2] - pred_bbox[0] + 1.) * (pred_bbox[3] - pred_bbox[1] + 1.) +
           (gt_bbox[2] - gt_bbox[0] + 1.) * (gt_bbox[3] - gt_bbox[1] + 1.) -
           inters)  # 并集 union = S1 + S2 - inters

overlaps = inters / uni  # IoU</code></pre><p><strong>均交并比（Mean Intersection over Union, MIoU）</strong>：MIoU 是语义分割的标准度量，其计算两个集合的交集和并集之比。</p><p class="math-container">\[\text{MIoU}=\frac{1}{k+1}\sum^{k}_{i=0}{\frac{p_{ii}}{\sum_{j=0}^{k}{p_{ij}+\sum_{j=0}^{k}{p_{ji}-p_{ii}}}}}\]</p><p>其中 <span>$p_{ij}$</span> 表示真实值为 <span>$i$</span>，被预测为 <span>$j$</span> 的数量。</p><p><img src="../../assets/CV MIoU Fig 1.png" alt/></p><p>橙色是真实值，蓝色是预测值，中间是两个部分的相交部分。</p><p><strong>均像素精度（Mean Pixel Accuracy, MPA）</strong>：预测正确的部分占整个真实值的比例，或者说真正例占假负例的比例，即面积 3 和面积 1 的比例。</p><p>而 MIoU 就是两个部分交集部分与并集部分的比，越接近 1 证明预测结果越好，最理想的情况是 1. </p><p><strong>非极大值抑制（Non-Maximum Suppression，NMS）</strong>：目标检测过程中在同一个目标的位置上会产生大量的候选框，这些候选框之间可能会有重叠，NMS 的作用就是消除冗余的边界框，找到最佳的目标边界框。NMS 的流程如下：</p><ul><li>步骤 1. 根据置信度得分进行排序；</li><li>步骤 2. 选择置信度最高的边界框添加到最终输出列表中，将其从边界框列表中删除；</li><li>步骤 3. 计算所有边界框的面积；</li><li>步骤 4. 计算置信度最高的边界框与其他候选框的 IoU；</li><li>步骤 5. 删除 IoU 大于给定阈值的边界框；</li><li>步骤 6. 重复上述过程，直到边界框列表为空；</li></ul><p>NMS 中的阈值给得越大，则越有可能出现同一个物体有多个边界框的情况。步骤 4 中如果置信度最高的边界框与其他候选框的 IoU 比较大的话，就可以认为这两个边界框中是同一个物体，因此只要留下最大的那一个，把其他的删除了。</p><p>代码：</p><pre><code class="language-python hljs">out = net(x)  # forward pass, 将图像 x 输入网络，得到 pred cls + reg
boxes, scores = detector.forward(out, priors)  # 结合 priors，将 pred reg（即预测的 offsets）解码成最终的 pred bbox
boxes = boxes[0]
scores = scores[0]

# scale each detection back up to the image
boxes *= sca;e  # (0, 1) 区间坐标的 bbox 做尺度反正则化
boxes = boxes.cpu().numpy()
scores = scores.cpu().numpy()

for	j in range(1, num_classes):  # 对每一个类 j 的 pred bbox 单独做 NMS
    # 因为第 0 类是 background，不用做 NMS，因此 index 从 1 开始
    inds = np.where(scores[:, j] &gt; thresh)[0]  # 找到该类 j 下，所有 cls score 大于 thresh 的 bbox
    # score 小于阈值的 bbox 直接过滤掉，不用进行 NMS
    if len(inds) == 0:  # 没有满足条件的 bbox，返回空，跳过
        all_boxes[j][i] = np.empty([0, 5], dtype=np.float32)
        continue
    c_bboxes = boxes[inds]
    c_scores = scores[inds, j]  # 找到对应类 j 下的 score 即可
    c_dets = np.hstack((c_bboxes, c_scores[:, np.newaxis])).astype(np.float32, copy=False)  # 将满足条件的 bbox + cls score 的 bbox 通过 hstack 完成合体
    
    keep = nms(c_dets, 0.45, force_cpu=args.cpu)  # NMS，返回需要保存的 bbox index: keep
    c_dets = c_dets[keep, :]
    all_boxes[j][i] = c_dets  # i 对应每张图片，j 对应图像中类别 j 的 bbox 清单</code></pre><p><strong>True Positive (TP)</strong>：<span>$\text{IoU} &gt; 0.5$</span> 的检测框数量（同一个 Ground Truth 只计算一次）</p><p><strong>False Positive (FP)</strong>：<span>$\text{IoU}\le 0.5$</span> 的检测框（检测到同一个 Ground Truth 的多余检测框的数量</p><p><strong>False Negative (FN)</strong>：没有检测到的 Ground Truth 的数量</p><p><strong>True Negative (TN)</strong>：在 mAP 评价指标中不会使用到</p><p><strong>查准率（Precision）</strong>：</p><p class="math-container">\[\text{Precision} = \frac{\text{TP}}{(\text{TP} + \text{FP})} = \frac{\text{FP}}{\text{all detections}}\]</p><p><strong>查全率、查全率（Recall）</strong>：</p><p class="math-container">\[\text{Recall} = \frac{\text{TP}}{(\text{TP}+\text{FN})}=\frac{\text{TP}}{\text{all ground truths}}\]</p><p><strong>PR 曲线（Precision-Recall Curve）</strong>：</p><p><strong>平均精确度（Average Precision）</strong>：PR 曲线下面积</p><p><strong>mAP（mean Average Precison）</strong>：各类别 AP 的平均值。在 VOC2010 以前（VOC07），只要选择当 <span>$\text{Recall}\ge 0, 0.1, 0.2, \dots, 1$</span> 共 11 个点时的 Precision 最大值，然后 AP 就是这 11 个 Precision 的平均值；在 VOC2010 开始，需要针对每一个不同的 Recall 值（包括 0 和 1），选取其大于等于这些 Recall 值时的 Precision 最大值，然后计算 PR 曲线下面积作为 AP 值。</p><blockquote><p>通常 VOC10 标准下计算的 mAP 值会高于 VOC07，原因如下：</p><p><strong>插值平均精度（Interpolated Average Precision）</strong>：一些作者选择了另一种近似值，称为插值平均精度。 通常，他们仍然称其为平均精度。 这种方法不使用 <span>$P(k)$</span>，在 <span>$k$</span> 个图像的截止处的精度，插值平均精度使用：</p><p class="math-container">\[\max_{\tilde{k}\ge k}P(\tilde{k})\]</p><p>换句话说，插值平均精度不是使用在截止 <span>$k$</span> 处实际观察到的精度，而是使用在所有具有更高召回率的截止上观察到的最大精度。计算插值平均精度的完整方程为：</p><p class="math-container">\[\sum_{k=1}^{N}\max_{\tilde{k}\ge k}P(\tilde{k})\Delta r(k)\]</p><p><strong>近似平均精度（Approximated Average）</strong>与实际观察到的曲线非常接近。 插值平均精度高估了许多点的精度，并产生比近似平均精度更高的平均精度值。</p><p>此外，在计算插值平均精度时，在何处采集样本存在差异。 有些人在从 0 到 1 的固定 11 个点采样：<span>$\{0, 0.1, 0.2, ..., 0.9, 1.0\}$</span>。 这称为 11 点插值平均精度。 其他人在召回率发生变化的每个 <span>$k$</span> 处采样。</p></blockquote><h3 id="目标检测历史"><a class="docs-heading-anchor" href="#目标检测历史">目标检测历史</a><a id="目标检测历史-1"></a><a class="docs-heading-anchor-permalink" href="#目标检测历史" title="Permalink"></a></h3><p><img src="../../assets/目标检测模型发展历程.png" alt/></p><p>在一开始的 CNNs 上，把一张图片划分成为固定的区域，然后识别这些区域中是否有某个物体，有的话就把这个区域标识出来。但是在实际中，图片上的物体大小是不固定的，用这种固定大小的区域去识别物体显然是不合理的。人们想到，如果想要让框更加合适，可以增加框的数量，然后让每个区域都变得尽可能地小。但是这样框太多的时候，又会导致计算量的增加。</p><h4 id="R-CNN"><a class="docs-heading-anchor" href="#R-CNN">R-CNN</a><a id="R-CNN-1"></a><a class="docs-heading-anchor-permalink" href="#R-CNN" title="Permalink"></a></h4><p><strong>基于区域的卷积神经网络（Region-based CNN，R-CNN）</strong>出现于 2014 年，是第一个将 CNN 用于目标检测的深度学习模型。它是是解决这种缺点的更好方法，它使用生成区域建议的方式来选择区域。R-CNN 的选框方式是根据选择性搜索来进行的，选框也叫做区域（regions）。</p><ol><li><p>首先使用无监督的<strong>选择性搜索（Selective Serch，SS）</strong>方法将图像中具有相似颜色直方图特征的区域进行合并，产生 2000 个大小不一样的候选区域。这个最后合成的区域就是物体在图片中的位置，即<strong>感兴趣区域（Region of Interest，RoI）</strong>；</p></li><li><p>然后从输入图像中截取这些候选区域对应的图像，将其裁剪缩放 reshape 至合适的尺寸，并相继送入一个 CNN 特征提取网络进行高层次的特征提取；</p></li><li><p>提取出的特征再被送入一个<strong>支持向量机（Support Vector Machine，SVM）</strong>来对这些区域进行分类，以及一个线性回归器进行边界框位置和大小的修正，即<strong>边界框回归（Bounding Box Regression）</strong>；</p></li><li><p>最后对检测结果进行<strong>非极大值抑制（Non-Maximum Suppression，NMS）</strong>，得到最终的检测结果；</p></li></ol><p>R-CNN 的不足：</p><ul><li>每一张图片都会生成很多个 RoI；</li><li>整个过程用了三个模型：特征提取的 CNN、物体分类的 SVM、预测边界框的回归模型，让 R-CNN 变得非常慢，预测一张图片要几十秒；</li></ul><p><strong>选择性搜索（Selective Serch，SS）</strong>：一个物体会包括四种信息：不同的尺度、颜色、纹理和边界，选择性搜索目标就是识别这些模式，提出不同的区域。首先，先生成最初的分割得很细的子分割，然后再将这些很细的小区域按照颜色相似度、纹理相似度、大小相似度和形状相似兼容性来合并成更大的感兴趣区域 RoI。</p><h4 id="SPPNet"><a class="docs-heading-anchor" href="#SPPNet">SPPNet</a><a id="SPPNet-1"></a><a class="docs-heading-anchor-permalink" href="#SPPNet" title="Permalink"></a></h4><p>SPPNet 出现于 2015 年，</p><h4 id="Fast-R-CNN"><a class="docs-heading-anchor" href="#Fast-R-CNN">Fast R-CNN</a><a id="Fast-R-CNN-1"></a><a class="docs-heading-anchor-permalink" href="#Fast-R-CNN" title="Permalink"></a></h4><p><strong>Fast R-CNN</strong> 出现于 2015 年，它添加了一个 <strong>RoI 池化层（RoI Pooling Layer）</strong>来把所有的建议区域转换成适合的尺寸，输入到后面的<strong>全连接层（Fully Connection）</strong>。Fast R-CNN 将 R-CNN 的三个独立的模型集合到一个模型中，因为减少了很多的计算量，Fast R-CNN 在时间花费大大地减少了。</p><p>具体步骤为：</p><ol><li>图片通过 CNN 得到 RoI，然后 RoI 池化层将 RoI 改变成相同的尺寸；</li><li>再将这些区域输入到全连接层上进行分类，同时使用 softmax 和<strong>线性回归层（Linear Regression Layers）</strong>来输出 Bounding Boxes；</li></ol><p><strong>RoI 池化层（RoI Pooling Layer）</strong>：目的是对非均匀尺寸的输入执行最大池化以获得固定尺寸的特征图。RoI 池化层的原型是何凯明提出的空间金字塔池化（Spatial Pyramid Pooling），RoI 池化是 SPP 只使用其中一层的特殊情况。</p><ul><li>RoI Pooling 接收卷积特征图作为输入；</li><li>将 RoI 分割为 <span>$H\times W$</span> 个网格（论文中为 <span>$7\times 7$</span>），对每一个网格都进行 max pooling 得到最终 <span>$H\times W$</span> 大小的特征图；</li></ul><p>下面是 RoI Pooling 的一个 GIF 示例：</p><p><img src="https://deepsense.ai/wp-content/uploads/2017/02/roi_pooling-1.gif" alt/></p><p>Fast R-CNN 的优势和不足：</p><ul><li>依然在使用选择性搜索来作为寻找 RoI 的方法，虽然速度提高了，但是一张图片依旧需要花费 2 秒的时间。</li></ul><h4 id="R-FCN"><a class="docs-heading-anchor" href="#R-FCN">R-FCN</a><a id="R-FCN-1"></a><a class="docs-heading-anchor-permalink" href="#R-FCN" title="Permalink"></a></h4><p>R-FCN 出现于 2016 年</p><h4 id="SSD"><a class="docs-heading-anchor" href="#SSD">SSD</a><a id="SSD-1"></a><a class="docs-heading-anchor-permalink" href="#SSD" title="Permalink"></a></h4><p>SSD 出现于 2016 年</p><h4 id="YOLO"><a class="docs-heading-anchor" href="#YOLO">YOLO</a><a id="YOLO-1"></a><a class="docs-heading-anchor-permalink" href="#YOLO" title="Permalink"></a></h4><p>YOLO 出现于 2016 年</p><h4 id="Faster-R-CNN"><a class="docs-heading-anchor" href="#Faster-R-CNN">Faster R-CNN</a><a id="Faster-R-CNN-1"></a><a class="docs-heading-anchor-permalink" href="#Faster-R-CNN" title="Permalink"></a></h4><p><strong>Faster R-CNN</strong> 出现于 2017 年，它使用一个<strong>区域建议网络（Region Proposal Network，RPN）</strong>来获得比 Fast R-CNN 更高的效率。RPN 将图片特征 map 作为输入，生成一系列带目标分数的建议，也就是告诉网络给出的区域有物体的可能性有多大，分数越高代表包含了物体的可能性越高。</p><p>具体步骤：</p><ol><li><p>把图片作为输入放进卷积网络中去，返回的是一个特征映射（feature map）；</p></li><li><p>RPN 处理这些 map，返回带分数的物体建议；</p></li><li><p>接下来的 RoI pooling 把这些建议都 reshape 成相同的尺寸；</p></li><li><p>最后，放到含有 softmax 层和线性回归层的全连接层上，来分类和输出 bounding boxes。</p></li></ol><p>RPN 被集成在了网络里面，等于从区域建议到最后的分类回归都在同一个网络，实现了端到端。即我们给这个网络输入一张图片，网络就会输出 bounding boxes 和分数。</p><p><strong>区域建议网络（Region Proposal Network，RPN）</strong>：可以输入任何大小的图片（或者特征映射图），然后输出一系列目标建议矩形框，每个矩形框都会有一个对应的分数，代表这个框里面有多大的概率是一个物体。在 Faster R-CNN 中 RPN 是一个全卷积网络（Fully-Convolutional Network，FCN）</p><p><img src="../../assets/Faster R-CNN RPN.png" alt/></p><blockquote><p>图中的数据，在原论文中的具体值为：<span>$C_2=256 \text{ or } 512$</span>，<span>$H=W=16$</span>，<span>$k=9$</span>. </p></blockquote><p>RPN 实际上可以看成是一个小型的 CNN，原文说的是它在 feature map 上使用一个大小为 <span>$n\times n$</span> 的滑动窗口（sliding window），在 Faster R-CNN 论文里 <span>$n=3$</span>：</p><ul><li><strong>步骤 1</strong>：实际上 RPN 就是一个 <span>$3\times 3$</span> 的卷积层，将维数（或者说通道数 Channel）为 <span>$C_1$</span> 的特征图 1 映射成维度为 <span>$C_2$</span> 的特征图 2（在 Faster R-CNN 论文中，在使用 ZF 模型时 <span>$C_2=256$</span>，在使用 VGG 模型时 <span>$C_2=512$</span>）；</li><li><strong>步骤 2</strong>：这个特征图 2 会分别进入两个 <span>$1\times 1$</span>  卷积层，一个做矩形框分类（判断是否为物体），对应特征图 3-1，另一个做矩形框回归，对应特征图 3-2。<span>$1\times 1$</span> 卷积的作用是压缩通道数（Channel），图中用于矩形框分类的特征图 3-1 通道数变为 <span>$2k$</span>，用于矩形框回归的特征图 3-2 通道数变为 <span>$4k$</span>，这里的 <span>$k$</span> 是 anchor boxes 的数量（在论文里取 <span>$k=9$</span>）。分类部分的维度为 2，分别表示框出的部分为「目标」与「非目标」的概率；回归部分的维度为 4，分别表征不同 anchor boxes 对 groud-truth 的长、宽、X 坐标、Y 坐标的预测；</li><li>在训练的时候，只有 RPN 输出的区域建议与 groud-truth 的 <span>$\text{IoU}&gt;0.7$</span> 的 anchor boxes 与 groud-truth 的位置大小误差才会对最终的损失 <span>$\mathcal{Loss}$</span> 有贡献。</li><li>对于特征图 1 中的每一个 <span>$n\times n$</span> 的滑动窗口， RPN 输出 <span>$k$</span> 个区域建议，这 <span>$k$</span> 区域建议都是由 <span>$k$</span> 个 anchor boxes 作为基准调整得到的。特征图 1 中的每一个点都可以对应到原图的某个点，这个点称为锚点（anchor）。</li><li>在论文中，对于每一个 anchor，以其为中心选择 9 个不同大小和不同长宽比的 anchor boxes，具体为 <span>$128^2, 256^2, 512^2$</span> 三种尺度，每个尺度按 <span>$1:1, 1:2, 2:1$</span> 的 3 种长宽比例进行缩放，因此一共有 9 个。</li><li>实际上 RPN 并不是直接预测最终的区域建议，而是调节所有的 anchor boxes 并且经过非极大值抑制得到最终的区域建议。对于一个大小为 <span>$H\times W$</span> 的特征图，会有 <span>$kHW$</span> 个 anchor boxes。</li><li>对于每个 anchor，如果满足两种情况：（1）与 ground-truth box 有最大的 IoU（并不一定会大于 0.7）；（2）与 ground-truth 的 IoU 大于 0.7，那么给其分配正标签，表示预测的效果是好的；如果与 ground-truth 的 IoU 小于 0.3 那么给其分配负标签，表示预测的结果很差。除了这些情况，其他的 anchor 不会对对损失函数有贡献。</li></ul><p>Faster RCNN 的损失函数由 4 个部分组成：</p><ul><li>RPN 分类损失：anchor 是否为 Ground Truth，二类交叉熵损失；</li><li>RPN 位置回归损失：anchor 位置微调，bbox 的第一次修正；</li><li>RoI 分类损失：RoI 所属类别，分类损失；</li><li>RoI 位置回归损失：继续对 RoI 位置微调，第二次对 bbox 的修正；</li></ul><p>最终的损失是这 4 个损失相加。</p><p>对于每一个图片，损失函数为：</p><p class="math-container">\[L\left( \{p_i\},\{t_i\} \right) = \frac{1}{N_{cls}}\sum_iL_{cls}(p_i,p_i^*) + \lambda\frac{1}{N_{reg}}\sum_ip_i^*L_{reg}(t_i,t_i^*).\]</p><p>其中，<span>$i$</span> 是 mini-batch 中 anchor 的索引，<span>$p_i$</span> 是第 <span>$i$</span> 个 anchor 中为物体的预测概率。对于 ground-truth label  <span>$p_i^*$</span>，如果 anchor 是正标签那么其为 1，如果为负标签那么其为 0。<span>$t_i$</span> 为一个向量，表示所预测的边界框（Bounding Box）的 4 个坐标，<span>$t_i^*$</span> 表示正标签 anchor 所对应的 ground-truth box 的坐标。分类损失 <span>$L_{cls}$</span> 是两个类别（「目标」与「非目标」）的对数损失（Log Loss）；回归损失 <span>$L_{reg}(t_i,t_i^*)=R(t_i,t_i^*)$</span>. </p><p>Faster R-CNN 的优势和不足：</p><ul><li>通过使用端到端的方式去进行，并且也不会考虑所有的 RoI，处理一张图片只需要 0.2 秒。</li></ul><h4 id="Light-Head-R-CNN"><a class="docs-heading-anchor" href="#Light-Head-R-CNN">Light-Head R-CNN</a><a id="Light-Head-R-CNN-1"></a><a class="docs-heading-anchor-permalink" href="#Light-Head-R-CNN" title="Permalink"></a></h4><p>Light-Head R-CNN 出现于 2017 年</p><h4 id="Mask-R-CNN"><a class="docs-heading-anchor" href="#Mask-R-CNN">Mask R-CNN</a><a id="Mask-R-CNN-1"></a><a class="docs-heading-anchor-permalink" href="#Mask-R-CNN" title="Permalink"></a></h4><p>Mask R-CNN 出现于 2017 年</p><h4 id="FPN"><a class="docs-heading-anchor" href="#FPN">FPN</a><a id="FPN-1"></a><a class="docs-heading-anchor-permalink" href="#FPN" title="Permalink"></a></h4><p><strong>特征金字塔网络（Feature Pyramid Networks，FPN）</strong>：低层的特征语义信息比较少，但是目标位置准确；高层的特征语义信息比较丰富，但是目标位置比较粗略。有些算法采用多尺度特征融合的方法，但是一般是采用融合后的特征做预测。这篇文章创新的点在于预测是在不同特征层独立进行的。</p><p><img src="../../assets/(2017 FPN) Fig 1.png" alt/></p><p>论文中的图 1 展示了 4 种利用特征的方式：</p><ul><li>图像金字塔（Featurized image pyramid）：将图像 reshape 为不同的尺度，不同尺度的图像生成对应不同尺度的特征。这种方式缺点在于增加了时间成本；</li><li>单个特征图（Single feature map）：像 SPPNet、Fast R-CNN、Faster R-CNN 等模型采用的方式，只使用最后一层的特征图；</li><li>金字塔特征层次结构（Pyramidal feature hierarchy）：像 SSD 模型采用多尺度特征融合的方式，没有上采样的过程，从网络不同层抽取不同尺度的特征做预测。优点在于不会增加额外的计算量；缺点在于 SSD 没有用到足够底层的特征（SSD 中最底层的特征是 VGG 网络的 Conv4_3）；</li><li>特征金字塔网络（Feature Pyramid Network）：顶层特征通过上采样和低层特征做融合，每层独立预测；</li></ul><p><img src="../../assets/(2017 FPN) Fig 2.png" alt/></p><p>论文的图 2 展示的是两种不同的金字塔结构，上面的结构将最顶部最小的特征图进行上采样之后与前面阶段的特征图相融合，最终只在最底层最大的特征图（自顶向下的最后一层也可以叫做 Finest Level）上进行预测。下面的结构预测是在每一层中独立进行的。</p><p>论文的算法结构如图 3 所示，其结构包括一个<strong>自底向上的路径（bottom-up pathway）</strong>、<strong>自顶向下的路径（top-down pathway）</strong>以及<strong>横向连接（lateral connections）</strong>，<span>$1\times 1$</span> 卷积层的主要作用是减少卷积核的个数。</p><p><img src="../../assets/(2017 FPN) Fig 3.png" alt/></p><ul><li>Bottom-Up Pathway 是网络的前向过程。论文将不改变 feature map 大小的层视为在同一个网络阶段（stage），每次抽取出来的 feature map 都是每个 stage 的最后一层输出，因为最后一层的特征是最强的，每个阶段的 feature map 记为 <span>$\{C_2, C_3, C_4, C_5\}$</span>。</li><li>Top-Down Pathway 过程采用上采样（Upsampling）进行，生成的 feature map 记为 <span>$\{P_2, P_3, P_4, P_5\}$</span>。</li><li>Lateral Connections 是将上采样的结果和自底向上生成的相同大小的 feature map 进行融合（merge）。</li><li>在融合过后会使用 <span>$3\times 3$</span> 卷积对每个融合结果进行卷积，以消除上采样的混叠效应（Aliasing Effect）。</li></ul><p>将 FPN 用于 RPN 网络中生成 Region Proposal，在每一个 stage 都定义了不同大小的 anchor，对于 <span>$\{P_2, P_3, P_4, P_5, P_6\}$</span> 分别为 <span>$\{32^2, 64^2, 128^2, 256^2, 512^2\}$</span>，每种尺度的 anchor 有不同的比例 <span>$1:2, 1:1, 2:1$</span>，整个特征金字塔有 15 种 anchors。</p><h4 id="RetinaNet-(Focal-Loss)"><a class="docs-heading-anchor" href="#RetinaNet-(Focal-Loss)">RetinaNet (Focal Loss)</a><a id="RetinaNet-(Focal-Loss)-1"></a><a class="docs-heading-anchor-permalink" href="#RetinaNet-(Focal-Loss)" title="Permalink"></a></h4><blockquote><p>推荐阅读：</p><ul><li><a href="https://mmdetection.readthedocs.io/en/latest/">MMDetection</a>；</li><li><a href="https://github.com/open-mmlab/mmdetection">GitHub：MMDetection</a>；</li></ul></blockquote><p>何凯明在 ICCV2017 上的新作 Focal Loss for Dense Object Detection 提出了一个一个新的损失函数 —— Focal Loss，主要用于解决在单阶段目标检测场景上训练时前景（foreground）和背景（background）类别极端失衡（比如 1:1000）的问题。Focal Loss 可以抑制负样本对最终损失的贡献以提升网络的整体表现。</p><blockquote><p>将不含有待检测物体的区域称为负样本，含有待检测物体的区域称为正样本。</p></blockquote><p>Focal Loss 的最终形式为：</p><p class="math-container">\[\text{FL}(p_t)=-\alpha_t(1-p_t)^\gamma\log(p_t).\]</p><p>演变过程如下，一般来说，对于二分类问题，交叉熵损失为：</p><p class="math-container">\[\text{CE}(p,y)=\begin{cases}-\log(p),\qquad \text{if } y=1\\ -\log(1-p),\quad \text{otherwise}. \end{cases}\]</p><p>其中 <span>$y\in\{\pm 1\}$</span> 是类别标签，<span>$p\in[0, 1]$</span> 是模型对于样本类别属于 <span>$y=1$</span> 的预测概率，定义</p><p class="math-container">\[p_t\begin{cases}p, \qquad \text{if } y=1,\\ 1-p, \quad \text{otherwise}. \end{cases}\]</p><p>因此交叉熵损失可以重写为 <span>$\text{CE}(p,y)=\text{CE}(p_t)=-\log(p_t).$</span></p><p>这里应该区分的一个点是：难易样本不平衡和正负样本不平衡，Focal Loss 主要是在解决难易样本不平衡的问题上。一般解决正负样本不平衡的问题，会在交叉熵损失前面加上一个参数 <span>$\alpha$</span> 得到</p><p class="math-container">\[\text{CE}(p_t)=-\alpha_t\log(p_t),\]</p><p>只是这样的方案只能解决正负样本不平衡的问题，至于难易样本不平衡，Focal Loss 的思想就是降低高置信度样本的损失：</p><p class="math-container">\[\text{FL}(p_t)=-(1-p_t)^\gamma\log(p_t).\]</p><p>假设 <span>$\gamma=2$</span> 时，如果样本置信度为 <span>$p=0.968$</span>，那么 <span>$(1-0.968)^2\approx 0.001$</span> 就可以将这个高置信度样本的损失衰减 1000 倍。</p><p>将增加参数 <span>$\alpha$</span> 添加到 Focal Loss 上就可以同时解决正负以及难易样本不平衡的问题，最终 Focal Loss 的形式为：</p><p class="math-container">\[\text{FL}(p_t)=-\alpha_t(1-p_t)^\gamma\log(p_t).\]</p><p>在 <a href="https://mmdetection.readthedocs.io/en/latest/_modules/mmdet/models/losses/focal_loss.html">MMDetection</a> 的 <a href="https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/losses/focal_loss.py">GItHub 开源代码</a>中可以看到对于 Focal Loss 实现的 Python 代码，实际上真正使用的是 CUDA 版本代码，因此这里给出的代码只是供人学习的。</p><p>Focal Loss 存在的问题：</p><ul><li>模型过多地关注那些特别难分的样本 —— 离群点（outliers），即便是模型已经收敛了，但是这些离群点依旧是</li></ul><h4 id="SENet"><a class="docs-heading-anchor" href="#SENet">SENet</a><a id="SENet-1"></a><a class="docs-heading-anchor-permalink" href="#SENet" title="Permalink"></a></h4><p>目标检测中的注意力机制</p><h4 id="FCOS"><a class="docs-heading-anchor" href="#FCOS">FCOS</a><a id="FCOS-1"></a><a class="docs-heading-anchor-permalink" href="#FCOS" title="Permalink"></a></h4><p>发表于 ICCV2019 的论文：FCOS: Fully Convolutional One-Stage Object Detection 提出了 FCOS，与 YOLO 类似，它直接将 backbone 输出的 feature map 上的每一个像素当做预测起点，即把每一个位置都当做训练样本，只要该位置落入某个 Ground-Truth 框，就将其当做正样本进行训练。为了让一个目标在推理时不会在多个 feature map 上被重复输出，认为限制了每一层回归目标的尺度大小，超过该限制的目标，这一层就不检测。</p><p>论文中图 2 展示了 FCOS 的具体结构：</p><p><img src="../../assets/(2019 FCOS) Fig2.png" alt/></p><p>FCOS 在检测头增加一个<strong>中心度（Centerness）</strong>分支，保证回归框的中心和 GT 较为接近，同时和 FPN 结合，在每一层上只回归特定大小的目标，从而将不同尺度的目标分配到对应层级上</p><p class="math-container">\[\text{centerness}^*=\sqrt{\frac{\min(l^*,r^*)}{\max(l^*,r^*)}\times \frac{\min(t^*,b^*)}{\max(t^*,b^*)}},\]</p><p>其中 </p><h4 id="CenterNet"><a class="docs-heading-anchor" href="#CenterNet">CenterNet</a><a id="CenterNet-1"></a><a class="docs-heading-anchor-permalink" href="#CenterNet" title="Permalink"></a></h4><h4 id="ATSS"><a class="docs-heading-anchor" href="#ATSS">ATSS</a><a id="ATSS-1"></a><a class="docs-heading-anchor-permalink" href="#ATSS" title="Permalink"></a></h4><p>论文 Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection 中提出了一种根据目标的统计信息自动选择正负样本的<strong>自适应样本选择机制（Adaptive Training Sample Selection，ATSS）</strong>。</p><p>论文里提到无论是 anchor-based 方法还是 anchor-free 方法，</p><h4 id="GFL"><a class="docs-heading-anchor" href="#GFL">GFL</a><a id="GFL-1"></a><a class="docs-heading-anchor-permalink" href="#GFL" title="Permalink"></a></h4><blockquote><p>推荐阅读：</p><ul><li>GFL 作者本人 李翔 的文章 <a href="https://zhuanlan.zhihu.com/p/147691786">知乎：大白话 Generalized Focal Loss</a>；</li></ul></blockquote><p>论文 Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection 所提出的 <strong>广义焦点损失（Generalized Focal Loss，GFL）</strong>的具体形式如下：</p><p class="math-container">\[\text{GFL}(p_{y_l}, p_{y_r})=-|y-(y_l p_{y_l}+y_rp_{y_r})|^\beta\left( (y_r-y)\log(p_{y_l}) + (y-y_l)\log(p_{{y_r}})\right).\]</p><p>GFL 主要解决两个问题：</p><ul><li>在训练和推理的时候，分类和质量估计的不一致性；</li><li>狄拉克分布针对复杂场景下（模糊和不确定性边界）存在不灵活的问题；</li></ul><p>解决这两个问题的方式是设计新的「表示」方法：</p><ul><li>通过联合质量估计和分类设计新的 Loss；</li><li>定义一种新的边界框表示方式来进行回归；</li></ul><p>GFL 工作的核心是围绕<strong>表示（representation）</strong>的改进去进行的，表示具体是指检测器最终的输出，也就是 head 末端的物理对象，以 FCOS、ATSS 为代表的 one-stage anchor-free 检测器基本会包含 3 个表示：</p><ul><li>分类表示；</li><li>检测框表示；</li><li>检测框的质量估计。在 FCOS、ATSS 中采用 centerness，一些其他的工作会采用 IoU，这些 score 基本都在 0 到 1 之间；</li></ul><p>现有的表示主要存在的问题：</p><ul><li>classification score 和 IoU / centerness score 训练测试不一致，具体有：<ul><li>用法不一致。训练的时候，分类和质量估计是分开训练的，但是在测试的时候又是乘在一起作为 NMS score 排序的依据；</li><li>对象不一致。质量估计通常只针对正样本训练，对于 one-stage 检测器而言，在做 NMS score 排序的时候，所有的样本都会将分类 score 和质量预测 score 相乘用于排序，这样会引发一个情况：一个分类 score 相对低的真正负样本，由于预测了一个不可信的高质量 score，导致到它可能排到一个分类 score 不高并且质量 score 较低的真正的正样本的前面；</li></ul></li><li>Bounding Box 回归采用的表示不够灵活，没有办法建模复杂场景下的 uncertainty；</li></ul><p>Focal Loss 支持 0 或者 1 类型的离散 label，而对于分类 - 质量联合表示，label 是 0～1 之间的连续值。如果对 Focal Loss 在连续 label 上进行拓展，就可以使其即保证平衡正负难易样本的特性，又支持连续数值的监督，因此得到 Quality Focal Loss（QFL），具体形式如下：</p><p class="math-container">\[\text{QFL}(\sigma)=-|y-\sigma|^\beta\left((1-y)\log(1-\sigma)+y\log(\sigma) \right),\]</p><p>其中 <span>$y$</span> 为 0～1 的质量标签，<span>$\sigma$</span> 为预测，QFL 的全局最小解是 <span>$\sigma=y$</span>。之后又增加了一个称为 Distribution Focal Loss（DFL）的 loss，目的是希望网络能够快速地聚焦到标注位置附近的数值，使得它们概率尽可能大，DFL 的具体形式如下：</p><p class="math-container">\[\text{DFL}(S_i,S_{i+1})=-\left((y_{i+1}-y)\log(S_i)+(y-y_i\log(S_{i+1})) \right).\]</p><p>如果将 QFL 和 DFL 统一起来，就可以表示为 GFL。</p><h2 id="人脸识别"><a class="docs-heading-anchor" href="#人脸识别">人脸识别</a><a id="人脸识别-1"></a><a class="docs-heading-anchor-permalink" href="#人脸识别" title="Permalink"></a></h2><p><strong>人脸识别中的模型 ArcFace……</strong></p><h3 id="人脸识别中的损失函数"><a class="docs-heading-anchor" href="#人脸识别中的损失函数">人脸识别中的损失函数</a><a id="人脸识别中的损失函数-1"></a><a class="docs-heading-anchor-permalink" href="#人脸识别中的损失函数" title="Permalink"></a></h3><p><strong>Softmax Loss</strong>：最常见的人脸识别函数，原理是去掉最后的分类层，作为解特征网络导出解特征向量用于人脸识别。</p><p class="math-container">\[\hat{\boldsymbol{y}}=\text{softmax}(\boldsymbol{W}^\top \boldsymbol{x})=\frac{\exp(\boldsymbol{W}^\top \boldsymbol{x}+\boldsymbol{b})}{\boldsymbol{1}^\top_C\exp(\boldsymbol{W}^\top \boldsymbol{x}+\boldsymbol{b})},\]</p><p>其中 <span>$\boldsymbol{W}=[\boldsymbol{w}_1\cdots,\boldsymbol{w}_C]$</span> 是由 C 个类的权重向量组成的矩阵，<span>$\boldsymbol{1}^\top_C$</span> 为 C 维的全 1 向量，<span>$\hat{\boldsymbol{y}}\in\mathbb{R}^C$</span> 为所有类别的预测条件概率组成的向量，第 c 维的值是第 c 类的预测条件概率。</p><p class="math-container">\[\mathcal{L}_{\text{softmax}}=-\frac{1}{N_b}\sum_{i=1}^{N_b}\log \frac{\exp({\boldsymbol{w}_{y_i}\boldsymbol{x}+b_{y_i}})}{\boldsymbol{1}^\top_C\exp(\boldsymbol{W}^\top \boldsymbol{x}+\boldsymbol{b})},\]</p><p>其中 <span>$\boldsymbol{w}_{y_i}, b_{y_i}$</span> 代表实际标签 <span>$y_i$</span> 对应的权重和偏置。softmax 在训练的时候收敛迅速，但是精确度一般达到 0.9 时就不会再上升。一方面作为分类网络，softmax 不能像 metric learning 一样显式地优化类间和类内距离，所以性能不会特别好；另外，人脸识别的关键在于得到泛化能力强的 feature，与分类能力并不是完全等价的。</p><p><strong>L-Softmax Loss</strong>：</p><p><strong>SphereFace（A-Softmax）</strong>：</p><p><strong>Focal Loss</strong>：</p><p><strong>Triplet Loss</strong>：</p><p>Triplet Loss 是在谷歌 2015 年的 FaceNet 论文中的提出来的，用于解决人脸识别相关的问题，原文为：《FaceNet: A Unified Embedding for Face Recognition and Clustering》。</p><p>Triplet 三元组指的是 anchor, negative, positive 三个部分，每一部分都是一个 embedding 向量，其中</p><ul><li>anchor 指的是基准图片；</li><li>positive 指的是与 anchor 同一分类下的一张图片；</li><li>negative 指的是与 anchor 不同分类的一张图片；</li></ul><p>Triplet Loss 的目的是让 anchor 和 positive 的距离变得越来越小，而与 negative 的距离变得越来越大，损失函数定义如下：</p><p class="math-container">\[\mathcal{L}=\max(d(a, p) - d(a, n) + \text{margin}, 0),\]</p><p>其中 <span>$a, p, n$</span> 分别代表 anchor，positive 和 negative。如果 negative example 很好识别时，anchor 与 negative 的距离会相对较大，即 <span>$d(a,n)$</span> 相比之下偏大，那么损失为 <span>$\mathcal{L}=0$</span>；否则通过最小化损失函数，可以让 anchor 与 positive 的距离 <span>$d(a,p)$</span> 更加接近 0，而与 negative 的距离 <span>$d(a,n)$</span> 更加接近给定的 margin。</p><p>基于 triplet loss 的定义，可以将 triplet（三元组）分为三类：</p><ul><li>easy triplets（简单三元组）：triplet 对应的损失为 0 的三元组：<span>$d(a,n)&gt;d(a,p) + \text{margin}$</span>；</li><li>hard triplets（困难三元组）：negative example 与 anchor 距离小于 anchor 与 positive example 的距离，形式化定义为：<span>$d(a,n)&lt;d(a,p)$</span>；</li><li>semi-hard triplets（一般三元组）：negative example 与 anchor 距离大于 anchor 与 positive example 的距离，但还不至于使得 <span>$\mathcal{L}oss$</span> 为 0，即 <span>$d(a,p)&lt;d(a,n)&lt;d(a,p)+\text{margin}$</span>；</li></ul><p>PyTorch 实现：</p><pre><code class="language-python hljs">class TripletLoss(nn.Module):
    &quot;&quot;&quot;Triplet loss with hard positive/negative mining.
    
    Reference:
        Hermans et al. In Defense of the Triplet Loss for Person Re-Identification. arXiv:1703.07737.
    
    Imported from `&lt;https://github.com/Cysu/open-reid/blob/master/reid/loss/triplet.py&gt;`_.
    
    Args:
        margin (float, optional): margin for triplet. Default is 0.3.
    &quot;&quot;&quot;
    
    def __init__(self, margin=0.3,global_feat, labels):
        super(TripletLoss, self).__init__()
        self.margin = margin
        self.ranking_loss = nn.MarginRankingLoss(margin=margin)
 
    def forward(self, inputs, targets):
        &quot;&quot;&quot;
        Args:
            inputs (torch.Tensor): feature matrix with shape (batch_size, feat_dim).
            targets (torch.LongTensor): ground truth labels with shape (num_classes).
        &quot;&quot;&quot;
        n = inputs.size(0)
        
        # Compute pairwise distance, replace by the official when merged
        dist = torch.pow(inputs, 2).sum(dim=1, keepdim=True).expand(n, n)
        dist = dist + dist.t()
        dist.addmm_(1, -2, inputs, inputs.t())
        dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability
        
        # For each anchor, find the hardest positive and negative
        mask = targets.expand(n, n).eq(targets.expand(n, n).t())
        dist_ap, dist_an = [], []
        for i in range(n):
            dist_ap.append(dist[i][mask[i]].max().unsqueeze(0))
            dist_an.append(dist[i][mask[i] == 0].min().unsqueeze(0))
        dist_ap = torch.cat(dist_ap)
        dist_an = torch.cat(dist_an)
        
        # Compute ranking hinge loss
        y = torch.ones_like(dist_an)
        return self.ranking_loss(dist_an, dist_ap, y)</code></pre><p><strong>Center Loss</strong>：</p><p>在 Triplet Loss 之后又提出了一个 Center Loss。Triplet 学习的是样本间的相对距离，没有学习绝对距离，尽管考虑了类间的离散性，但没有考虑类内的紧凑性。Center Loss 希望可以通过学习每个类的类中心，使得类内的距离变得更加紧凑，其公式如下：</p><p class="math-container">\[\mathcal{L}_C=\frac{1}{2}\sum_{i=1}^m\| x_i -c_{y_i} \|_2^2,\]</p><p class="math-container">\[c_{y_i}\in\mathbb{R}^d\]</p><p>表示深度特征的第 <span>$y_i$</span> 类中心。理想情况下，<span>$c_{y_i}$</span> 应该随着深度特性的变化而更新。</p><p>训练时：第一是基于mini-batch执行更新。在每次迭代中，计算中心的方法是平均相应类的特征（一些中心可能不会更新）。第二，避免大扰动引起的误标记样本，用一个标量 <span>$\alpha$</span> 控制中心的学习速率，一般这个 <span>$\alpha$</span> 很小（如 0.005）。</p><p>计算 <span>$\mathcal{L}_C$</span> 相对于 <span>$x_i$</span> 的梯度和 <span>$c_{y_i}$</span> 的更新方程为：</p><p class="math-container">\[\frac{\partial \mathcal{L}_C}{\partial x_i}=x_i-c_{y_i},\]</p><p class="math-container">\[\Delta c_j = \frac{\sum_{i=1}^m \delta(y_i=j)\cdot(c_j-x_i)}{1+\sum_{i=1}^m \delta(y_i=j)}.\]</p><pre><code class="language-python hljs">class CenterLoss(nn.Module):
    &quot;&quot;&quot;Center loss.
    Reference:
    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.
    Args:
        num_classes (int): number of classes.
        feat_dim (int): feature dimension.
    &quot;&quot;&quot;
 
    def __init__(self, num_classes=751, feat_dim=2048, use_gpu=True):
        super(CenterLoss, self).__init__()
        self.num_classes = num_classes
        self.feat_dim = feat_dim
        self.use_gpu = use_gpu
 
        if self.use_gpu:
            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())
        else:
            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))
 
    def forward(self, x, labels):
        &quot;&quot;&quot;
        Args:
            x: feature matrix with shape (batch_size, feat_dim).
            labels: ground truth labels with shape (num_classes).
        &quot;&quot;&quot;
        assert x.size(0) == labels.size(0), &quot;features.size(0) is not equal to labels.size(0)&quot;
 
        batch_size = x.size(0)
        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()
        distmat.addmm_(1, -2, x, self.centers.t())
 
        classes = torch.arange(self.num_classes).long()
        if self.use_gpu: classes = classes.cuda()
        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)
        mask = labels.eq(classes.expand(batch_size, self.num_classes))
        print(mask)
 
        dist = []
        for i in range(batch_size):
            print(mask[i])
            value = distmat[i][mask[i]]
            value = value.clamp(min=1e-12, max=1e+12)  # for numerical stability
            dist.append(value)
        dist = torch.cat(dist)
        loss = dist.mean()
        return loss</code></pre><h2 id="光学字符识别"><a class="docs-heading-anchor" href="#光学字符识别">光学字符识别</a><a id="光学字符识别-1"></a><a class="docs-heading-anchor-permalink" href="#光学字符识别" title="Permalink"></a></h2><p><strong>光学字符识别（Optical Character Recognition，OCR）</strong>：挖掘图像中的文本信息，需要对图像中的文字进行检测和识别。OCR 的确切定义是，将包含键入、印刷或场景文本的电子图像转换成机器编码文本的过程。</p><p>OCR 算法通常分为两个基本模块，属于物体检测其中一个子类的<strong>文本检测</strong>以及<strong>文本识别</strong>。</p><p>传统的文本检测：基于二值化的连通区域提取，基于最大极值稳定区域（Maximally Stable Extremal Regions，MSER），方向梯度直方图（Histogram of Oriented Gradient，HOG）可以提取特征；隐马尔可夫模型（Hidden Markov Model，HMM）对最终的词语进行预测。</p><p>文本检测框架的两种类型：</p><ul><li><strong>基于候选框</strong>：在通用物体检测的基础上，通过设置更多不同长宽比的锚框来适应文本变长的特性，以达到文本定位的效果。类似的模型包括：Rosetta、SegLink、TextBoxes++；</li><li><strong>基于像素分割</strong>：首先通过图像语义分割获得可能属于的文本区域的像素，之后通过像素点直接回归或者对文本像素的聚合得到最终的文本定位。类似的模型包括：TextSnake、SPCNet、MaskTextSpotter；</li><li>不同方法的优缺点：<ul><li>基于候选框的文本检测对文本尺度本身不敏感，对小文本的检出率更高；但是对于倾斜角度较大的密集文本块，该方法很容易因为无法适应文本方向的剧烈变化以及对文本的包覆性不够紧密而检测失败。</li><li>基于候选框的检测方法利用整体文本的粗粒度特征，而非像素级别的精细特征，因此其检测精度往往不如基于像素分割的文本检测。</li><li>基于像素分割的文本检测往往具有更好的精确度，但是对于小尺度的文本，因为对应的文本像素过于稀疏，检出率通常不搞，除非以牺牲检测效率为代价对输入图像进行大尺度的放大。</li></ul></li><li><strong>同时基于候选框和像素分割</strong>：将基于候选框的文本检测框架和基于像素分割的文本检测框架结合在一起，共享特征提取部分，并将像素分割的结果转换为候选框检测回归过程中的一种注意力机制，从而使文本检测的准确性和召回率都得到提高，例如云从科技公司提出的 Pixel-Anchor；</li></ul><p>检测文字所在位置（CTPN）和识别文本区域内容（CRNN）</p><h4 id="EAST"><a class="docs-heading-anchor" href="#EAST">EAST</a><a id="EAST-1"></a><a class="docs-heading-anchor-permalink" href="#EAST" title="Permalink"></a></h4><p><strong>一种高效准确的场景文本检测器（An Efficient and Accurate Scene Text Detector，EAST）</strong>：</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../GAN/">« 生成对抗网络</a><a class="docs-footer-nextpage" href="../NLP/">自然语言处理 »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Thursday 1 September 2022 01:43">Thursday 1 September 2022</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
