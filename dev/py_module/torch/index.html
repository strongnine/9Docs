<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>- · 9Docs</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">9Docs</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">人工智能</span><ul><li><a class="tocitem" href="../../AI/FE/">特征工程</a></li><li><a class="tocitem" href="../../AI/ML/">机器学习</a></li><li><a class="tocitem" href="../../AI/NN/">神经网络</a></li><li><a class="tocitem" href="../../AI/CNN/">卷积神经网络</a></li><li><a class="tocitem" href="../../AI/RNN/">循环神经网络</a></li><li><a class="tocitem" href="../../AI/GNN/">图神经网络</a></li><li><a class="tocitem" href="../../AI/GAN/">生成对抗网络</a></li><li><a class="tocitem" href="../../AI/CV/">计算机视觉</a></li><li><a class="tocitem" href="../../AI/NLP/">自然语言处理</a></li><li><a class="tocitem" href="../../AI/Transformer/">Transformer</a></li></ul></li><li><span class="tocitem">编程语言</span><ul><li><a class="tocitem" href="../../lang/Python/">Python</a></li><li><a class="tocitem" href="../../lang/Cpp/">C++</a></li><li><a class="tocitem" href="../../lang/Julia/">Julia</a></li></ul></li><li><span class="tocitem">数据结构与算法</span><ul><li><a class="tocitem" href="../../dataStruc&amp;algo/dataStruc/">数据结构</a></li><li><a class="tocitem" href="../../dataStruc&amp;algo/algo/">算法</a></li><li><a class="tocitem" href="../../dataStruc&amp;algo/leetcode/">LeetCode</a></li><li><a class="tocitem" href="../../dataStruc&amp;algo/interview/">笔试题目</a></li></ul></li><li><a class="tocitem" href="../../git/">Git</a></li><li><a class="tocitem" href="../../docker/">Docker</a></li><li><a class="tocitem" href="../../Linux/">Linux</a></li><li><a class="tocitem" href="../../vim/">Vim</a></li><li><a class="tocitem" href="../../books/">推荐书籍大全</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>-</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>-</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/strongnine/9Docs/blob/main/docs/src/py_module/torch.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h2 id="张量（Tensors）"><a class="docs-heading-anchor" href="#张量（Tensors）">张量（Tensors）</a><a id="张量（Tensors）-1"></a><a class="docs-heading-anchor-permalink" href="#张量（Tensors）" title="Permalink"></a></h2><p>官方文档：<a href="https://pytorch.org/docs/1.12/torch.html#">Torch</a></p><ul><li>张量的构建、索引、切片、拼接、修改、随机采样</li><li>数学操作：逐点操作、规约操作、比较操作、频谱分析、其他操作、BLAS and LAPACK</li></ul><h3 id="张量的构建"><a class="docs-heading-anchor" href="#张量的构建">张量的构建</a><a id="张量的构建-1"></a><a class="docs-heading-anchor-permalink" href="#张量的构建" title="Permalink"></a></h3><p><a href="https://pytorch.org/docs/1.12/generated/torch.tensor.html#torch.tensor"><code>tensor</code></a>：通过复制数据构造一个没有梯度历史的张量（也称为「叶张量」）。</p><blockquote><p>叶张量：在自动微分机制（<a href="https://pytorch.org/docs/1.12/notes/autograd.html">Autograd Mechanics</a>）一节可以了解 PyTorch 是如何实现自动微分的。</p></blockquote><p><a href="https://pytorch.org/docs/1.12/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"><code>sparse_coo_tensor</code></a>：在给定索引处构造一个具有指定值的 COO (rdinate) 格式的稀疏张量。</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.asarray.html#torch.asarray"><code>asarray</code></a>：转化为张量。</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.as_tensor.html#torch.as_tensor"><code>as_tensor</code></a>：将数据转成张量，共享数据以及保留梯度历史。</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.as_strided.html#torch.as_strided"><code>as_strided</code></a>：创建具有指定大小、步幅和 <code>storage_offset</code> 的现有 torch.Tensor 输入的视图（view）。</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.from_numpy.html#torch.from_numpy"><code>from_numpy</code></a>：将 <code>numpy.ndarray</code> 转化为 <code>Tensor</code></p><p><a href="https://pytorch.org/docs/1.12/generated/torch.from_dlpack.html#torch.from_dlpack"><code>from_dlpack</code></a>：将外部库中的张量转换为 <code>torch.Tensor</code></p><p><a href="https://pytorch.org/docs/1.12/generated/torch.frombuffer.html#torch.frombuffer"><code>frombuffer</code></a>：从实现 Python 缓冲区协议的对象创建一维张量</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.zeros.html#torch.zeros"><code>zeros</code></a>、<a href="https://pytorch.org/docs/1.12/generated/torch.ones.html#torch.ones"><code>ones</code></a>：构建给定 <code>size</code> 一样的全 0 张量或者全 1 张量</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.zeros_like.html#torch.zeros_like"><code>zeros_like</code></a>、<a href="https://pytorch.org/docs/1.12/generated/torch.ones_like.html#torch.ones_like"><code>ones_like</code></a>：构建与给定张量一样 <code>size</code> 的全 0 张量或者全 1 张量</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.arange.html#torch.arange"><code>arange</code></a>：在区间 <code>[start, end)</code> 中以固定步长 <code>step</code> 构建一维张量，大小为 <span>$\lceil (\text{end} - \text{start})/\text{step} \rceil$</span></p><p><a href="https://pytorch.org/docs/1.12/generated/torch.range.html#torch.range"><code>range</code></a>：从 <code>start</code> 到 <code>end</code> 以步长 <code>step</code> 构建一维张量，大小为 <span>$\lfloor (\text{end} - \text{start})/\text{step} \rfloor + 1$</span></p><p><a href="https://pytorch.org/docs/1.12/generated/torch.linspace.html#torch.linspace"><code>linspace</code></a>：在  <code>start</code> 到 <code>end</code>  均匀分布的一维张量，大小为 <code>step</code>（包含 <code>end</code> 在内）</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.logspace.html#torch.logspace"><code>logspace</code></a>：在以 <code>base</code> 为底的对数尺度中 <span>$\text{base}^\text{start}$</span> 到 <span>$\text{base}^\text{end}$</span> 均匀分布的一维向量（包含 <span>$\text{base}^\text{end}$</span> 在内）</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.eye.html#torch.eye"><code>eye</code></a>：单位二维向量</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.empty.html#torch.empty"><code>empty</code></a>：构建还未初始化的张量</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.empty_like.html#torch.empty_like"><code>empty_like</code></a>：构建与给定张量一样 <code>size</code> 的未初始化张量</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.empty_strided.html#torch.empty_strided"><code>empty_strided</code></a>：创建一个具有指定大小和步幅并填充未定义数据的张量</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.full.html#torch.full"><code>full</code></a>、<a href="https://pytorch.org/docs/1.12/generated/torch.full_like.html#torch.full_like"><code>full_like</code></a>：构建全为给定值 <code>fill_value</code> 的张量</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.quantize_per_tensor.html#torch.quantize_per_tensor"><code>quantize_per_tensor</code></a>：将浮点张量转换为具有给定比例和零点的量化张量</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"><code>quantize_per_channel</code></a>：将浮点张量转换为具有给定比例和零点的每通道量化张量</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.dequantize.html#torch.dequantize"><code>dequantize</code></a>：通过反量化量化张量返回 <code>float32</code> 张量</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.complex.html#torch.complex"><code>complex</code></a>：构造一个复数张量，其实部等于 <code>real</code>，虚部等于 <code>imag</code></p><p><a href="https://pytorch.org/docs/1.12/generated/torch.polar.html#torch.polar"><code>polar</code></a>：构造一个复数张量，其元素是笛卡尔坐标，对应于绝对值 <code>abs</code> 和角度 <code>angle</code> 的极坐标</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.heaviside.html#torch.heaviside"><code>heaviside</code></a>：计算输入中每个元素的 <code>Heaviside</code> 阶跃函数</p><h3 id="索引、切片、拼接、修改"><a class="docs-heading-anchor" href="#索引、切片、拼接、修改">索引、切片、拼接、修改</a><a id="索引、切片、拼接、修改-1"></a><a class="docs-heading-anchor-permalink" href="#索引、切片、拼接、修改" title="Permalink"></a></h3><p><a href="https://pytorch.org/docs/1.12/generated/torch.adjoint.html#torch.adjoint"><code>adjoint</code></a>：返回张量共轭和最后两个维度转置的引用视图</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.argwhere.html#torch.argwhere"><code>argwhere</code></a>：返回输入的所有非零元素的索引张量</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.cat.html#torch.cat"><code>cat</code></a>、<a href="https://pytorch.org/docs/1.12/generated/torch.concat.html#torch.concat"><code>concat</code></a>：连接给定维度中给定的 <code>seq</code> 张量序列</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.conj.html#torch.conj"><code>conj</code></a>：返回输入张量的翻转共轭位的引用视图</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.chunk.html#torch.chunk"><code>chunk</code></a>：尝试将张量拆分为指定数量的块</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.split.html#torch.split"><code>split</code></a>：将张量拆分为块</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.tensor_split.html#torch.tensor_split"><code>tensor_split</code></a>：根据索引或由 <code>indices_or_sections</code> 指定的部分数量，将张量拆分为多个子张量，所有这些子张量都是输入的视图</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.vsplit.html#torch.vsplit"><code>vsplit</code></a>、<a href="https://pytorch.org/docs/1.12/generated/torch.hsplit.html#torch.hsplit"><code>hsplit</code></a>、<a href="https://pytorch.org/docs/1.12/generated/torch.dsplit.html#torch.dsplit"><code>dsplit</code></a>：根据 <code>indices_or_sections</code> 将输入张量（具有三个或更多维度的张量）沿着垂直、水、深度方向拆分为多个张量</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.row_stack.html#torch.row_stack"><code>row_stack</code></a>：通过在张量中垂直堆叠张量来创建一个新张量</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.column_stack.html#torch.column_stack"><code>column_stack</code></a>：通过在张量中水平堆叠张量来创建一个新张量</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.vstack.html#torch.vstack"><code>vstack</code></a>、<a href="https://pytorch.org/docs/1.12/generated/torch.hstack.html#torch.hstack"><code>hstack</code></a>、<a href="https://pytorch.org/docs/1.12/generated/torch.dstack.html#torch.dstack"><code>dstack</code></a>：沿着垂直、水平、深度方向堆叠张量</p><blockquote><p>垂直、水平、深度三个方向对应着第一、二、三个维度</p></blockquote><p><a href="https://pytorch.org/docs/1.12/generated/torch.stack.html#torch.stack"><code>stack</code></a>：沿着给定的维度拼接张量序列</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.gather.html#torch.gather"><code>gather</code></a>：沿由 <code>dim</code> 指定的轴收集值</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.index_add.html#torch.index_add"><code>index_add</code></a>、<a href="https://pytorch.org/docs/1.12/generated/torch.index_copy.html#torch.index_copy"><code>index_copy</code></a>、<a href="https://pytorch.org/docs/1.12/generated/torch.index_reduce.html#torch.index_reduce"><code>index_reduce</code></a>：</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.index_select.html#torch.index_select"><code>index_select</code></a>：返回一个新张量，该张量使用索引中的条目沿维度 <code>dim</code> 索引输入张量，该条目是 <code>LongTensor</code></p><p><a href="https://pytorch.org/docs/1.12/generated/torch.masked_select.html#torch.masked_select"><code>masked_select</code></a>：返回一个新的一维张量，它根据布尔掩码掩码索引输入张量，该掩码掩码是 <code>BoolTensor</code></p><p><a href="https://pytorch.org/docs/1.12/generated/torch.movedim.html#torch.movedim"><code>movedim</code></a>、<a href="https://pytorch.org/docs/1.12/generated/torch.moveaxis.html#torch.moveaxis"><code>moveaxis</code></a>：将 <code>source</code> 中位置的输入维度移动到目标位置</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.narrow.html#torch.narrow"><code>narrow</code></a>：返回一个新的张量，它是输入张量的缩小版本</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.nonzero.html#torch.nonzero"><code>nonzero</code></a>：返回非零元素的索引</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.permute.html#torch.permute"><code>permute</code></a>：返回原始张量 <code>input</code> 的置换之后的视图</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.reshape.html#torch.reshape"><code>reshape</code></a>：重新定义输入张量的 <code>size</code></p><p><a href="https://pytorch.org/docs/1.12/generated/torch.select.html#torch.select"><code>select</code></a>：在给定索引处沿选定维度对输入张量进行切片</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.scatter.html#torch.scatter"><code>scatter</code></a>、<a href="https://pytorch.org/docs/1.12/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"><code>torch.Tensor.scatter_()</code></a>：</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.diagonal_scatter.html#torch.diagonal_scatter"><code>diagonal_scatter</code></a>：将 <code>src</code> 张量的值相对于 <code>dim1</code> 和 <code>dim2</code> 沿输入的对角线元素嵌入到输入张量中</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.select_scatter.html#torch.select_scatter"><code>select_scatter</code></a>：将 <code>src</code> 张量的值嵌入到给定索引处的输入中</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.slice_scatter.html#torch.slice_scatter"><code>slice_scatter</code></a>：将 <code>src</code> 张量的值嵌入到给定维度的输入中</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.scatter_add.html#torch.scatter_add"><code>scatter_add</code></a>：</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.scatter_reduce.html#torch.scatter_reduce"><code>scatter_reduce</code></a>：</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.tile.html#torch.tile"><code>tile</code></a>：通过重复输入的元素构造一个张量</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.squeeze.html#torch.squeeze"><code>squeeze</code></a>：将所有大小为 1 的维度都去掉</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.unsqueeze.html#torch.unsqueeze"><code>unsqueeze</code></a>：返回一个插入到指定位置的尺寸为 1 的新张量</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.unbind.html#torch.unbind"><code>unbind</code></a>：去除指定维度（返回的是一个保存对这个维度进行分割之后所有矩阵的元组）</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.swapaxes.html#torch.swapaxes"><code>swapaxes</code></a>、<a href="https://pytorch.org/docs/1.12/generated/torch.swapdims.html#torch.swapdims"><code>swapdims</code></a>、<a href="https://pytorch.org/docs/1.12/generated/torch.transpose.html#torch.transpose"><code>torch.transpose()</code></a>：交换 <code>dim0</code> 和 <code>dim1</code></p><p><a href="https://pytorch.org/docs/1.12/generated/torch.t.html#torch.t"><code>t</code></a>：要求 <code>input</code> 张量的维度必须小于等于 2，转换维度 0 和维度 1</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.take.html#torch.take"><code>take</code></a>：返回一个新的张量，其中输入的元素在给定的索引处</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.take_along_dim.html#torch.take_along_dim"><code>take_along_dim</code></a>：从沿给定暗淡的索引中选择一维索引处的输入值</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.where.html#torch.where"><code>where</code></a>：根据条件返回从 <code>x</code> 或 <code>y</code> 中选择的元素的张量</p><h3 id="随机采样"><a class="docs-heading-anchor" href="#随机采样">随机采样</a><a id="随机采样-1"></a><a class="docs-heading-anchor-permalink" href="#随机采样" title="Permalink"></a></h3><p><a href="https://pytorch.org/docs/1.12/generated/torch.seed.html#torch.seed"><code>seed</code></a>：随机设置一个随机种子，不需要给定参数（会返回一个 64 比特的数字代表设置的种子）</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.manual_seed.html#torch.manual_seed"><code>manual_seed</code></a>：设定随机种子为给定的 <code>seed</code>，会返回一个 <code>torch.Generator</code></p><p><a href="https://pytorch.org/docs/1.12/generated/torch.initial_seed.html#torch.initial_seed"><code>initial_seed</code></a>：将生成随机数的初始种子以  Python <code>long</code> 数据类型返回</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.get_rng_state.html#torch.get_rng_state"><code>get_rng_state</code></a>：以 <code>torch.ByteTensor</code> 的数据类型返回随机数生成器</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.set_rng_state.html#torch.set_rng_state"><code>set_rng_state</code></a>：设置随机数生成器状态</p><p><strong>随机数生成器</strong></p><p><a href="https://pytorch.org/docs/1.12/generated/torch.bernoulli.html#torch.bernoulli"><code>bernoulli</code></a>：伯努利分布，生成 0 或者 1</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.multinomial.html#torch.multinomial"><code>multinomial</code></a>：返回一个张量，其中每行包含从位于张量输入的相应行中的多项概率分布中采样的 <code>num_samples</code> 个索引</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.normal.html#torch.normal"><code>normal</code></a>：返回从给出均值和标准差的独立正态分布中抽取的随机数张量</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.poisson.html#torch.poisson"><code>poisson</code></a>：返回与输入相同大小的张量，每个元素从泊松分布中采样，速率参数由输入中的相应元素给出</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.rand.html#torch.rand"><code>rand</code></a>、<a href="https://pytorch.org/docs/1.12/generated/torch.rand_like.html#torch.rand_like"><code>rand_like</code></a>：单位均匀分布 <span>$[0, 1)$</span></p><p><a href="https://pytorch.org/docs/1.12/generated/torch.randint.html#torch.randint"><code>randint</code></a>、<a href="https://pytorch.org/docs/1.12/generated/torch.randint_like.html#torch.randint_like"><code>randint_like</code></a>：<code>[low, high)</code> 之间的随机整数</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.randn.html#torch.randn"><code>randn</code></a>、<a href="https://pytorch.org/docs/1.12/generated/torch.randn_like.html#torch.randn_like"><code>randn_like</code></a>：标准正太分布采样</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.randperm.html#torch.randperm"><code>randperm</code></a>：返回从 0 到 n - 1 的整数的随机排列</p><p>一些 In-place 的随机采样：</p><ul><li><a href="https://pytorch.org/docs/1.12/generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_"><code>torch.Tensor.bernoulli_()</code></a> - in-place version of <a href="https://pytorch.org/docs/1.12/generated/torch.bernoulli.html#torch.bernoulli"><code>torch.bernoulli()</code></a></li><li><a href="https://pytorch.org/docs/1.12/generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_"><code>torch.Tensor.cauchy_()</code></a> - numbers drawn from the Cauchy distribution</li><li><a href="https://pytorch.org/docs/1.12/generated/torch.Tensor.exponential_.html#torch.Tensor.exponential_"><code>torch.Tensor.exponential_()</code></a> - numbers drawn from the exponential distribution</li><li><a href="https://pytorch.org/docs/1.12/generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_"><code>torch.Tensor.geometric_()</code></a> - elements drawn from the geometric distribution</li><li><a href="https://pytorch.org/docs/1.12/generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_"><code>torch.Tensor.log_normal_()</code></a> - samples from the log-normal distribution</li><li><a href="https://pytorch.org/docs/1.12/generated/torch.Tensor.normal_.html#torch.Tensor.normal_"><code>torch.Tensor.normal_()</code></a> - in-place version of <a href="https://pytorch.org/docs/1.12/generated/torch.normal.html#torch.normal"><code>torch.normal()</code></a></li><li><a href="https://pytorch.org/docs/1.12/generated/torch.Tensor.random_.html#torch.Tensor.random_"><code>torch.Tensor.random_()</code></a> - numbers sampled from the discrete uniform distribution</li><li><a href="https://pytorch.org/docs/1.12/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_"><code>torch.Tensor.uniform_()</code></a> - numbers sampled from the continuous uniform distribution</li></ul><h3 id="逐点操作"><a class="docs-heading-anchor" href="#逐点操作">逐点操作</a><a id="逐点操作-1"></a><a class="docs-heading-anchor-permalink" href="#逐点操作" title="Permalink"></a></h3><p><a href="https://pytorch.org/docs/1.12/generated/torch.abs.html#torch.abs"><code>abs</code></a>、<a href="https://pytorch.org/docs/1.12/generated/torch.absolute.html#torch.absolute"><code>absolute</code></a>：计算输入张量 <code>input</code> 每个元素的绝对值</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.cos.html#torch.cos"><code>cos</code></a> (<a href="https://pytorch.org/docs/1.12/generated/torch.cosh.html#torch.cosh"><code>cosh</code></a>)：计算每个元素的正弦、余弦、正切</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.asin.html#torch.asin"><code>asin</code></a> (<a href="https://pytorch.org/docs/1.12/generated/torch.arcsin.html#torch.arcsin"><code>arcsin</code></a>)、<a href="https://pytorch.org/docs/1.12/generated/torch.acos.html#torch.acos"><code>acos</code></a> (<a href="https://pytorch.org/docs/1.12/generated/torch.arccos.html#torch.arccos"><code>arccos</code></a>)、<a href="https://pytorch.org/docs/1.12/generated/torch.atan.html#torch.atan"><code>atan</code></a> (<a href="https://pytorch.org/docs/1.12/generated/torch.arctan.html#torch.arctan"><code>arctan</code></a>)：计算每个元素的反正弦、反余弦、反正切</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.asinh.html#torch.asinh"><code>asinh</code></a> (<a href="https://pytorch.org/docs/1.12/generated/torch.arcsinh.html#torch.arcsinh"><code>arcsinh</code></a>)、<a href="https://pytorch.org/docs/1.12/generated/torch.acosh.html#torch.acosh"><code>acosh</code></a> (<a href="https://pytorch.org/docs/1.12/generated/torch.arccosh.html#torch.arccosh"><code>arccosh</code></a>)、<a href="https://pytorch.org/docs/1.12/generated/torch.atanh.html#torch.atanh"><code>atanh</code></a> (<a href="https://pytorch.org/docs/1.12/generated/torch.arctanh.html#torch.arctanh"><code>arctanh</code></a>)：计算每个元素的反双曲正弦、反双曲余弦、反双曲正切</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.atan2.html#torch.atan2"><code>atan2</code></a> (<a href="https://pytorch.org/docs/1.12/generated/torch.arctan2.html#torch.arctan2"><code>arctan2</code></a>)：有两个输入 <code>input</code> 和 <code>other</code>，考虑象限的 <span>$\text{input}_i/\text{other}_i$</span> 的逐元素反正切</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.add.html#torch.add"><code>add</code></a>：将输入 <code>input</code> 点乘以 <code>alpha</code> 再加上 <code>other</code></p><p><a href="https://pytorch.org/docs/1.12/generated/torch.div.html#torch.div"><code>div</code></a> (<a href="https://pytorch.org/docs/1.12/generated/torch.divide.html#torch.divide"><code>divide</code></a>)：将输入 <code>input</code> 的每个元素除以 <code>other</code> 的对应元素</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.frac.html#torch.frac"><code>frac</code></a>：计算输入中每个元素的小数部分</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.floor_divide.html#torch.floor_divide"><code>floor_divide</code></a>：按元素计算 <code>input</code> 除以 <code>other</code>，并将每个商向零的方向舍入</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.addcdiv.html#torch.addcdiv"><code>addcdiv</code></a>：将 <code>tensor1</code> 元素点除 <code>tensor2</code> 的结果乘以标量 <code>value</code> 再加到 <code>input</code> 上</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.addcmul.html#torch.addcmul"><code>addcmul</code></a>：将 <code>tensor1</code> 元素点乘 <code>tensor2</code> 的结果乘以标量 <code>value</code> 再加到 <code>input</code> 上</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.exp.html#torch.exp"><code>exp</code></a>：计算每个元素的指数</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.frexp.html#torch.frexp"><code>frexp</code></a>：将输入分解为尾数（mantissa）和指数张量，即 <span>$\text{input}=\text{mantissa}\times 2^{\text{exponent}}$</span></p><p><a href="https://pytorch.org/docs/1.12/generated/torch.exp2.html#torch.exp2"><code>exp2</code></a> (<a href="https://pytorch.org/docs/1.12/special.html#torch.special.exp2"><code>special.exp2()</code></a>)</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.expm1.html#torch.expm1"><code>expm1</code></a> (<a href="https://pytorch.org/docs/1.12/special.html#torch.special.expm1"><code>special.expm1()</code></a>)</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.float_power.html#torch.float_power"><code>float_power</code></a>：计算每个元素的 <code>exponent</code> 次幂</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.angle.html#torch.angle"><code>angle</code></a>：计算每个元素的角度（以弧度为单位）</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.deg2rad.html#torch.deg2rad"><code>deg2rad</code></a>：返回一个新的张量，输入的每个元素都从角度转换为弧度</p><p><strong>位计算</strong></p><p><a href="https://pytorch.org/docs/1.12/generated/torch.bitwise_not.html#torch.bitwise_not"><code>bitwise_not</code></a>：计算 <code>input</code> 张量的按位非</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.bitwise_and.html#torch.bitwise_and"><code>bitwise_and</code></a>：计算 <code>input</code> 和 <code>other</code> 的按位与</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.bitwise_or.html#torch.bitwise_or"><code>bitwise_or</code></a>：计算 <code>input</code> 和 <code>other</code> 的按位或</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.bitwise_xor.html#torch.bitwise_xor"><code>bitwise_xor</code></a>：计算 <code>input</code> 和 <code>other</code> 的按位异或</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.bitwise_left_shift.html#torch.bitwise_left_shift"><code>bitwise_left_shift</code></a>：计算 <code>input</code> 左移 <code>other</code> 位的结果</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.bitwise_right_shift.html#torch.bitwise_right_shift"><code>bitwise_right_shift</code></a>：计算 <code>input</code> 右移 <code>other</code> 位的结果</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.ceil.html#torch.ceil"><code>ceil</code></a>：返回一个新的张量，其中输入元素的 <code>ceil</code> 是大于或等于每个元素的最小整数</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.floor.html#torch.floor"><code>floor</code></a>：返回一个新的张量，其中输入元素的 <code>floor</code>，最大整数小于或等于每个元素</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.clamp.html#torch.clamp"><code>clamp</code></a> (<a href="https://pytorch.org/docs/1.12/generated/torch.clip.html#torch.clip"><code>clip</code></a>)：将 <code>input</code> 中的所有元素限制在 <code>[min, max]</code> 范围内</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.conj_physical.html#torch.conj_physical"><code>conj_physical</code></a>：计算给定输入张量的元素共轭</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.copysign.html#torch.copysign"><code>copysign</code></a>：创建一个新的浮点张量，元素为 <code>input</code> 的大小和 <code>other</code> 元素的符号</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.digamma.html#torch.digamma"><code>digamma</code></a> (<a href="https://pytorch.org/docs/1.12/special.html#torch.special.digamma"><code>special.digamma()</code></a>)：</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.erf.html#torch.erf"><code>erf</code></a> (<a href="https://pytorch.org/docs/1.12/special.html#torch.special.erf"><code>special.erf()</code></a>)：</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.erfc.html#torch.erfc"><code>erfc</code></a> (<a href="https://pytorch.org/docs/1.12/special.html#torch.special.erfc"><code>special.erfc()</code></a>)：</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.erfinv.html#torch.erfinv"><code>erfinv</code></a> (<a href="https://pytorch.org/docs/1.12/special.html#torch.special.erfinv"><code>special.erfinv()</code></a>)：</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine"><code>fake_quantize_per_channel_affine</code></a>：返回一个新张量，其中输入假数据中的数据使用 <code>scale</code>、<code>zero_point</code>、<code>quant_min</code> 和 <code>quant_max</code> 在指定 <code>axis</code> 的通道上按通道进行量化</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.fake_quantize_per_tensor_affine.html#torch.fake_quantize_per_tensor_affine"><code>fake_quantize_per_tensor_affine</code></a>：返回一个新张量，其中输入假数据中使用 <code>scale</code>、<code>zero_point</code>、<code>quant_min</code> 和 <code>quant_max</code> 量化</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.fix.html#torch.fix"><code>fix</code></a> (<a href="https://pytorch.org/docs/1.12/generated/torch.trunc.html#torch.trunc"><code>trunc()</code></a>)：</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.fmod.html#torch.fmod"><code>fmod</code></a>：Applies C++’s <a href="https://en.cppreference.com/w/cpp/numeric/math/fmod">std::fmod</a> entrywise.</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.gradient.html#torch.gradient"><code>gradient</code></a>：使用二阶精确中心差法（ <a href="https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf">second-order accurate central differences method</a>）估计函数 <span>$g:\mathbb{R}^n\rightarrow \mathbb{R}$</span> 在一个或多个维度上的梯度</p><p><a href="https://pytorch.org/docs/1.12/generated/torch.imag.html#torch.imag"><code>imag</code></a>：将张量自身虚数部分作为新张量进行返回</p><h2 id="自动微分机制"><a class="docs-heading-anchor" href="#自动微分机制">自动微分机制</a><a id="自动微分机制-1"></a><a class="docs-heading-anchor-permalink" href="#自动微分机制" title="Permalink"></a></h2><p><a href="https://pytorch.org/docs/1.12/notes/autograd.html">Autograd Mechanics</a></p></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Thursday 3 November 2022 13:13">Thursday 3 November 2022</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
